{"0": {"title": "Extracting Important Tweets for News Writers using Recurrent Neural Network with Attention Mechanism and Multi-task Learning", "url": "https://aclanthology.org/Y17-1048", "text": "1 Introduction Social media information is now an important source for news writers. People who encounter an incident can post what is happening before his/her eyes using photos and videos. These posts are important primary information, so news writers want to gather them. However, extracting useful information from the vast amount of social media information is laborious. For this reason, services that enable news writers to extract information that can be used as a news source are desired. In fact, some services such as Spectee1 and FASTALERT2 have been launched 1 http://www.spectee.com 2 https://fa.xwire.jp in Japan. These services gather much information from social media and extract information that can be used as news sources. Information that news writers want to extract from social media includes many different topics such as fires, accidents, and other incidents. Therefore, extracting information from social media by filtering with keywords is difficult. Assuming the words \u201cdelay\u201d and \u201ctrain\u201d are included in the keywords, the tweet \u201cxxx line is delayed by accident,\u201d which can be used as a news source, can be extracted. However, the tweet \u201cI hope the train is delayed because I haven\u2019t studied for today\u2019s exam,\u201d which cannot be used as a news source, is also extracted. To extract tweets that include important information, filtering by keyword is not enough because the output may include tweets that cannot be used as news sources. For this reason, we have been studying automatic extraction of useful information from social media. Our purposes are to reduce the amount of laborious work and extract information that cannot be extracted by using queries. In this paper, we describe a method to extract tweets that include useful information for news writers. Generally, social media posts are often written in colloquial style and often include abbreviations, slang and emojis. This makes word segmentation difficult. Therefore, our method is character-based approach, not a word-based one. Our method analyzes each character in a tweet by using a Recurrent Neural Network (RNN) and then decides whether the tweet includes important information. We adopted an attention mechanism and multi-task learning in our method and confirmed the effectiveness of our method. Our contribution is to 363 Figure 1: Architecture of our basic system. reveal that the combination of attention mechanism and multi-task learning is effective for characterbased approaches. 2 Methods for extracting important tweets In this section, we describe our method for extracting important tweets for news writers. We use a Recurrent Neural Network (RNN)-based model as the basic method. And, we add the attention mechanism and multi-task learning. 2.1 Basic model (RNN-based model) As we mentioned, sentences in social media are often written in colloquial style and often include abbreviations, slang and emojis. This makes it difficult to use state-of-the-art word segmentation or other natural language processing (NLP) tools. Japanese, our target language, is written without word separation, so the accuracy of the word segmentation directly affects the performance of word-based NLP tasks. Actually, according to Ling et al. ( 2015) and Dhingra et al. ( 2016), the character-based approach outperformed the word-based one in the social media analysis task. For these reasons, we chose characters, not words, as the input of our models. Our basic method uses bi-directional RNN (biRNN) for obtaining vector representations of the input tweet. Each character in a tweet is sequentially inputted for both the forward and backward directions. When all characters are inputted, the final Figure 2: Two types of attention. hidden states of the biRNN are used as vector representations. Then, our method classifies the tweet according to whether it is important by using a twolayer Feed-Forward Neural Network (FFNN). Figure 1 shows the architecture of our basic model. 2.2 Attention-based model The attention mechanism has been used in many NLP tasks, such as machine translation (Bahdanau et al., 2015; Luong et al., 2015a) and image captioning (Xu et al., 2015), and can give weights to each input data taking into account the importance. In this paper, for comparison, we prepare two types of attention mechanism: \u201cFinalState\u201d and \u201cMeanVector.\u201d 2.2.1 FinalState attention FinalState attention is the conventional method. In this method, we calculate the attention weight using the final hidden state of biRNN (Figure 2- (a)). Here, we explain using the example of forward RNN: actually, we use both forward and backward RNNs. The score for the t-th character scoret is calculated as follows: scoret = h T f h\u00af t Here h\u00af t is the hidden state of RNN, in which the t-th character of the input tweet has been inputted, and hf is the hidden state of RNN, in which the final character of the tweet has been processed. 364 By using the score, the weight for the t-th character Wt is as follows: Wt = exp(scoret) \u2211 t \u2032 exp(scoret \u2032) Here, t \u2032 means the set of all characters in the tweet. By using the weight and hidden state of the character, the FinalState attention af can be given: af = \u2211 t \u2032 Wtht Our method uses the sum of af and hf as a feature, and judges whether the tweet can be used as a news sources. The architecture of the FFNN consists of two layers as shown in Figure 1. This method can give high weight to the characters that strongly affect the vector representation of the whole tweet because the higher the similarity between the t-th character\u2019s vector ht and the tweet\u2019s vector hf , the higher the weight Wt is. 2.2.2 MeanVector attention MeanVector attention is our proposed method. In this method, we calculate the attention weight using the mean vector of the hidden state of biRNN for every character in the tweet (Figure 2-(b)). Similar to section 2.2.1, we explain using the example of the forward RNN. The score for the t-th character scoret is calculated as follows: scoret = h T mh\u00af t hm = \u2211 t \u2032 h\u00af t t \u2032 By using scoret , we can calculate the weight Wt and MeanVector attention am in a similar way to that in section 2.2.1. Wt = exp(scoret) \u2211 t \u2032 exp(scoret \u2032) am = \u2211 t \u2032 Wtht We use the sum of am and hf as the input of the two-layer FFNN, and judge the tweet according to whether it can be used as a news source. MeanVector attention can also give high weight to the characters that strongly affect the meaning of the Figure 3: Overall structure using FinalState attention and multi-task learning. whole tweet. However, compared with FinalState attention, the effect of the position of the character appeared to be reduced. 2.3 Multi-task learning In some studies using a neural network, models are trained with multiple tasks. This technique is called \u201cmulti-task learning.\u201d It has been reported that by using multi-task learning, the model can be generic and accurate (Luong et al., 2015b; S\u00f8gaard et al., 2016). Therefore we also use multi-task learning so that our model is more accurate. In addition to our target task, by judging whether the tweet can be used as a news sources, we prepare the task that involves estimating the next character of the input character as another task. This task is the same as \u201cneural language model learning.\u201d We do not need to prepare training data for this task; we can use the same dataset as that of the target task without new annotated data. We designed our architecture as sharing input and a biRNN layer with these two tasks and prepared two output layers for each task. To train this model, first we start to train with the neural language model learning. After finishing this task, we start to train for the target task using the results of the first training as the initial model of the target task. Figure 3 illustrates the overall structure of our method using FinalState attention and multi-task learning. 5 Conclusion We presented a method to extract tweets that can be used as news sources using a recurrent neural network with attention and multi-task learning. In this paper, we confirmed the effect of the attention mechanism and multi-task learning in our task. Comparing the two methods of attention mechanism, FinalState and MeanVector, we showed that the MeanVector method is better in our task. Overall, our method (MeanVector attention with multitask learning) achieved an F-measure of 0.627 in F-measure, which is 0.037 higher than baseline method. In our experiment, the attention mechanism is effective only when used with multi-task learning. Our future work is adding new features according to the task, multi-class classifying to detect which kind of incident is mentioned in the tweet, and extracting more information such as geolocation and the subject person of the tweet.", "SumGPT3": "**Summary:**\n\n*Introduction:*  \nThe paper addresses the challenge of extracting relevant information from social media for news writing. Traditional keyword-based filtering methods often yield irrelevant results due to colloquial language and slang commonly found in social media posts. To tackle this, the authors propose a character-based approach using a Recurrent Neural Network (RNN) with attention mechanism and multi-task learning.\n\n*Novelty:*  \n1. **Character-based Approach:** Unlike traditional word-based methods, the paper employs a character-based approach for analyzing social media posts. This is particularly effective for languages like Japanese where word segmentation accuracy is crucial.\n2. **Attention Mechanism:** Introducing an attention mechanism allows the model to assign weights to each input character based on its importance, enhancing the accuracy of tweet classification.\n3. **MeanVector Attention:** The paper introduces MeanVector attention, a novel method for calculating attention weights using the mean vector of the hidden state of the biRNN. This method proves to be more effective than the conventional FinalState attention, especially in capturing the overall meaning of the tweet.\n4. **Multi-task Learning:** By incorporating multi-task learning, the model not only classifies tweets but also predicts the next character in the input sequence, enhancing the model's genericity and accuracy. This approach demonstrates improved performance in tweet extraction compared to traditional methods.\n\n*Conclusion:*  \nThe study presents a comprehensive method for extracting tweets relevant for news writing, leveraging character-based analysis, attention mechanisms, and multi-task learning. MeanVector attention coupled with multi-task learning emerges as the most effective approach, outperforming baseline methods in tweet extraction tasks. Future work may focus on further enhancing the model with additional features and information extraction capabilities.", "SumGem": "## Summary\n\nThis paper proposes a method to extract tweets relevant for news writers by leveraging Recurrent Neural Networks (RNNs) with attention mechanisms and multi-task learning. The key novelty of this approach lies in two aspects:\n\n* **Character-based analysis:** Unlike traditional methods that rely on word segmentation, this approach analyzes tweets on a character-by-character basis. This is particularly useful for social media text ridden with informal language, abbreviations, and emojis, where word segmentation can be challenging.\n\n* **MeanVector attention:** The paper introduces a novel attention mechanism called MeanVector attention. This method assigns weights to characters in a tweet based on their influence on the entire tweet's meaning, as opposed to just the final character (FinalState attention, a commonly used approach). This can potentially capture long-range dependencies within the tweet and improve the identification of informative elements.\n\n## Conclusion\n\nThe paper demonstrates that the proposed method using MeanVector attention and multi-task learning outperforms a baseline model in identifying news-worthy tweets. The character-based analysis and the MeanVector attention mechanism contribute to this success, offering a valuable approach for extracting relevant information from social media text.", "SumGPT4": "Introduction\nThe paper discusses the development of an automated system to extract relevant tweets that can serve as primary information sources for news writers. Given the challenges posed by the informal language of social media\u2014which includes abbreviations, slang, and emojis\u2014a character-based analysis using Recurrent Neural Networks (RNNs) is proposed, diverging from traditional word-based methods. This approach addresses the difficulties of segmenting text where there is no clear separation between words, as is the case in Japanese.\n\nNovelty\n\nCharacter-Based Analysis: Unlike standard word-based methods, this paper introduces a character-based approach using bi-directional RNNs to process each character from both forward and backward directions, which helps in handling texts with no clear word boundaries and in colloquial style.\nAttention Mechanisms: Two types of attention mechanisms are introduced: FinalState and MeanVector. FinalState attention utilizes the last character\u2019s hidden state to weight the importance of characters, whereas MeanVector attention calculates weights based on the mean vector of all character states, which helps reduce the bias towards characters based on their position in the text.\nMulti-Task Learning: The system enhances accuracy through multi-task learning, incorporating a secondary task of predicting the next character in a sequence along with the primary task of tweet relevance classification. This method uses the neural language model learning for initial training, which improves the generalizability and performance of the primary task without requiring additional annotated data.\nConclusion\nThe paper validates the effectiveness of combining a character-based approach with advanced attention mechanisms and multi-task learning in extracting useful tweets for news sources. The findings reveal that the MeanVector attention mechanism, when used with multi-task learning, offers superior performance in determining the relevance of tweets, achieving a higher F-measure compared to baseline methods. Future work will focus on enhancing feature extraction, improving incident classification, and integrating additional information like geolocation and subject identity from tweets.", "rankGPT3": "3", "rankGem": "2", "rankGPT4": "1", "expGPT3": ": While this summary provides a good overview of the paper's focus on suggestion mining, it lacks detail in explaining the specific components of the proposed model and the semi-supervised learning approach. It briefly mentions the hybrid architecture and semi-supervised learning but doesn't delve into the technical details or novelty as deeply as the other summaries. Additionally, the conclusion lacks specificity in discussing the model's performance and potential future research directions compared to the other summaries.", "expGem": "This summary offers a clear and concise overview of the paper's main contributions, including the hybrid deep learning model and the self-training approach for semi-supervised learning. It provides a good balance of technical detail and readability, making it easy to understand the novelty of the proposed methods. However, it could benefit from slightly more detail on the specific linguistic features and how they are integrated into the model compared to Summary 3.", "expGPT4": "This summary provides the most detailed and comprehensive overview of the paper's contributions. It effectively explains the hybrid neural architecture, the integration of linguistic features, and the semi-supervised learning approach in depth. Additionally, it highlights the focus on explicitly stated suggestions directed at fellow customers, which adds clarity to the paper's objectives. Overall, this summary offers the most thorough understanding of the paper's content and its implications for future research in NLP applications."}, "1": {"title": "Amsqr at SemEval-2020 Task 12: Offensive Language Detection Using Neural Networks and Anti-adversarial Features", "url": "https://aclanthology.org/2020.semeval-1.250", "text": "1 Introduction Keeping social media platforms free from unwanted publications such as spam, scam, phishing, hate speech, targeted attacks and fake news is still an active research topic nowadays. This is due not only to the relative low cost of creating fake accounts, bots (Albadi et al., 2019) and forging online identities but also to the large amount of personal information made available on the Internet which makes targeting certain groups and individuals easier than ever. While some of these threats were seen before affecting traditional messaging platforms such as email and SMS, the reach and adoption of social media applications have amplified their impact, requiring additional cost and effort to mitigate. The use of offensive language as a vehicle to attack individuals and communities poses challenges not only for humans, which are prone to subjective and biased judgement (Sap et al., 2019) but also for automatic moderation systems. The inherent ambiguity when dealing with messages which are often short, can be written in mixed languages, contain informal words and are usually subject to adversarial modifications render na\u00a8\u0131ve filtering approaches such as word lists or re-purposed spam detection models ineffective. For this reason, in order to solve this problem more sophisticated approaches such as state of the art natural language processing (NLP) is needed. In order to keep track and measure the progress in the area on offensive language detection several English datasets with annotations for hate (Davidson et al., 2017), (Waseem and Hovy, 2016), targeted (Zampieri et al., 2019a) and personal attacks (Wulczyn et al., 2016) were released over the last years. Likewise, public evaluations such as HatEval (Basile et al., 2019) and OffenseEval (Zampieri et al., 2019b) were recently introduced highlighting the need for stronger baselines to assess the performance of more complex systems. This paper evaluates the method and system submitted to the shared task 12 of SemEval-2020: Multilingual Offensive Language Identification in Social Media (Zampieri et al., 2020) for the subtask A (English) based on an stacked ensemble of neural networks. The rest of the document is organised as follows: In section 2, we review related work relevant for detecting abusive language. In Section 3 we describe our layered model approach including our anti-adversarial strategy based on text normalisation and stacking-based ensembling. In Section 4 we show the results obtained in the test and evaluation datasets. Finally, in Section 5 we draw our conclusions and outline future work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1899 2 Related Work Previous work on automatic hate speech and offensive language detection made use of linear models over word n-grams (Malmasi and Zampieri, 2017) and sentiment lexicons (Davidson et al., 2017). However most recent research is dominated by netural network architectures: Liu et al. (2019) and Zhu et al. (2019) applied bidirectional transformers (BERT) (Devlin et al., 2018) with success showing that pre-trained models fine-tuned for this task can outperform other approaches. On the other hand, convolutional neural networks (CNN) and bidirectional LSTMs (bi-LSTMs) provided strong results (Mahata et al., 2019) when paired with pre-trained embeddings such as FastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014) or word2vec (Mikolov et al., 2013). While adding more complexity, combining several models can effectively reduce classification bias and variance. We have seen good results using voting ensembles (Seganti et al., 2019) and stacked generalisation (Malmasi and Zampieri, 2018) when applied to this particular problem. 3 Methodology The goal of Subtask-A is determining if a tweet is either offensive or not offensive, which conceptually translates to a binary classifier using F1-macro as scoring function. However, during the exploratory data analysis of the training set we\u2019ve identified group of instances where users intentionally crafted offensive messages to bypass profanity and moderation filters. For this reason, our design choices have an anti-adversarial strategy in mind. Best performing models in previous benchmarks (Basile et al., 2019), (Zampieri et al., 2019b) were based on popular pre-trained embeddings and architectures, either using transfer learning or leveraging these directly. While this is quite convenient in terms of computing cost it also introduces potential weaknesses which can be exploited in a black-box scenario. By guessing the base architecture the model was built upon, since there is a reduced set of high-quality pretrained models, an attacker could launch more successful black-box attacks (Wang et al., 2018). This is usually performed via input perturbation such as introducing synonyms (Jin et al., 2019), flipping characters (Pruthi et al., 2019) or including targeted keywords and typos (Shi et al., 2020). Being even possible to steal the whole model altogether (Krishna et al., 2019) in more sophisticated attacks. 3.1 Text Normalisation Lexical normalisation techniques are particularly effective against black-box adversarial attacks (Alshemali and Kalita, 2019), while they also can increase the performance of NLP tools and applications when working with informal text (Mosquera and Moreda, 2013). For this reason, we have applied to some of our inputs a text normalisation filter in order to reduce out-of-vocabulary words (OOV). This is not only effective against some adversarial perturbations but also replaces common typos and informal lexical variants commonly found in microblogs with their canonical version. This is performed at 2 levels: lexical and graphical. At lexical level we follow a similar modular architecture as TENOR (Mosquera et al., 2012) where a high-precision, low recall normalization dictionary is recursively combined with shortening/lengthening and re-casing rules. See table 1. Likewise, unicode homographs and near-homographs are translated to their ASCII equivalent by using a lookup table. See last entry in table 1. Original Normalised Then these dumba$$es vote Democrat!?!!! then these dumb asses vote democrat @USER Again another b******* story no one is again another bullshit story no one is watching football because of this a****** watching football because of this asshole theyre abso shite quality tho they are absolute shit quality though Gets Period* You are the cause of my gets period you are the cause of my dysphoria Table 1: Text fragments where after normalisation a label flip was observed during validation. 1900 3.2 Ensembling Aiming to minimize the impact of adversarial attacks targeting popular models we have designed a 2- level classifier based on stacked generalisation as shown in Figure 1. Figure 1: Stacked generalisation. Image reproduced from Polikar (2006) The first level (L1) comprises of 42 models trained over several lexical resources using the OLID (Zampieri et al., 2019a) dataset and labels. This effectively encapsulates different models and training datasets, having more chances to thwart off-the-shelf attacks for specific architectures. Details of individual models and datasets for level 1 at can be found at Table 2. At the second level (L2) there is a LightGBM (Ke et al., 2017) binary classifier trained over 55 boosting rounds with binary logarithmic loss. These and other model hyper-parameters were tuned against the OffenseEval 2019 evaluation set. The most important level-1 features of the final model considering both split count and gain can be seen at Figure 2. From there we can observe that both BERT (toxic, toxicnorm), and GloVe-based (capsuleglove, cnnglove) neural networks are clearly the strongest models in the ensemble. Figure 2: Feature importance (top 20) based on splits (left) and gain (right) for the LightGBM metaclassifier 1 https://github.com/kermitt2/delft 2 https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data 3 https://www.kaggle.com/c/detecting-insults-in-social-commentary/data 1901 Level 1 model Description Training labels capsuleglove Capsule network (CapsNet) + GloVe. CapsNets (Sabour et al., 2017) have been shown as an alternative to Convolutional Neural Networks (CNNs) but more robust against white-box adversarial attacks (Frosst et al., 2018). CapsNets have been also seen outperforming CNNs in offensive text identification (Hettiarachchi and Ranasinghe, 2019) OLID cnnglove CNN + GloVe OLID delft.* GRU + FastText. The multi-class toxicity classifier from DeLFT 1 is leveraged to provide 6 inputs (delftidentity hate, delftinsult, delftobscene, delftsevere toxic, delftthreat and delfttoxic) Wikipedia Toxic 2 normaliseddelft.* GRU + FastText + text normalisation produces 6 inputs (normaliseddelftidentity hate, normaliseddelftinsult, normaliseddelftobscene, normaliseddelftsevere toxic, normaliseddelftthreat and normaliseddelfttoxic) Wikipedia Toxic bert.* BERT finetuned provides 6 inputs (toxic, severe toxic, obscene, threat, insult and identity hate) Wikipedia Toxic normalisedbert.* BERT + text normalisation provides 6 inputs (toxicnorm, severe toxicnorm, obscenenorm, threatnorm, insultnorm and identity hatenorm) Wikipedia Toxic vader VADER (Hutto and Gilbert, 2014) polarity score Unsupervised normalisedvader VADER polarity score after text normalisation Unsupervised flair Recurrent Neural Network (RNN) sentiment classifier based on Flair (Akbik et al., 2019) IMDB (Maas et al., 2011) normalisedflair RNN-Flair sentiment score applied to normalised text IMDB qcount Number of question marks Unsupervised emoji max max Emoji offensive priors extracted from (Rosenthal et al., 2020). Only the highest offensive score (in case more than emoji appears in a message) of the maximum offensive score per emoji is considered OffensEval 2020 charhasoc Character n-gram (3-6) + logistic regression HASOC (Mandl et al., 2019) chartrac Character n-gram (3-6) + logistic regression TRAC (Kumar et al., 2018) hateval.* Word n-gram (1-3) + logistic regression over the 3 different labels, providing wordhate, wordtarget and wordag inputs for hate spech, targeted attack and aggression respectively at word level. The same were also trained at characted level (3-6), resulting the another 3 inputs ( charhate, chartarget and charag) HatEval (Basile et al., 2019) charinsult Character n-gram (3-6) + logistic regression Kaggle insults 3 Table 2: Summary of models and inputs. 5 Conclusion and Future Work In this paper we describe our system and method for detecting offensive tweets built for SemEval-2020 Task 12 - subtask A. Our design choices had an adversarial environment in mind and therefore we\u2019ve made use of anti-adversarial features such as text normalisation and ensemble learning, obtaining strong results in 2 evaluation datasets. In a future work we would like to explore different attack and defence scenarios for this particular problem.", "SumGPT3": "**Summary:**\n\n**Introduction:**\nThe paper addresses the ongoing challenge of keeping social media platforms free from offensive content like hate speech and fake news. It emphasizes the increasing sophistication of online threats and the limitations of traditional filtering methods, highlighting the need for advanced natural language processing (NLP) techniques.\n\n**Novelty:**\n1. **Anti-Adversarial Strategy:** The paper introduces an innovative approach to combatting adversarial attacks by incorporating an anti-adversarial strategy into the offensive language detection model. This strategy involves text normalization techniques to mitigate the impact of input perturbations commonly used in black-box attacks.\n   \n2. **Layered Model with Ensembling:** Instead of relying on single models, the paper proposes a layered model approach using stacked generalization. This involves combining multiple models trained on diverse lexical resources, effectively reducing vulnerability to off-the-shelf attacks and enhancing classification robustness.\n\n3. **Text Normalization:** The paper emphasizes the importance of text normalization in enhancing NLP tools' performance and resilience against adversarial attacks. It describes a modular text normalization filter that addresses lexical and graphical variations in informal text, thereby reducing out-of-vocabulary words and enhancing model generalization.\n\n**Conclusion:**\nThe paper presents a comprehensive method and system for detecting offensive language on social media platforms, emphasizing the novel anti-adversarial strategy, layered model approach, and the integration of text normalization techniques. Through rigorous evaluation and comparison with existing benchmarks, the proposed system demonstrates strong performance in identifying offensive content, paving the way for future research in defense mechanisms against evolving online threats.", "SumGem": "## Summary of Scientific Paper on Offensive Language Detection in Social Media\n\nThis paper describes a system for detecting offensive language in social media tweets, with a focus on addressing potential weaknesses in previous approaches.\n\n**Novelty:**\n\n* The system incorporates an **anti-adversarial strategy** to mitigate the effects of potential attackers who might try to bypass filters by crafting messages to dodge detection.\n* This strategy includes two main components:\n    * **Text normalization:** This reduces the impact of out-of-vocabulary words (OOV) and common typos by converting them to standard spellings. This makes it harder for attackers to craft adversarial messages that rely on these variations.\n    * **Stacked ensemble learning:** The system uses a two-level classification approach. The first level uses a variety of models trained on different datasets and lexical resources. This helps to avoid vulnerabilities that attackers might target if they knew the specific model being used. The second level combines the predictions from the first level to make a final classification.\n\n**Conclusion**\n\nThe paper\u2019s proposed system demonstrates strong results in offensive language detection while incorporating methods to improve robustness against potential adversarial attacks. The authors plan for future work to explore different attack and defense scenarios in this area.", "SumGPT4": "Introduction\nThe paper addresses the ongoing challenge of moderating offensive language on social media platforms, where traditional filtering techniques fall short due to the sophistication of adversarial attacks and the dynamic nature of language usage online. It evaluates a method designed for SemEval-2020 Task 12 - Subtask A, which involves identifying offensive content in multilingual social media posts using a novel stacked ensemble of neural networks.\n\nNovelty\n\nText Normalization: The proposed system incorporates text normalization to combat adversarial text modifications such as synonym replacement, character flipping, and typo insertion. This preprocessing step converts informal and manipulated text into a more standardized form, reducing out-of-vocabulary words and improving the robustness of subsequent model predictions.\nEnsemble Learning: Unlike single-model approaches, the paper introduces a two-level stacked generalization ensemble model. The first level consists of 42 diverse models trained on different lexical resources, enhancing the system's ability to generalize across various forms of offensive content. The second level utilizes a LightGBM classifier that integrates the outputs of the first-level models, effectively reducing bias and variance in predictions.\nAnti-Adversarial Features: The model design explicitly considers potential black-box adversarial attacks by employing a strategy that anticipates and mitigates such attempts. This proactive approach leverages the strengths of both text normalization and ensemble learning to defend against sophisticated methods aimed at fooling the detection system.\nConclusion\nThe paper presents a comprehensive and robust approach to detecting offensive language on social media, significantly advancing the field by integrating text normalization and ensemble learning techniques. This method not only improves the accuracy of offensive content detection but also enhances the system\u2019s resilience against adversarial attacks, setting a new standard for automated content moderation technologies. Future work will explore more complex attack and defense scenarios, further solidifying the system's effectiveness in real-world applications.", "rankGPT3": "2", "rankGem": "3", "rankGPT4": "1", "expGPT3": "This summary offers a comprehensive overview of the paper's main contributions, including the introduction, novelty, and conclusion sections. It effectively highlights the need for advanced NLP techniques to combat offensive content on social media platforms and introduces the novel anti-adversarial strategy, layered model approach, and text normalization techniques. However, it lacks some of the specific details provided in Summary 3, which slightly reduces its ranking.", "expGem": "While this summary provides a good overview of the paper's main points, it lacks the depth and specificity of Summaries 1 and 3. It briefly mentions the anti-adversarial strategy, text normalization, and ensemble learning but does not delve into the details of each component as extensively as the other summaries. Additionally, the language could be clearer and more concise, impacting its ranking compared to the other summaries.", "expGPT4": "This summary provides a detailed overview of the paper's key points, including the problem addressed, the novelty of the proposed approach, and the conclusion. It effectively summarizes the introduction, novelty, and conclusion sections of the paper, offering insights into text normalization, ensemble learning, and anti-adversarial features. The language is clear, and the summary is well-structured, making it easy to understand the paper's contributions."}, "2": {"title": "A Neural Network Architecture for Multilingual Punctuation Generation", "url": "https://aclanthology.org/D16-1111", "text": "1 Introduction Although omnipresent in (language learner) grammar books, punctuation received much less attention in linguistics and natural language processing (Krahn, 2014). In linguistics, punctuation is generally acknowledged to possess different functions. Its traditionally most studied function is that to encode prosody of oral speech, i.e., the prosodic rhetorical function; see, e.g., (Kirchhoff and Primus, 2014) and the references therein. In particular the comma is assumed to possess a strong rhetorical function (Nunberg et al., 2002). Its other functions are the grammatical function, which leads it to form a separate (along with semantics, syntax, and phonology) grammatical submodule (Nunberg, 1990), and the syntactic function (Quirk et al., 1972), which makes it reflect the syntactic structure of a sentence. The different functions of punctuation are also reflected in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introduction (or restoration) of punctuation in speech transcripts. Most often, classifier models are used that are trained on n-gram models (Gravano et al., 2009), on n-gram models enriched by syntactic and lexical features (Ueffing et al., 2013) and/or by acoustic features (Baron et al., 2002; Kola\u00b4\u02c7r and Lamel, 2012). Tilk and Alumae\u00a8 (2015) use a lexical and acoustic (pause duration) feature-based LSTM model for the restoration of periods and commas in Estonian speech transcripts. The grammatical and syntactic functions of punctuation have been addressed in the context of written 1048 language. Some of the proposals focus on the grammatical function (Doran, 1998; White and Rajkumar, 2008), while others bring the grammatical and syntactic functions together and design rule-based grammatical resources for parsing (Briscoe, 1994) and surface realization (White, 1995; Guo et al., 2010). Guo et al. (2010) is one of the few works that is based on a statistical model for the generation of punctuation in the context of Chinese sentence generation, trained on a variety of syntactic features from LFG f-structures, preceding punctuation bigrams and cue words. Our proposal is most similar to Tilk and Alumae\u00a8 (2015), but our task is more complex since we generate the full range of punctuation marks. Furthermore, we do not use any acoustic features. Compared to Guo et al. (2010), we do not use any syntactic features either since our input is just raw text material. 3 Model Our model is inspired by a number of recent works on neural architectures for structure prediction: Dyer et al. (2015)\u2019s transition-based parsing model, Dyer et al. (2016)\u2019s generative language model and phrase-structure parser, Ballesteros et al. (2015)\u2019s character-based word representation for parsing, and Ling et al. (2015b)\u2019s part-of-speech tagging . 3.1 Algorithm We define a transition-based algorithm that introduces punctuation marks into sentences that do not contain any punctuation. In the context of NLG, the input sentence would be the result of the surface realization task (Belz et al., 2011). As in transitionbased parsing (Nivre, 2004), we use two data structures: Nivre\u2019s queue is in our case the input buffer and his stack is in our case the output buffer. The algorithm starts with an input buffer full of words and an empty output buffer. The two basic actions of the algorithm are SHIFT, which moves the first word from the input buffer to the output buffer, and GENERATE, which introduces a punctuation mark after the first word in the output buffer. Figure 1 shows an example of the application of the two actions. At each stage t of the application of the algorithm, the state, which is defined by the contents of the outTransition Output Input [ ] [No it was not] SHIFT [No] [it was not] GENERATE(\u201c,\u201d) [No ,] [it was not] SHIFT [No , it] [was not] SHIFT [No , it was ] [not] SHIFT [No , it was not] [ ] GENERATE(\u201c.\u201d) [No, it was not .] [ ] Figure 1: Transition sequence for the input sequence No it was not \u2013 with the output No, it was not. put and input buffers, is encoded in terms of a vector st ; see Section 3.3 for different alternatives of state representation. As Dyer et al. (2015), we use st to compute the probability of the action at time t as: p(zt | st) = exp g > zt st + qzt  P z 0\u2208A exp g > z 0st + qz 0  (1) where gz is a vector representing the embedding of the action z, and qz is a bias term for action z. The set A represents the actions (either SHIFT or GENERATE(p)).1 st encodes information about previous actions (since it may include the history with the actions taken and the generated punctuation symbols are introduced in the output buffer, see Section 3.3), thus the probability of a sequence of actions z given the input sequence is: p(z | w) = Y |z| t=1 p(zt | st). (2) As in (Dyer et al., 2015), the model greedily chooses the best action to take given the state with no backtracking.2 3.2 Word Embeddings Following the tagging model of Ling et al. (2015b) and the parsing model of Ballesteros et al. (2015), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber, 2005) to learn similar representation for words that are similar from an orthographic/morphological point of view. 1Note that GENERATE(p) includes all possible punctuations that the language in question has, and thus the number of classes the classifier predicts in each time step is #punctuations + 1. 2 For further optimization, the model could be extended, for instance, by beam-search. 10 The character-based representations may be also concatenated with a fixed vector representation from a neural language model. The resulting vector is passed through a component-wise rectifier linear unit (ReLU). We experiment with and without pretrained word embeddings. To pretrain the fixed vector representations, we use the skip n-gram model introduced by Ling et al. (2015a). 3.3 Representing the State We work with two possible representations of the input and output buffers (i.e, the state st): (i) a lookahead model that takes into account the immediate context (two embeddings for the input and two embeddings for the output), which we use as a baseline, and (ii) the LSTM model, which encodes the entire input sequence and the output sentence with LSTMs. 3.3.1 Baseline: Look-ahead Model The look-ahead model can be interpreted as a 4- gram model in which two words belong to the input and two belong to the output. The representation takes the average of the two first embeddings of the output and the two first embeddings at the front of the input. The word embeddings contain all the richness provided by the character-based LSTMs and the pretrained skip n-gram model embeddings (if used). The resulting vector is passed through a componentwise ReLU and a softmax transformation to obtain the probability distribution over the possible actions given the state st ; see Section 3.1. 3.3.2 LSTM Model The baseline look-ahead model considers only the immediate context for the input and output sequences. In the proposed model, we apply recurrent neural networks (RNNs) that encode the entire input and output sequences in the form of LSTMs. LSTMs are a variant of RNNs designed to deal with the vanishing gradient problem inherent in RNNs (Hochreiter and Schmidhuber, 1997; Graves, 2013). RNNs read a vector xt at each time step and compute a new (hidden) state ht by applying a linear map to the concatenation of the previous time step\u2019s state ht\u22121 and the input, passing then the outcome through a logistic sigmoid non-linearity. We use a simplified version of the stack LSTM model of Dyer et al. (2015). The input buffer is encoded as a stack LSTM, into which we PUSH the entire sequence at the beginning and POP words from it at each time step. The output buffer is a sequence, encoded by an LSTM, into which we PUSH the final output sequence. As in (Dyer et al., 2015), we include a third sequence with the history of actions taken, which is encoded by another LSTM. As already mentioned above, the three resulting vectors are passed through a component-wise ReLU and a softmax transformation to obtain the probability distribution over the possible actions that can be taken (either to shift or to generate a punctuation mark), given the current state st ; see Section 3.1. 5 Conclusions We presented an LSTM-based architectured that is capable of adding punctuation marks to sequences of tokens as produced in the context of surface realization without punctuation with high quality and linear time.5 Compared to other proposals in the field, the architecture has the advantage to operate on sequences of word forms, without any additional syntactic or acoustic features. This tool could be used for ASR (Tilk and Alumae, 2015) and grammatical \u00a8 error correction (Ng et al., 2014). In the future, we plan to create cross-lingual models by applying multilingual word embeddings (Ammar et al., 2016).", "SumGPT3": "**Introduction:**\nThe paper addresses the under-explored area of punctuation generation in natural language processing (NLP) and linguistics. While punctuation's prosodic, grammatical, and syntactic functions are acknowledged, its automated generation has received less attention. Previous NLP tasks primarily focused on punctuation restoration in speech transcripts, using classifiers trained on n-gram models enriched with various features.\n\n**Novelty:**\n1. **Punctuation Generation Algorithm:** The paper presents a novel punctuation generation algorithm based on transition-based parsing with long short-term memories (LSTMs) and character-based continuous-space vector embeddings. Unlike existing approaches, this algorithm introduces a full range of punctuation symbols into raw text material, making it function- and language-neutral. It differs from previous methods that relied on function- and/or language-specific features.\n   \n2. **Complexity and Scope:** Unlike previous works focused on specific punctuation marks or language-specific features, this algorithm handles the generation of the entire spectrum of punctuation marks. Furthermore, it doesn't rely on acoustic features, making it applicable across various languages and contexts.\n\n3. **Model Architecture:** Inspired by recent advances in neural architectures for structure prediction, the proposed model integrates elements from transition-based parsing, generative language models, phrase-structure parsing, character-based word representation, and part-of-speech tagging. It combines LSTM networks to encode entire input and output sequences, enabling high-quality punctuation addition with linear time complexity.\n\n**Conclusion:**\nThe paper introduces a pioneering approach to punctuation generation in NLP, addressing limitations of previous methods and expanding the scope of punctuation-related tasks. By leveraging advanced neural architectures and character-based embeddings, the proposed algorithm offers a versatile and efficient solution applicable to diverse linguistic contexts without relying on language-specific features. This innovation opens avenues for applications in automated speech recognition (ASR) and grammatical error correction, with future directions including the development of cross-lingual models using multilingual word embeddings.", "SumGem": "## Summary\n\n**Introduction**\n\nThis paper proposes a novel algorithm for punctuation generation in natural language processing (NLP). Punctuation plays a crucial role in written language, but it has received less attention compared to other aspects like grammar and syntax. The authors aim to address this gap by introducing a new method for generating punctuation marks in written text.\n\n**Novelty**\n\nThe proposed algorithm is based on a transition-based system with Long Short-Term Memories (LSTMs). It has several key novelties compared to existing approaches:\n\n* **Function and language-neutral:** Unlike most existing punctuation generation methods that rely on features specific to a particular function or language, this algorithm is general-purpose. It can be applied to various punctuation tasks and languages without modification.\n* **Full range of punctuation marks:** The algorithm can generate the entire range of punctuation symbols, whereas most prior work focuses on a limited set like commas and periods. \n* **Character-based word embeddings:** The model uses character-based LSTMs to learn word representations, which can capture similarities between words based on morphology and orthography. This is beneficial for handling unseen words. \n* **No need for syntactic or acoustic features:** The algorithm operates solely on the word sequence, eliminating the need for additional features like syntactic information or acoustic cues from speech transcripts, which can be complex to obtain.\n\n**Conclusion**\n\nThe authors claim that their LSTM-based architecture achieves high-quality punctuation generation with linear time complexity. This method offers several advantages compared to existing approaches. It is applicable to various NLP tasks like automatic speech recognition and grammatical error correction, and paves the way for creating punctuation generation models for multiple languages.", "SumGPT4": "Introduction\nIntroduction\nThis paper addresses the relatively underexplored field of punctuation in natural language processing (NLP), focusing on the generation of punctuation in raw text. This task has profound implications for enhancing the clarity and readability of text in various applications, from speech synthesis to grammatical correction.\n\nNovelty\n\nFunction- and Language-Neutral Algorithm: Unlike most existing models that incorporate language-specific or function-specific features, this paper presents a punctuation generation algorithm that is both function- and language-neutral. This broad applicability marks a significant advancement over more specialized systems.\nTransition-Based Parsing with LSTMs: The model employs a novel application of a transition-based parsing approach combined with long short-term memory (LSTM) networks. This method allows for the dynamic generation of punctuation based on the evolving context of the sentence, rather than relying solely on static rules or predefined patterns.\nCharacter-Based Word Embeddings: By utilizing bidirectional LSTMs to create character-based continuous-space vector embeddings of words, the model captures fine-grained orthographic and morphological features that help in accurately predicting punctuation.\nState Representation Techniques: The paper experiments with two different state representation techniques\u2014the baseline lookahead model and a more complex LSTM model. The latter encodes the full sequence of input and output, providing a richer context for the generation decisions.\nConclusion\nThe presented approach introduces several innovations in the field of punctuation generation, particularly its neutrality to function and language and its sophisticated use of LSTM networks for state representation and decision making. This makes the model highly versatile and capable of being applied across different languages and text types without modification. Future work will focus on extending this model to multilingual contexts, leveraging cross-lingual embeddings to further enhance its applicability and accuracy.", "rankGPT3": "3", "rankGem": "1", "rankGPT4": "2", "expGPT3": "While this summary provides a comprehensive overview of the paper, it lacks a bit of clarity and organization compared to the other summaries. The introduction and novelty sections are well-presented, highlighting the key contributions of the paper. However, the conclusion section could be more concise and focused. Additionally, the language used in this summary is slightly more complex, which might make it slightly less accessible to readers unfamiliar with the topic.", "expGem": "This summary provides a clear and concise overview of the paper. It effectively captures the key points of the introduction, novelty, and conclusion sections, making it easy for readers to understand the main contributions of the research. The language used is accessible and straightforward, which enhances readability. Overall, this summary effectively communicates the significance of the proposed algorithm for punctuation generation in NLP.", "expGPT4": "This summary offers a detailed overview of the paper, particularly in the novelty section where it delves into the technical aspects of the proposed algorithm. However, it could benefit from slightly better organization and clarity, especially in the introduction and conclusion sections. The language used is slightly more technical, which might make it a bit less accessible to general readers. Nonetheless, it effectively communicates the innovations introduced in the paper and the potential implications of the research."}, "3": {"title": "Helping each Other: A Framework for Customer-to-Customer Suggestion Mining using a Semi-supervised Deep Neural Network", "url": "https://aclanthology.org/2018.icon-1.26", "text": "1 Introduction The online platforms like social media websites, e-commerce sites of products and services, blogs, online forums and discussion forums etc. are very much attached today with our day-to-day lives.The availability of the these information sharing platforms has fueled the humans\u2019 desires to share one\u2019s opinions, emotions and sentiments with respect to the entities of all kinds: be it people, events, places, organizations, institutions, products, services, hobbies, games, movies, politics, technology etc. Generally people express their opinions in three ways: (1) through an independent piece of content writing (2) writing disposed towards a theme (such as a question in a community based question answering platform, or a topic in a discussion forum, or an entity in a product reviewing website/ e-commerce website) and (3) conversational writings in the form of exchange of utterances in dialog systems/chats or comments for a post in social media/online forums. Such opinions which exist in different forms and places, have often hidden in them the experiences of people, their subjective emotions and sentiments towards different aspects of different entities, as well as the intentions of advices and suggestions proposing some action in a prescribed way. Suggestion mining can be thought of as a subproblem of opinion mining, entrusted with the task of extracting mentions of suggestions from the unstructured texts. Suggestions in the domain of reviews can be generally of two kinds: 1. Customer to Companies: These suggestions are directed from customers to the producers/service providers. Customers provide companies with feedbacks, often expressing their contentment or complaining about their dissatisfaction with certain product features, services, processes or amenities. They provide detailed reasons and personal experiences for the same and offer alternative ideas for implementation. These kinds of suggestions are not only important as a tool for the companies to review their current offerings, but they are also a great source of ideas for new directions. arXiv:1811.00379v1 [cs.CL] 1 Nov 2018 2. Customer to Customer: These suggestions are provided from customers/users to the fellow customers/users. Customers share their experiences in reviews, and provide tips and recommendations to the other customers. This is sometimes more than merely the information whether they like some specific attributes of the products or services. 1.1 Motivation and Contributions There are several use cases of automated retrieval and natural language understanding for suggestion mining. Apart from their own experiences, understanding and knowledge, people depend on the online community to form their own opinions and readily look for suggestions and tips from the other customers. The extracted suggestions and tips are equivalent to a set of effective guidelines for the other customers before they make their own decisions. The fellow users can avail more information, and hence the decision taken would be better. This is often beyond the sense conveyed by aspect based sentiment analysis (Thet et al., 2010; Gupta et al., 2015; Gupta and Ekbal, 2014). Suggestions and feedbacks are also an important component of the market survey performed by the companies to drive innovation, change and improvements. This task is a prerequisite to other nuanced tasks which include classifying the domain of the suggestion, identifying the other arguments of the suggestions (finding the entity towards whom the suggestion is directed, identifying the aspects regarding which a suggestion has been made, finding the word boundaries of the suggestive expressions), and aggregation of such suggestions from multiple sources to comprehend a customer friendly summary. We summarize the contributions of our proposed work as follows: \u2022 We develop a linguistically motivated hybrid neural architecture to identify the review sentences that carry an intention of suggestion. \u2022 We employ semi-supervised learning (selftraining) along with a deep learning based supervised classification approach. This gives us the opportunity to harness the treasure of huge (unlabeled) data available in the form of customer reviews. To the best of our knowledge, this is the very first attempt in this direction to handle the target problem. \u2022 Outperforming the current state-of-the-art customer-to- customer suggestion mining techniques and setting up a new state-of-theart. 2 Related works The field of suggestion classification and customer feedback analysis are relatively new in the area of Natural Language Processing (NLP) and Text Mining. Our work is most closely related to the prior research as reported in (Negi and Buitelaar, 2015; Negi et al., 2016). In (Negi and Buitelaar, 2015) authors defined the annotation guidelines for customer-to-customer suggestion mining. They trained a support vector machine (SVM) classifier over the features relevant for classification in the domains of hotels and electronics reviews. They used heuristic features, features extracted from the Part-of-Speech (PoS) tags, sequential pattern mining features, sentiment features and the features extracted from the dependency relations. In their subsequent work, (Negi et al., 2016) demonstrated the improved performance using Convolutional neural networks (CNN) (Kim, 2014) and Long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) based deep learning architectures to solve this problem. They experimented with both in-domain and crossdomain training data, and also compared their performance with a SVM based classifier trained with the same set of features similar to (Negi and Buitelaar, 2015). There are some other existing works for suggestion mining, beyond customer-to-customer suggestions. (Ngo et al., 2017) developed a binary classification model based on Maximum Entropy and CNN for filtering suggestion intents in Vietnamese conversational texts like posts, comments, reviews, messages chat and spoken texts. Brun and Hagege (Brun and Hagege, 2013) developed a feature-based suggestion mining system for the domain of product reviews. (Dong et al., 2013) performed suggestion mining on tweets of the customers regarding Microsoft Windows\u2019 phone. A model is proposed in (Wicaksono and Myaeng, 2013) which focused on extracting advices for the domains of travel using Hidden Markov Model (HMM) and Conditional Random Field (CRF). The work as reported in (Gupta et al., 2017) focused on classifying the customer feedback sentences of users into six classes using deep learning based models. Our proposed model differs from these existing works with respect to the problem addressed and the model developed. We have presented a very detailed comparison (in the experiments section) to the state-of-the-art system as reported in (Negi and Buitelaar, 2015; Negi et al., 2016). 3 Methodology In this section at first we discuss the various deep learning models and then semi-supervised model. 3.1 Problem Definition Given a multi-sentence review R having N sentences {s1, s2, . . . , sN } the task is to categorize each sentence si into one of the classes c \u2208 C , where C ={\u201csuggestive\u201d, \u201cnonsuggestive }. For a sentence s with a sequence w1, w2, . . . , wn\u22121, wn of n words, the associated suggestion class c can be computed as: c = argmaxyp(y|s) (1) where y \u2208 C The following example review sentence, \u201cTip if you want a beach chair at the beach or pool, go there before 9 am or so and put your magazine or towel on your chair.\u201d is a \u201csuggestion\u201d intent directed towards a fellow customer. Here the expression of the intent is explicitly conveyed in the form of a review sentence with imperative mood1 . The \u201cnon-suggestive\u201d sentences instead contain statements and facts (e.g. (1) \u201cWe stayed in the Westin Grand Berlin in July 2007.\u201d) or expressions of one\u2019s sentiments (e.g. (2) \u201cBut the rooms are small and not very functional.\u201d). An interesting thing to note is that the second example has implicit suggestions for the fellow customers as well as the service provider (hotel owner). The other visiting customers are implicitly advised against renting the rooms of the hotel as they are small and have less utility. Moreover, this review sentence also consists of an implicit suggestion to the hotel owner to offer larger rooms to their customers, and also improve the functionalities that they provide. However in our work, we only deal with the suggestions which are very explicitly mentioned, and that too directed specifically to the fellow customers. 3.2 Proposed Deep Learning Model The customer-to-customer suggestion mining task requires recognizing specific syntactic and semantic constructions represented in texts. It should be able to capture the constructions representing imperative moods, and identify the patterns or phrases which are highly correlated with suggestive sentences in a review. It should also have a way for deep semantic understanding of text in order to disambiguate suggestions from the sentences which appear like suggestions on the surface. We propose a hybrid model consisting of two deep learning based encoders designed to integrate different views or representations of the review sentences, and a linguistically motivated feature set. The information from the encoders along with linguistic knowledge are effectively combined with the help of a multi-layer perception (MLP) network. This is done to achieve higher abstraction necessary for a complex task like identifying the suggestive review sentence. Specifically, we use two different encoders, namely Convolutional Neural Network (CNN) and attention based Recurrent Neural Network (RNN). The effectiveness of CNN and RNN based encoder has been proven in other NLP tasks (Gupta et al., 2018d; Maitra et al., 2018; Gupta et al., 2018a,c). The CNN encoder uses multiple fully-connected over the convolution layer while the RNN encoder uses a LSTM layer with the attention (Raffel and Ellis, 2015) followed by multiple fully-connected layers. An overview of the architecture for suggestion mining is shown in Figure 1. 3.2.1 Linguistic Features We use the following set of linguistic features in our model. We use slightly modified subset of features from (Negi and Buitelaar, 2015) and similar to (Gupta et al., 2018b) Suggestive keywords : The suggestive keywords are usually associated with the texts containing actual suggestions. We use the following small set of suggestive keywords: advice, suggest, may, suggestion, ask, warn, recommend, do, advise, request, warning, tip, recommendation, not, should, can, would, will A binary-valued feature is defined that checks 1 Imperative mood is a category or form of a verb which expresses a request or a command. For example, \u201cGet ready\u201d Figure 1: The proposed model architecture for customer suggestion mining whether the current word is one of the keywords or not (1-presence, 0-absence). N-gram features : We extract the most frequent 300 unigrams, 100 bigrams, and 100 trigrams from the training set. These are then used as a bag of n-gram features. Part-of-Speech (PoS) N-gram features : We extract the most frequent PoS unigrams, bigrams and trigrams of size 50. These are then used as a bag of PoS n-grams features. Imperative mood features : Most of the suggestions containing sentences have imperative mood. We try to capture this phenomenon by introducing the features obtained from the dependency trees2 . We use the following imperative mood features: 1. Base verb (VB) at the beginning of sentence or without nsubj arc: In many imperative sentences, the subject (denoted by nsubj) is absent, i.e. it implies to be the second person. Moreover, the clause containing the suggestive expression begins with the base form of the verb (denoted by VB). Hence, this does not have any dependency relation with nsubj. This feature is illustrated in Figure 2. 2. \u2018nsubj\u2019 dependency relation features: The pair of PoS tags of the words connected by the dependency arc \u2018nsubj\u2019 is used as the bag of PoS feature. We describe the presence of this feature in Figure 3 and 4. 2We use spaCy dependency parser. For visualization, we used Stanford dependency parser Figure 2: Presence of \u2018VB\u2019 without nsubj arc Figure 3: nsubj dependency arc relations. From this dependency tree the extracted features are (VBP, PRP). This set of linguistic features are fed into a multilayer perceptron having two hidden layers of size 150 and 25, respectively. 3.2.2 Recurrent and CNN Encoders The words in the sequence {w1, w2 . . . wn} from a given review sentence s are mapped to their corresponding word vectors {x1, x2 . . . xn}. The word embeddings are obtained through the publicly available3 GloVe word embeddings(Pennington et al., 2014) of dimension 300 and trained on the Common Crawl. The recurrent encoder uses a LSTM network (hidden size 64) over the embedded sequences and it then applies an internal attention over the hidden states. The LSTM network is able to process the sentence as a sequence, with the ability to capture long term dependencies. Thus the hidden layers 3 https://nlp.stanford.edu/projects/glove/ Figure 4: nsubj dependency arc relations. Here, (VB, PRP) and (VBP, PRP) features are active. can efficiently perform composition over the local context, and help to identify patterns which are found in suggestive sentences. The attention mechanism then finds salient contexts and aggregates the important ones to build the context vector. The motivation for using attention stems from the fact that suggestive expressions can be identified in a short span of text within the sentence and the attention can effectively attend to those specific contexts encoded by LSTM. The Attention layer is followed by dense layers with 150, and 25 neurons, each having ReLU activations and a dropout value of 0.2. The convolutional layer applies 250 one dimensional CNN filters of size 5 over the embeddings. The global max pooling is applied separately for the feature map obtained from each filter, and it helps to identify the presence of the n-gram feature corresponding to that feature in the sentence. The following dense layer with 250 neurons (ReLU activation and 0.75 dropout) helps to non-linearly compose multiple such features, thus giving itself an opportunity to learn a more diverse set of features. 3.2.3 Hybrid Model The extracted linguistic features, the recurrent encoder representation and the convolutional encoder representation are concatenated (into a feature set p) and fed to a fully-connected layer with two neurons, followed by softmax activation. The softmax layer outputs the probability of the given review sentence being suggestive or nonsuggestive. The probability that the output class y\u02c6 is i given the sentence s and parameters \u03b8 is computed as: P(\u02c6y = i|s, \u03b8) = sof tmaxi(p Twi + zi) = e p T wi+zi PK k=1 e p T wk+zk (2) where zk and wk are the bias and weight vector of the k th labels, p is the concatenated feature set, and K is the number of total classes (i.e. 2). \u03b8 is the set of all the parameters of the model. The system predicts the most probable class. 3.3 Semi-supervised Model Semi-supervised learning makes use of both labeled (small) and unlabeled (huge) data for designing a more efficient classifier, as compared to the traditional supervised learning. We utilize self-training algorithm (Zhu, 2006), also known as bootstrapping, which can be flexibly used as a wrapper over any supervised learning algorithm. We use our hybrid model for this semi-supervised learning. In self-training, we iteratively train a classifier enhancing each time the original training dataset with newly labeled instances. At the end of each iteration, the classifier is made to predict on the unlabeled dataset and 100 most confidently predicted instances of each class is added to the training data, with the predicted labels as the true labels. For self-training, a methodology similar to early stopping is applied, with a maximum of six iterations. We stop the iteration when the F1-Score on the validation data 4 does not improve over the existing best model in consecutive three iterations, saving only the best performing model for testing. For example in Fig. 5, the training terminates after the 6th iteration, and the model trained in the 3rd iteration is chosen for the final evaluation. Effect of adding unlabeled data to training for the electronics domain is depicted in Figure 6. Figure 5: Scores on the validation set during selftraining: Hotel domain. For this semi-supervised setting, the crossentropy error is minimized using the Adam Optimizer, and the training is stopped as the validation loss stops decreasing (early stopping). Because of the class imbalance (cf. Table 1), the loss function 4A part of the training set was used for validation. Figure 6: Scores on the validation set during selftraining: Electronics domain. weighs the loss for the positive class instances 10 times more than the loss for the negative class instances. All the other configurations are similar to the supervised setting5 . 6 Conclusion and Future Work In this paper, we have proposed a hybrid deep learning model for the task of suggestion mining by incorporating richer and diverse representations of the inputs. We have also used self-training algorithm, which even improved the performance of the hybrid model, opening up more opportunities for the use of semi-supervised learning for this task. Experiments on benchmark datasets show that we obtain superior performance over the existing state-of-the-art system. In the future, we would like extend our work to other semisupervised learning algorithms.", "SumGPT3": "Received all parts. Thank you! Now, let's summarize the paper:\n\nIntroduction:\nThe paper addresses the task of suggestion mining within the realm of natural language processing. It highlights the prevalence of online platforms where people express opinions and provide suggestions across various domains like social media, e-commerce, and forums. The motivation lies in the need to extract valuable suggestions from unstructured text data to aid decision-making and drive innovation.\n\nNovelty:\n1. Linguistically Motivated Hybrid Architecture: The paper proposes a novel hybrid neural architecture that integrates linguistic features with deep learning techniques to identify review sentences containing suggestions. This approach aims to capture both syntactic and semantic constructions representing imperative moods and patterns highly correlated with suggestive sentences.\n2. Semi-supervised Learning: A self-training algorithm, also known as bootstrapping, is employed for semi-supervised learning. This iterative process enhances the original training dataset with newly labeled instances predicted by the model on unlabeled data. This approach improves classifier efficiency by leveraging both labeled and unlabeled data, thereby outperforming traditional supervised learning methods.\n\nConclusion:\nThe paper presents a comprehensive exploration of suggestion mining, offering significant contributions to the field of natural language processing. The proposed hybrid model, combined with semi-supervised learning, demonstrates superior performance compared to existing state-of-the-art systems. The findings suggest promising avenues for future research in extending the application of semi-supervised learning algorithms to similar tasks.\n\nThe summary encapsulates the paper's focus on introducing a novel hybrid architecture and leveraging semi-supervised learning for improved suggestion mining, highlighting its relevance and potential impact in the field of NLP.", "SumGem": "## Summary of Scientific Paper on Suggestion Mining in Customer Reviews\n\nThis paper proposes a novel deep learning model to address customer-to-customer suggestion mining in online reviews. The model aims to identify sentences that express suggestions directed towards fellow customers.\n\n**Novelty:**\n\n* **Hybrid Model:** The model combines two deep learning encoders (Recurrent Neural Network and Convolutional Neural Network) with a set of linguistic features to capture various aspects of suggestive sentences. This hybrid approach allows for more comprehensive representation of the text data compared to previous works that rely solely on one type of encoder or features.\n* **Self-training for Semi-supervised Learning:** The paper introduces a semi-supervised learning approach using self-training to improve the model's performance. This approach leverages a large amount of unlabeled data along with a smaller set of labeled data to enhance the model's ability to identify suggestions.\n\n**Conclusion**\n\nThe paper demonstrates that the proposed hybrid deep learning model with self-training achieves better results on customer-to-customer suggestion mining tasks compared to existing state-of-the-art methods. The authors suggest that future work can explore the application of other semi-supervised learning algorithms for this task.", "SumGPT4": "ntroduction\nThe paper presents a novel deep learning model designed to enhance suggestion mining from customer reviews across various online platforms. Suggestion mining is framed as an extension of opinion mining but focuses specifically on extracting actionable suggestions rather than general sentiments or opinions.\n\nNovelty\n\nHybrid Neural Architecture: The proposed model utilizes a hybrid architecture that combines Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with attention mechanisms. This dual approach leverages the spatial hierarchical feature extraction capabilities of CNNs and the sequence handling strengths of RNNs to identify suggestion-related patterns in text more effectively.\nLinguistic Feature Integration: Unlike purely data-driven approaches, the model incorporates linguistically motivated features such as n-grams, Part-of-Speech tags, and imperative mood indicators derived from dependency parsing. This integration aims to enhance the model's ability to discern the nuanced linguistic structures typical of suggestive content.\nSemi-Supervised Learning Approach: To capitalize on the vast amounts of unlabeled data available in customer reviews, the model employs a semi-supervised learning strategy using self-training. This approach incrementally incorporates confidently predicted unlabeled data, improving the model's learning efficacy and adaptability to diverse textual contexts.\nFocus on Explicit Suggestions: The model is specifically tuned to identify explicitly stated suggestions directed at fellow customers, which is a narrower and more defined task compared to previous broader suggestion mining efforts. This focus allows for more precise and actionable output useful for businesses and consumers alike.\nConclusion\nThe paper introduces a cutting-edge approach to suggestion mining, combining advanced neural network architectures, strategic integration of linguistic knowledge, and innovative semi-supervised learning techniques. These contributions significantly advance the capability to mine suggestions from text effectively, offering promising directions for future research in NLP applications within consumer feedback analysis. The model not only sets a new benchmark for this specific task but also opens avenues for applying similar methodologies to other domains of text analysis.", "rankGPT3": "2", "rankGem": "3", "rankGPT4": "1", "expGPT3": "This summary is also well-written and covers most of the important points from the paper. It effectively summarizes the introduction, novelty, and conclusion of the paper, highlighting the hybrid architecture and semi-supervised learning approach. However, it lacks some details compared to Summary 3, particularly regarding the specific components of the hybrid architecture and the linguistic features integrated into the model. Nonetheless, it still provides a solid overview of the paper's focus and contributions.", "expGem": "This summary ranks the lowest primarily because it lacks detail and clarity compared to the other summaries. While it briefly mentions the hybrid model and semi-supervised learning approach, it doesn't provide enough information about these aspects to fully understand the paper's contributions. Additionally, the language used in this summary is somewhat vague and lacks specificity, making it harder to grasp the key points of the paper. Overall, it could benefit from more elaboration and clarity to improve its ranking.", "expGPT4": "This summary ranks the highest because it provides a clear and comprehensive overview of the paper's content. It covers all key aspects of the proposed model, including its architecture, linguistic features, semi-supervised learning approach, and focus on explicit suggestions. The summary effectively communicates the novelty of the proposed model and its potential impact in the field of NLP. It also highlights how the model differs from previous approaches and its specific contributions to suggestion mining. Overall, this summary offers a well-rounded understanding of the paper's content and its significance."}, "4": {"title": "Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks", "url": "https://aclanthology.org/2020.emnlp-main.258", "text": "1 Introduction\nOver the past several years, neural network approaches have rapidly gained popularity in natural language processing (NLP) because of their\nimpressive performance and flexible modeling capacity. Nevertheless, symbolic rules are still an\nindispensable tool in various industrial NLP applications. Regular expressions (RE) are one of the\nmost representative and useful forms of symbolic\nrules and are widely used for solving tasks such as\npattern matching (Hosoya and Pierce, 2001; Zhang\net al., 2018) and intent classification (Luo et al.,\n2018). RE-based systems are highly interpretable\n\u2217Corresponding author.\nand therefore support fine-grained human inspection and manipulation. For example, individual RE\nrules in a system can be easily added, revised, or\nremoved to quickly adapt the system to changes\nin the task specification. Moreover, RE-based systems do not require a training stage with labeled\ndata and hence can be quickly deployed with decent\nperformance in zero-shot scenarios. However, REs\nrely on human experts to write and often have high\nprecision but moderate to low recall; RE-based\nsystems cannot evolve by training on labeled data\nwhen available and thus usually underperform neural networks in rich-resource scenarios.\nHow to combine the advantages of symbolic\nrules and neural networks is an open question and\nis drawing increasing attention recently. One possible way is to use rules to constrain neural networks,\nusually in the manner of regularization via knowledge distillation (Hu et al., 2016) and multi-task\nlearning (Awasthi et al., 2020; Xu et al., 2018),\nor by tuning the output logits of neural networks\n(Li and Srikumar, 2019; Luo et al., 2018). In this\nway, information from rules can be injected into\nneural networks, though the neural networks still\nrequire training and remain black boxes that are\nhard to interpret and manipulate. Another way of\nutilizing rules is to design novel neural network\narchitectures inspired by rule systems (Schwartz\net al., 2018; Graves et al., 2014; Peng et al., 2018;\nLin et al., 2019). Models designed based on this\nidea usually achieve better interpretability, but they\nmust be trained on labeled data and cannot be directly converted from rules or manually specified\nby human experts because of their structural differences from rule systems.\nIn this paper, we propose finite-automaton recurrent neural networks (FA-RNN), a novel type of\nrecurrent neural networks that is designed based\non the computation process of weighted finite-state\nautomata. Because of the equivalence between\n3194\nLabel [distance]\nRE $*(how ( far | long ) | distance) $*\nMatched\nText\nhBOSi tell me how far is oakland airport from downtown hEOSi\nFA\nTable 1: RE for matching sentences asking about distance, and a matched sentence. \u2018$\u2019 is the wildcard. \u2018|\u2019\nis the OR operator. \u2018*\u2019 is the Kleene star operator. We\nalso show the finite automaton converted from the RE.\ns2 is the final state.\nREs and finite-state automata, we can convert any\nREs into an FA-RNN, which can be deployed in\nzero-shot and cold-start scenarios. When there are\nlabeled data, the FA-RNN can also be trained in\nthe same way as any neural network, which improves its prediction accuracy over the original REs.\nThe FA-RNN has good interpretability. When converted from REs, it is (approximately) equivalent to\nthe REs and is fully interpretable. Even after training, it often remains highly interpretable and can\nbe converted back into REs. The interpretability\nof FA-RNNs opens the possibility of fine-grained\nmanipulation such as integrating new REs into a\ntrained FA-RNN and disabling old REs that are\nused to initialize an FA-RNN.\nWe apply FA-RNNs to the text classification task\nand compare them with neural network baselines\nas well as existing approaches of integrating REs\nand neural networks. Our experiments find that FARNNs show clear advantages in both zero-shot and\nlow-resource settings and remain very competitive\nin rich-resource settings.\n2 Background\n2.1 Regular Expressions\nRegular expressions (RE) are patterns usually used\nfor searching or matching a string and are a succinct\nway to denote regular languages. We show a simple\nexample RE for matching sentences1\nin Table 1.\n2.2 RE System for Text Classification\nThe text classification task aims to assign a class\nlabel to an input sentence. Let x = hx0, \u00b7 \u00b7 \u00b7 , xN i\nbe a sentence and L = {l1, \u00b7 \u00b7 \u00b7 , lk} be the label set.\nOne common and straight-forward way to use REs\n1Example taken from the ATIS intent classification dataset.\nFigure 1: RE and FA-RNN systems for text classification.\nfor classification is as follows. Firstly, write m REs\nR = {r1, \u00b7 \u00b7 \u00b7 , rm}, where each RE corresponds\nto some label in L. Then, for each sentence x,\napply these REs to get matching results. Finally,\naggregate the matching results to produce a final\nlabel for sentence x based on a set of propositional\nlogic rules. Each rule specifies a logical expression\nof matching results that implies a specific label.\nFor example, let Mi represent whether RE ri\nis\nmatched, then we may have a rule: (Mi \u2228 Mj ) \u2227\n\u00acMk \u2192 lp. The whole procedure is shown in the\ntop half of Figure.1.\n2.3 Finite-State Automaton\nFinite-state automata (FA) are machines with finite\nnumbers of states. An FA can transit from one state\nto another in response to an input. It has a start state\ns0 and a set of final states S\u221e. Every RE can be\nconverted into an FA expressing the same language\nby Thompson\u2019s construction algorithm (Thompson,\n1968). For a sequence x = hx1, \u00b7 \u00b7 \u00b7 , xN i, an RE\nmatches the sequence if and only if the converted\nFA starts from s0 and finally reaches a final state\nafter consuming x. Table 1 shows an FA converted\nfrom the example RE. Further, for every RE, there\nexists a unique FA with a minimum number of\nstates and deterministic transitions (m-DFA) such\nthat they express the same language (Hopcroft et al.,\n2001). Deterministic transitions mean that given\na current state and an input, there is a unique next\nstate. The m-DFA can be obtained by running\nthe powerset construction algorithm (Rabin and\nScott, 1959) and the DFA minimization algorithm\n(Hopcroft, 1971).\n3195\n2.4 Weighted Finite-State Automaton\nA weighted finite-state automaton (WFA) assigns a\nweight to each transition, which is formally defined\nas a 5-tuple: A = h \u03a3, S, T , \u03b10, \u03b1\u221e i .\n\u2022 \u03a3: a finite input vocabulary. |\u03a3| = V .\n\u2022 S: a finite set of states. |S| = K.\n\u2022 T \u2208 R\nV \u00d7K\u00d7K: a tensor of transition weights.\nT [\u03c3, i, j] is the weight of transiting from si\nto sj\nin response to input \u03c3. T [\u03c3] \u2208 R\nK\u00d7K denotes\nthe transition matrix of \u03c3.\n\u2022 \u03b10 \u2208 R\nK: initial weights. \u03b10[i] is the weight\nof staying at state si at time t = 0.\n\u2022 \u03b1\u221e \u2208 R\nK: final weights. \u03b1\u221e[i] is the weight\nof staying at state si after reading all the inputs.\nAn FA can be seen as a WFA with 0/1 weights.\nT [\u03c3, i, j] is 1 if si can transit to sj in response\nto \u03c3 and 0 otherwise. \u03b10[i] = 1{si \u2208 S0} and\n\u03b1\u221e[i] = 1{si \u2208 S\u221e}, where 1() is the indicator\nfunction and S0 denote the set of start states2\n.\nFor sequence x, the score of WFA A accepting\nx can be calculated using the forward (Baum and\nPetrie, 1966) and Viterbi (Viterbi, 1967) algorithms.\nLet path p = hu1, \u00b7 \u00b7 \u00b7 , uN+1i be a sequence of\nindexes of the states that we visit when consuming\nx. The score B(A, p) of path p can be computed\nby:\n\u03b10[u1] \u00b7\n Y\nN\ni=1\nT [xi, ui, ui+1]\n!\n\u00b7 \u03b1\u221e[uN+1] (1)\nLet \u03c0(x) be the set of all paths that start from\nstart state s0 and reach a final state si \u2208 S\u221e after\nconsuming sequence x. The forward algorithm\ncomputes the sum of path scores.\nBforward(A, x) = X\np\u2208\u03c0(x)\nB(A, p)\n= \u03b1\nT\n0\n\u00b7\n Y\nN\ni=1\nT [xi]\n!\n\u00b7 \u03b1\u221e\n(2)\nThe Viterbi algorithm computes the maximum of\npath scores.\nBViterbi(A, x) = max\np\u2208\u03c0(x)\nB(A, p)\n(3)\nIt can be computed by replacing matrix multiplication in Eqa.2 with the max-plus operator. For\nan FA A, the forward score is exactly the number\nof paths in \u03c0(x) while the Viterbi score indicates\nwhether \u03c0(x) is non-empty.\n2Normally, we define that an FA has only one start state,\nbut any FA with multiple start states can be converted into\nan FA with one start state by adding \u000f-transitions from a new\nstart state to all the original start states.\n3 Method\nWe show step-by-step how we can convert REs to\na novel type of recurrent neural networks called\nFA-RNNs.\n3.1 From REs to Recurrent Neural Networks\nRE to FA As mentioned in Sec.2.3, we can convert an RE into an m-DFA. In order to obtain a\nconcise FA with better interpretability and faster\ncomputation speed, we treat the wildcard \u2018$\u2019 as a\nspecial word in the vocabulary and run the algorithms mentioned in Sec.2.3 to obtain a \u201cpseudo\u201d\nm-DFA A.\nFA as RNN As discussed in Sec.2.4, the FA A\ncan be seen as a WFA with 0/1 weights which is\nparameterized by \u0398 = h\u03b10, T , \u03b1\u221ei.\nThe computation of the WFA forward score\n(Eqa.2) can be rewritten into a recurrent form. Let\nht \u2208 R\nK be the forward score vector after consuming t words in x. ht\n[i] can be interpreted as the\nnumber of paths starting from s0 and reaching si\nat step t.\nh0 = \u03b1\nT\n0\nht = ht\u22121 \u00b7 T [xt\n], 1 \u2264 t \u2264 N\nBforward(A, x) = hN \u00b7 \u03b1\u221e\n(4)\nThe computation of the WFA Viterbi score can be\nformulated in a similar way. Therefore, we can\nview a WFA as a form of recurrent neural networks\n(RNN) parameterized by \u0398.\n3.2 Decomposing the Parameter Tensor\nDespite the equivalence to FAs and hence better\ninterpretability, the RNNs proposed in Sec.3.1 has\nmuch more parameters than a traditional RNN\nbecause of the tensor T \u2208 R\nV \u00d7K\u00d7K. To reduce the parameter number, we propose to apply\ntensor rank decomposition (explained in the Appendix.A) and decompose T into three matrices\nER \u2208 R\nV \u00d7r\n, D1 \u2208 R\nK\u00d7r\n, D2 \u2208 R\nK\u00d7r\n, where r\nis a hyper-parameter. Note that if r is smaller than\nthe rank of T , then the decomposition is approximate. We empirically find that, for a 100-state FA\nconverted from RE, we can obtain a small decomposition error (\u2264 1%) if r \u2265 100.\nNow the RNN is parameterized by \u0398D =\nh\u03b10, \u03b1\u221e, ER, D1, D2i. ER has a dimension associated with vocabulary size V and can be viewed\nas a word embedding matrix containing RE information for each word. Let vt \u2208 R\nr be the embedding of word xt contained in ER. The recurrent\n3196\nupdate in Eqa.4 becomes:\na = (ht\u22121 \u00b7 D1) \u25e6 vt\nht = a \u00b7 DT\n2\n(5)\nwhere \u25e6 denotes element-wise product. Eqa.5 produces the same result as Eqa.4 with sufficiently\nlarge r.\nNote that the size of ht is determined by the state\nnumber K of the m-DFA converted from RE. In\nsome cases, K may be too small, resulting in limited representational power of the RNN. A simple\nmethod to solve this problem is to concatenate D1\nand D2 with a K0\u00d7r zero matrix, hence increasing\nthe hidden state size by K0\n. Subsequent training (to\nbe introduced later) would update D1 and D2 so\nthat these added dimensions can be utilized. This\nis equivalent to adding K0\nisolated states in the\nFA and relying on training to establish transitions\nbetween the old and new states.\n3.3 Integrating Pretrained Word Embedding\nPretrained word embeddings have been found very\nuseful in bringing external lexical knowledge into\nneural networks. Let Ew \u2208 R\nV \u00d7D be the word\nembedding matrix and ut \u2208 R\nD be the word embedding of xt\nin Ew. We introduce another matrix\nG \u2208 R\nD\u00d7r\nthat can transform the D-dimensional\nword embedding ut\ninto r-dimension, which can\nthen replace vt\nin the recurrent update of Eqa.5. We\ninitialize G by setting G = E\n\u2020\nwER, where E\n\u2020\nw is\nthe pseudo-inverse of Ew. In this way, we approximate vt with utG and hence the initialized RNN\nstill tries to mimic the FA. After training, however,\nthe RNN will be able to utilize the additional information contained in pretrained word embeddings\nand hence may outperform the original FA.\nIn practice, we find it beneficial to interpolate\nthe two r-dimension embeddings vt and utG with\na hyper-parameter \u03b2 \u2208 [0, 1]. When \u03b2 is 1, we only\nuse RE information. When \u03b2 gets closer to 0, we\nintegrate more external lexical information into the\nmodel. The recurrent update formula becomes:\nzt = \u03b2vt + (1 \u2212 \u03b2)utG\na = (ht\u22121 \u00b7 D1) \u25e6 zt\nht = a \u00b7 DT\n2\n(6)\nWe name this new form of RNNs as FA-RNNs, i.e.,\nrecurrent neural networks built from finite-state\nautomata.\n3.4 Extensions of FA-RNN\nGated Extension (FA-GRU) Inspired by the\nGated Recurrent Unit (Chung et al., 2014), we sacrifice some interpretability and add an update gate\nft and a reset gate rt\ninto the FA-RNN. The update\ngate determines how much information from the\npast shall be retained. The reset gate determines\nwhether to reset the previous score vector to h0,\nThe recurrent update is as follows.\nzt = \u03b2vt + (1 \u2212 \u03b2)utG\nft = \u03c3(Wfzt + Ufht\u22121 + bf )\nrt = \u03c3(Wrzt + Urht\u22121 + br)\nh\u02c6t\u22121 = (1 \u2212 rt) \u25e6 h0 + rt \u25e6 ht\u22121\na = (h\u02c6t\u22121 \u00b7 D1) \u25e6 zt\nh\u02c6t = a \u00b7 DT\n2\nht = (1 \u2212 ft) \u25e6 ht\u22121 + ft \u25e6 h\u02c6t\n(7)\n\u03c3 is the sigmoid activation function and Wf , Wr,\nUf , Ur are additional parameters for gates. Note\nthat when ft and rt\nis close to 1, the FA-GRU degenerates to the FA-RNN. Therefore, we initialize\nbf , br to a large value and Wf ,Wr, Uf , Ur randomly using Xavier initialization (Glorot and Bengio, 2010) to ensure that the initialized FA-GRU\nis approximately equivalent to the FA-RNN and\nhence the original REs.\nBidirectional Extension (BiFA-RNN) Our networks can be easily extended to their bidirectional\nvariants. For any RE, we can reverse it by simply reversing its word order (e.g., \u201cfree $* ( phone\n| phones ) $*\u201d can be reversed to \u201c$* (phone |\nphones) $* free\u201d) and then convert the reversed\nRE into WFA\n\u2190\u2212A and the corresponding FA-RNN.\nScore vector\n\u2190\u2212\nhN can be computed by applying\nEqa.6 or Eqa.7 on the reversed input sentence \u2190\u2212x .\nThen we take the average of\n\u2190\u2212\nhN and the left-toright score vector\n\u2212\u2192hN to obtain the final score vector hN = (\u2212\u2192hN +\n\u2190\u2212\nhN )/2.\n3.5 Aggregation Layer for Text Classification\nAs introduced in Sec.2.2, an RE system for text\nclassification contains multiple REs that are aggregated to form a class label prediction. Here we\ndescribe how to convert such an RE system to an\nFA-RNN system for text classification (the bottom\nhalf of Figure.1).\nFor each RE ri\nin the RE system, we convert it\ninto a WFA Ai with Ki states, start weights \u03b10,i,\nand final weights \u03b1\u221e,i. We can view these WFAs\n3197\nLogic Soft Logic\n\u00acA 1 \u2212 a\nA \u2228 B min(1, a + b)\nA \u2227 B max(0, a + b \u2212 1)\nTable 2: Soft logic. A, B are proposition symbols with\nsoft truth values a, b.\nas a single WFA\nP\nA with a total number of K =\ni Ki states and multiple start states. We then\nconvert this WFA to an FA-RNN. After we run this\nFA-RNN on sentence x, the last state vector hN\ncontains the matching information of all the REs.\nTo predict a class label from hN , we create a\nsoft aggregation layer. First, we extract the forward\nor Viterbi score of each RE from hN . For forward\nscoring, we follow Eqa.4 and have:\nBforward(Ai\n, x) = hN \u00b7 \u03b1\u00af\u221e,i (8)\nwhere \u03b1\u00af\u221e,i expands \u03b1\u221e,i by filling zeros for states\nnot belonging to Ai\n. For Viterbi scoring, we replace matrix multiplication with max-plus. The\ncomputed score for Ai can be seen as a soft matching result of RE ri\n. Second, we rewrite the logical\nRE aggregation rules introduced in Sec.2.2 to soft\nlogic expressions (Kimmig et al., 2012; Li and\nSrikumar, 2019) (Table 2). Instead of predicting\na single label, the soft aggregation layer outputs\nthe label logits l \u2208 R\nk\n. When all the elements in\nhN are close to either 0 or 1, the output of the soft\naggregation layer is approximately equivalent to\nthat of the RE aggregation layer of Sec.2.2.\nSince the logical RE aggregation rules can be\nexpressed in the conjunctive normal form, we can\nimplement the corresponding soft aggregation layer\nwith a two-layer MLP with ReLU-like activation\nfunctions. This is similar to the MLP layer commonly used at the end of traditional neural networks to map the hidden representation to label\nlogits. In practice, we find it sometimes beneficial\nto not use any activation function in the MLP.\n3.6 Training with Labeled Data\nSo far we have introduced how to initialize an FARNN system that is approximately equivalent to an\nRE classification system. When there are labeled\ndata, the FA-RNN can also be trained to improve its\nperformance. We simply use the output logits l to\ncompute the cross-entropy loss on the training data\nand use a gradient-based method such as Adam\n(Kingma and Ba, 2014) to optimize it.\n#Train #Dev #Test |L| |R| K %Acc\nATIS 3982 996 893 26 27 107 87.0\n$ * flights | flight | ( ( go | get | fly ) from $ * to $\n* ) $ * \u2192 FLIGHT\nQC 4965 500 500 6 68 94 64.4\n$ * what $ ? does $+ ( stand? for ) $* \u2192\nABBREVIATION\nSMS 4502 500 500 2 36 52 93.2\n$* free $ * ( phone | phones ) $* \u2192 SPAM\nTable 3: Dataset statistics and example REs. L is the\nlabel set. R is the RE set. K is the state number of\nthe converted WFA. %Acc is the classification accuracy\nof the RE system. We provide an example RE and its\ntargeting label for each dataset.\nATIS QC SMS\nRE system 87.01 64.40 93.20\nFA-RNN 86.53 61.95 93.00\nFA-GRU 86.81 62.90 93.20\nBiFA-RNN 88.10 62.90 93.00\nBiFA-GRU 88.63 62.90 93.20\nBiGRU+i 1.34 18.75 11.90\nBiGRU+o 30.74 27.50 30.40\nBiGRU+io 38.69 25.70 73.25\nBiGRU+pr 9.94 17.70 53.00\nBiGRU+kd 9.94 17.70 53.00\nBiGRU+i+u 86.42 64.85 92.75\nBiGRU+o+u 83.03 64.95 93.05\nBiGRU+io+u 86.14 64.75 92.70\nBiGRU+pr+u 85.67 64.60 93.5\nBiGRU+kd+u 87.37 63.70 93.55\nTable 4: Accuracy of zero-shot classification. The RE\nsystem and baselines trained on RE-labeled data are included for reference.\nNote that we typically fix ER during training\nbecause we find that updating ER is not helpful.\nTherefore, the number of trainable parameters in\nan FA-RNN is similar to (usually smaller than) that\nof an RNN. We compare the number of parameters\nof different models in Appendix.C.\n7 Related Work\nNeural Networks Enhanced by Rules Hu et al.\n(2016); Li and Rush (2020) use rules to constrain\nneural networks by knowledge distillation and posterior regularization. Awasthi et al. (2020) inject\nrule knowledge into neural networks using multitask learning. Lin et al. (2020) train a trigger matching network using additional annotation and use the\noutput of trigger matching results as the attention\nof a sequence labeler. Rockt\u00e4schel et al. (2015);\nXu et al. (2018); Hsu et al. (2018) use parsed rule\nresults to regularize neural network predictions by\nadditional loss terms. Li and Srikumar (2019); Luo\net al. (2018) inject declarative knowledge in the\nform of parsed RE results or first-order expressions\ninto neural networks by hacking the prediction logits or the attention scores. Hu et al. (2016); Hsu\net al. (2018) use rules as additional input features.\n3201\nAll these previous methods use matching results\nor truth values of rules to enhance existing neural\nmodels. In contrast, we directly turn REs into a\nnovel type of trainable networks.\nRelating Neural Networks and WFA Schwartz\net al. (2018) propose a type of neural networks for\nlearning soft surface patterns (a subset of REs),\nwhich is inspired by WFAs but cannot be converted\nfrom WFAs or surface patterns. In contrast, our\nFA-RNN can be initialized from REs and converted\nback to REs. Peng et al. (2018); Dodge et al. (2019)\nformulate the update of each hidden dimension of\nvarious RNN architectures as a small WFA (2-4\nstates). Weiss et al. (2018); Merrill (2019) provide\ntheoretical analysis of various neural networks and\ntheir accepting languages. Our work differs from\nthese more theoretical studies in that we aim for a\npractical text classification approach. Omlin et al.\n(1998); Giles et al. (1999) show the equivalence\nbetween WFA and second-order RNN. The main\ndifferences between our model and theirs include\nthe following. First, compared with the undecomposed version of our FA-RNN, their RNN model\ninvolves nonlinear activation functions which complicate the model. Second, our FA-RNN further\ndecomposes the tensor parameter, integrate word\nembeddings, and propose the gated and bidirectional extensions. Third, while their work is mostly\ntheoretical, we empirically show the usefulness of\nour model in text classification.\n8 Conclusion and Future Work\nWe propose a type of recurrent neural networks\ncalled FA-RNN. It can be initialized from REs\nand can also learn from data, hence applicable to\nvarious scenarios including zero-shot, cold-start,\nlow-resource and rich-resource scenarios. It is\nalso interpretable and can be converted back into\nREs. Our experiments on text classification show\nthat it outperforms previous neural approaches in\nboth zero-shot and low-resource scenarios and is\nvery competitive in rich-resource scenarios. In the\nfuture, we plan to apply FA-RNN to other tasks\nand explore other variants of FA-RNN. We release\nour data, RE rules and code at https://github.com/\njeffchy/RE2RNN.", "SumGPT3": "Introduction\nThe paper presents a novel type of recurrent neural networks, named finite-automaton recurrent neural networks (FA-RNN), which combines the advantages of regular expressions (RE) and neural networks. The approach aims to leverage the interpretability and zero-shot capabilities of REs with the learning power of neural networks.\n\nNovelty\nIntegration of REs into Neural Networks:\n\nThe FA-RNN model is directly initialized from REs by converting them into weighted finite-state automata (WFAs). This allows the system to be deployed in zero-shot and cold-start scenarios without requiring labeled data.\nThe conversion from REs to FA-RNNs enables the system to inherit the interpretability of REs. Even after training, the model remains interpretable and can be converted back into REs for fine-grained manipulation.\nDecomposition of Transition Weights:\n\nThe transition weights in the FA-RNN are decomposed using tensor rank decomposition, reducing the number of parameters and improving computational efficiency while maintaining interpretability.\nIntegration of Pretrained Word Embeddings:\n\nFA-RNNs are enhanced by integrating pretrained word embeddings, which injects external lexical knowledge into the model. This hybrid approach allows the model to leverage both RE patterns and rich semantic information from embeddings.\nSoft Logic Aggregation for Classification:\n\nThe paper introduces a soft logic aggregation layer for text classification tasks, translating logical RE aggregation rules into soft logic expressions. This layer is implemented as a two-layer MLP, facilitating seamless integration with neural network architectures.\nExtensions with Gated Mechanisms and Bidirectionality:\n\nThe FA-RNN is extended with gated mechanisms, similar to Gated Recurrent Units (FA-GRU), and bidirectional processing (BiFA-RNN), enhancing the model's ability to retain and utilize contextual information.\nConclusion\nThe FA-RNN framework offers a novel way to combine the strengths of symbolic rules and neural networks. It shows significant advantages in zero-shot and low-resource settings and remains competitive in rich-resource environments. The approach maintains interpretability and flexibility, bridging the gap between rule-based systems and trainable neural networks. Future work will explore applying FA-RNN to other tasks and further variants of the model.", "SumGem": "Introduction\n\nThis paper proposes a novel type of recurrent neural networks (RNN) called FA-RNN. FA-RNNs are designed to address the limitations of existing methods for integrating symbolic rules and neural networks for text classification tasks.\n\nNovelty\n\nFA-RNNs can be initialized from regular expressions (REs), making them applicable to zero-shot and cold-start scenarios where labeled data is scarce.\nFA-RNNs are interpretable and can be converted back into REs, allowing for human inspection and manipulation of the learned model.\nFA-RNNs can be trained on labeled data to improve their performance over the original REs.\nFA-RNNs decompose the parameter tensor to reduce the number of parameters compared to traditional RNNs.\nFA-RNNs integrate pretrained word embeddings to incorporate external lexical knowledge.\nFA-RNNs have gated and bidirectional extensions for improved performance.\nConclusion\n\nThe experiments show that FA-RNNs outperform previous neural approaches in both zero-shot and low-resource scenarios and are very competitive in rich-resource scenarios. FA-RNNs are a promising approach for combining the strengths of symbolic rules and neural networks for text classification tasks.", "SumGPT4": "**Introduction**  \nThe paper introduces Finite-Automaton Recurrent Neural Networks (FA-RNN), a novel neural network model designed to merge the advantages of symbolic rule-based systems, specifically regular expressions (RE), with the adaptability and learning capabilities of recurrent neural networks (RNNs). Regular expressions are widely used for tasks like pattern matching and intent classification due to their high interpretability and ability to quickly adapt to changes without needing a labeled training dataset. However, they often suffer from limitations such as low recall and cannot evolve based on new data, which neural networks can overcome.\n\n**Novelty**  \n1. **Hybrid Model Design**: The FA-RNN model uniquely integrates the structure and interpretability of finite automata (FA) with the learning prowess of RNNs. This integration allows the model to be initialized from existing REs, preserving their interpretability while also enabling the learning from data.\n2. **Decomposition and Efficiency**: The model incorporates tensor rank decomposition to reduce the parameter space significantly compared to traditional RNNs. This makes the FA-RNN both computationally efficient and easier to interpret.\n3. **Extensions and Adaptability**: The FA-RNN framework includes gated and bidirectional extensions, enhancing its utility and performance across different scenarios. These extensions allow for flexibility in maintaining past information and improving prediction accuracy.\n4. **Soft Logic Aggregation**: A novel aggregation layer uses soft logic to integrate results from multiple REs, enabling a smooth transition between traditional hard logic used in rule-based systems and the probabilistic outputs typical of neural networks.\n\n**Conclusion**  \nFA-RNN stands out by effectively combining rule-based systems' clarity with neural networks' dynamic learning capabilities. It demonstrates superior performance in both zero-shot and low-resource scenarios while remaining competitive in richer resource settings. Future work will explore broader applications and further variants, enhancing its utility in diverse NLP tasks. This approach not only bridges the gap between symbolic and neural methods but also opens up new avenues for creating highly interpretable and adaptable NLP systems.", "rankGPT3": 1, "rankGem": 3, "rankGPT4": 2, "expGPT3": "Comprehensive Coverage: This summary provides a detailed explanation of the FA-RNN model, covering all the novel aspects such as initialization from REs, parameter decomposition, integration of pretrained word embeddings, soft logic aggregation, and extensions with gated mechanisms and bidirectionality.\nClarity and Structure: It is well-structured with clear sections for each novel contribution, making it easy to understand the innovations and their significance.\nConclusion: The summary effectively concludes by highlighting the strengths of the FA-RNN framework in different scenarios and hints at future research directions.\nContextual Integration: It integrates the context and motivation behind the research, explaining why combining REs with neural networks is beneficial.", "expGem": "Brevity: This summary is brief and less detailed compared to the other two. It covers the main points but does not delve into the specifics of each novel aspect.\nStructure: The structure is less clear, making it harder to follow the logical flow of the contributions and innovations of the FA-RNN model.\nConclusion: The conclusion is relatively weaker, not providing as strong a sense of the model's overall impact and potential future directions.\nMissing Details: Key innovations like soft logic aggregation and the detailed benefits of extensions are not explicitly mentioned, which makes it less informative.", "expGPT4": "Conciseness: This summary is concise and captures the key points of the FA-RNN model, including its hybrid design, decomposition for efficiency, extensions, and soft logic aggregation.\nFocused: It is focused and avoids unnecessary details, making it a good choice for readers who want a quick yet comprehensive understanding of the paper.\nConclusion: The conclusion is strong, emphasizing the model's ability to combine rule-based clarity with dynamic learning capabilities and suggesting future work.\nLack of Depth: However, it lacks some depth compared to Summary 1, especially in explaining specific technical aspects and contributions in more detail."}, "5": {"title": "Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation", "url": "https://aclanthology.org/2022.emnlp-main.181", "text": "1 Introduction\nCompositional, hierarchical, and recursive processing are widely believed to be essential traits\nWork done while at Ant Group. To contact Haitao,\nhaitaomi@global.tencent.com\n1The code is available at: https://github.com/\nalipay/StructuredLM_RTDT\nof human language across diverse linguistic theories (Chomsky, 1956, 2014). Chart-based models (Maillard et al., 2017; Kim et al., 2019a; Drozdov et al., 2019; Hu et al., 2021) have made promising progress in both grammar induction and hierarchical encoding in recent years. The differential CKY encoding architecture of Maillard et al.\n(2017) simulates the hierarchical and recursive process explicitly by introducing an energy function\nto combine all possible derivations when constructing each cell representation. However, this entails\na cubic time complexity, which makes it impossible to scale to large language model training like\nBERT (Devlin et al., 2018). Simultaneously, its\ncubic memory cost also limits the tree encoder\u2019s\nability to draw on huge parameter models as a backbone.\nHu et al. (2021) introduced a heuristic pruning\nmethod, successfully reducing the time complexity\nto a linear number of compositions. The experiments show that chart-based models exhibit great\npotential for grammar induction and representation learning when applying a sophisticated tree\nencoder such as Transformers with large corpus\npretraining, leading to a Recursive Transformer\nbased on Differentiable Trees, or R2D2 for short.\nHowever, R2D2\u2019s heuristic pruning approach is\nrule-based and only considers certain composition\nprobabilities. Thus, trees constructed in this way\nare not guaranteed to be globally optimal. Moreover, as each step during pruning is based on previous decisions, the entire encoding process is sequential and thus slow in the inference stage.\nIn this work, we resolve these issues by proposing a unified method with a new global pruning\n2809\nstrategy based on a lightweight and fast top-down\nparser. We cast parsing as split point scoring,\nwhere we first encode the input sentence with a\nbi-directional LSTM, and score all split points in\nparallel. Specifically, for a given sentence, the\nparser first scores each split point between words\nin parallel by looking at its left and right contexts,\nand then recursively splits a span (starting with the\nwhole sentence) into two sub-spans by picking the\nhighest-scoring split point among the current split\ncandidates. Subsequently, the reverse order of the\nsorted split points can serve as the merge order\nto guide the pruning of the CKY encoder, which\nenables the encoder to search for more reasonable\ntrees. As the gradient of the pretrained component\ncannot be back-propagated to the parser, inspired\nby URNNG (Kim et al., 2019b), we optimize the\nparser by taking trees sampled from the CKY chart\ntable generated by the encoder as ground truth.\nThus, the parser and the chart-based encoder promote each other in this way back and force just like\nthe strategy and value networks in AlphaZero (Silver et al., 2017). Additionally, the pretrained tree\nencoder can compose sequences recursively in parallel according to the trees generated by the parser,\nwhich makes Fast-R2D2 a Recursive Neural Network (Pollack, 1990; Socher et al., 2013) variant.\nIn this paper, we make the following main contributions:\n1. We propose an architecture to jointly pretrain\nparser and encoder of a recursive network\nin linear memory cost. Experiments show\nthat our pretrained parser outperforms models\ncustom-tailored for grammar induction.\n2. By encoding in parallel following trees generated by the top-down parser, Fast-R2D2 significantly improves the inference speed 30 to\n50 fold compared to R2D2.\n3. We pre-train Fast-R2D2 on a large corpus and\nevaluate it on downstream tasks. The experiments demonstrate that a pretrained recursive\nmodel based on an unsupervised parser significantly outperforms pretrained sequential\nTransformers (Vaswani et al., 2017) with the\nsame parameter size in single sentence classification tasks.\n2 Preliminaries\n2.1 R2D2 Architecture\nDifferentiable Trees. R2D2 follows the work\nof Maillard et al. (2017) in defining a CKYstyle (Cocke, 1969; Kasami, 1966; Younger, 1967)\nencoder. For a sentence S = {s1, s2, ..., sn} with\nn words or word-pieces, it defines a chart table\nas illustrated in Figure 1. In the table, each cell\nTi,j is a tuple \u27e8ei,j , pi,j , pei,j \u27e9, where ei,j is a vector\nrepresentation, pi,j is the probability of a single\ncomposition step, and pei,j is the probability of the\nsubtree for the span [i, j] over the sub-string si:j .\nWhen i equals j, the table has terminal nodes Ti,i\nwith ei,i initialized with the embeddings of input\ntokens si\n, while pi,i and pei,i are set to one. When\nj > i, the representation ei,j is a weighted sum of\nintermediate combinations c\nk\ni,j , defined as:\nc\nk\ni,j , pk\ni,j = f(ei,k, ek+1,j ) (1)\npe\nk\ni,j = p\nk\ni,j pei,k pek+1,j (2)\n\u03b1i,j = GUMBEL(log(pei,j )) (3)\nei,j = [c\ni\ni,j , ci+1\ni,j , ..., c\nj\u22121\ni,j ] \u03b1i,j (4)\n[pi,j , pei,j ] = \u03b1\n\u22ba\ni,j [pi,j , pei,j ] (5)\nk is a split point from i to j \u2212 1, f(\u00b7) is an n-layer\nTransformer encoder, p\nk\ni,j and pe\nk\ni,j denote the single\nstep combination probability and the subtree probability, respectively, at split point k, pi,j and pei,j\nare the concatenation of all p\nk\ni,j or pe\nk\ni,j values, and\nGUMBEL is the Straight-Through Gumbel-Softmax\noperation of Jang et al. (2017) with temperature set\nto one. As GUMBEL picks the optimal splitting\npoint k at each cell in practice, it is straightforward\nto recover the complete derivation tree from the\nroot node T1,n in a top-down manner recursively.\nFigure 1: Chart data structure. There are two alternative ways of generating T1,3: combining either\n(T1,2, T3,3) or (T1,1, T2,3).\nHeuristic pruning. As shown in Figure 2, R2D2\nstarts to prune if all cells beneath height m have\nbeen encoded. The heuristic rules work as follows:\n1. Recover the maximum sub-tree for each cell\nat the m-th level, and collect all cells at the\n2nd level that appear in any sub-tree.\n2810\nFigure 2: Example of chart pruning and encoding process. With R2D2\u2019s original heuristic pruning, cells\nto merge are selected according to local composition probabilities. For better model-based pruning, we\npropose selecting cells according to the merge order estimated by a top-down parser.\n2. Rank candidates in Step 1 by the composition\nprobability pi,j , and pick the highest-scoring\ncell as a non-splittable span (e.g., T1,2).\n3. Remove any invalid cells that would break the\nnow non-splittable span from Step 2, e.g., the\ndark cells in (c), and reorganize the chart table\nmuch like in the Tetris game as in (d).\n4. Encode the blank cells at the m-th level, e.g.,\nthe cell highlighted with stripes in (d), and\ngo back to Step 1 until the root cell has been\nencoded.\nPretraining. To learn meaningful structures without gold trees, Hu et al. (2021) propose a selfsupervised pretraining objective. Similar to the\nbidirectional masked language model task, R2D2\nreconstructs a given token si based on its context\nrepresentation e1,i\u22121 and ei+1,n. The probability of\neach token is estimated by the tree encoder defined\nin R2D2. The final objective is:\nmin\n\u03b8\nXn\ni=1\n\u2212 log p\u03b8(si\n| e1:i\u22121, ei+1:n) (6)\n3 Methodology\n3.1 Global Pruning Strategy\nWe propose a top-down parser based on syntactic\ndistance (Shen et al., 2018) to evaluate scores for\nall split points in a sentence and generate a merge\norder according to the scores.\nTop-down parser. Given a sentence S =\n{s1, s2, ..., sn}, there are n\u22121 split points between\nwords. We define a top-down parser by giving confidence scores to all split points as follows:\nv = [v1, v2, ..., vn\u22121] = f(S; \u03b8) (7)\nTo keep it simple and rigorously maintain linear\ncomplexity, we select bidirectional LSTMs as the\nbackbone, though Transformers are also an option.\nAs shown in Figure 3, first, a bi-directional LSTM\nFigure 3: (a) A parsed tree obtained by sorting split\nscores (vi). (b) A sampled tree by adding Gumbel\nnoise (gi\nin dark vertical bars).\nencodes the sentence, and then, for each split point,\nan MLP over the concatenation of the left and right\ncontext representations yields the final split scores.\nFormally, we have:\n\u2212\u2192h ,\n\u2190\u2212h = BiLSTM(E; \u03b8)\nvi = LayerNorm(MLP(\u2212\u2192h i \u2295\n\u2190\u2212h i+1))\n(8)\nHere, E is the embedding of the input sentence\nS, while \u2212\u2192h and \u2190\u2212h denote the forward and reverse representation, respectively. vi\nis the score\nof the i-th split point, whose left and right context\nrepresentations are \u2212\u2192h i and \u2190\u2212h i+1. Given scores\n[v1, v2, ..., vn\u22121], one can easily recover the binary\ntree shown in Figure 3: We recursively split a span\n(starting with the entire sentence) into two subspans by picking the split point with the highest\nscore in the current span. Taking the sentence in\nFigure 3 (a) as an example, we split the overall sentence at split point 3 in the first step, which leads to\ntwo sub-trees over s1:3 and s4:6. Then we split s1:3\n2811\nat 2 and s4:6 at 4. We can continue this procedure\nuntil the complete tree has been recovered.\nTree sampling. In the training stage, we\nperform sampling over the computed scores\n[v1, v2, ..., vn\u22121] in order to increase the robustness\nand exploration of our model. Let P\nt denote the list\nof split points at time t in ascending order, which\nis {1, 2, 3, ..., n\u22121} in the first step. Then a particular split point at\nis selected from P\nt by sampling\nbased on the probabilities estimated by stacking of\nsplit points scores. The sampled {a1, a2, ..., an\u22121}\ntogether form the final split point sequence A. At\neach time step, we remove at from P\nt when at\nis\nselected, then sample the next split point until the\nset of remaining split points is empty. Formally,\nwe have:\nat \u223c softmax(v\nt\n) (9)\nP\nt+1 = P\nt\n{at} (10)\nwhere v\nt\nis concatenation of vi\nin P\nt\n. As the\nGumbel-Max trick (E.J.Gumbel, 1954; Maddison\net al., 2014) provides a simple and efficient way\nto draw samples from a categorical distribution\nwith class probabilities, we can obtain at via the\nGumbel-Max trick as:\nat = argmax\ni\n[vi + gi\n], i \u2208 Pt\n, (11)\nwhere gi\nis the Gumbel noise for the i-th split\npoint. Therefore, the aforementioned process is\nequivalent to sorting the original sequence of split\npoints scores with added Gumbel noise. Figure 3\n(b) shows a sampled tree with respect to the split\npoint scores. The split point sequence A can hence\nbe obtained simply as:\nA = argsort\ni\n(v + g) (12)\nHere, argsort sorts the array in descending order\nand returns the indices of the original array. The\nsampled A is {2, 4, 3, 5, 1} in Figure 3 (b).\nSpan Constraints. As word-pieces (Wu et al.,\n2016) and Byte-Pair Encoding (BPE) are commonly used in pretrained language models, it is\nstraightforward to incorporate multiple word-piece\nconstraints into the top-down parser to reduce wordlevel parsing errors. We denote a list of span\nconstraints composed of beginning and end positions of non-split-table spans as C, defined as\nC = {(b1, e1),(b2, e2), ...,(bn, en)}. For each\n(bi\n, ei) in C, there should be a sub-tree for a span\ncovering the sub-string sbi:ei\n. This goal can be\nachieved by simply adjusting the scores of all splits\nwithin the spans in C by \u2212\u03b4. To make them smaller\nthan the scores of span boundaries, \u03b4 could be defined as (max(v) \u2212 min(v) + c), where c could be\nany positive number.\nModel-based Pruning. We denote the reverse\norder of the split point sequence A as M and then\ntreat M as a bottom-up merge order inferred by\nthe top-down parser based on the global context.\nSubsequently, the complete pruning process is as\nfollows:\n1. Pick the next merge index by invoking Alg 1.\n2. Perform Steps 3 and 4 in the heuristic pruning\npart in Section 2.1\nAs shown in Figure 2, we still retain the threshold and the pruning logic of R2D2, but we select\ncells to merge according to M instead of following heuristic rules. Specifically, given a shrinking\nchart table, we select the next merge index among\nthe second row by popping and modifying M in\nAlgorithm 1.\nAlgorithm 1 Next merge index in the second row\n1: function NEXT-INDEX(M)\n2: i = pop(M) \u25b7 Index\n3: for j \u2208 1 to M.len do\n4: if Mj > i then \u25b7 Merging at left\n5: Mj = Mj \u2212 1 \u25b7 Shift left\n6: return i\nTake the example in Figure 3 (b) for instance: M\nstarts with {1, 5, 3, 4, 2}. Then we merge the first\ncell in the second row in Figure 2 (b), and obtain a\nnew M = {4, 2, 3, 1}. In the next round, we treat\nthe 4th cell covering s5:6 as a non-splittable cell in\nFigure 2 (e), and M becomes {2, 3, 1}.\n3.2 Optimization\nWe denote the tree probabilities estimated by the\ntop-down parser and R2D2 as p\u03b8(z|S), q\u03d5(z|S), respectively. The difficulty here is that while q\u03d5(z|S)\nmay be optimized by the objective defined in Equation 6, there is no gradient feedback for p\u03b8(z|S).\nTo make p\u03b8(z|S) learnable, an intuitive solution is\nto fit p\u03b8(z|S) to q\u03d5(z|S) by minimizing their Kullback\u2013Leibler distance. While the tree probabilities\nof both distributions are discrete and not exhaustive, inspired by URNNG (Kim et al., 2019b), a\nMonte Carlo estimate for the gradient with respect\n2812\nto \u03b8 can be defined as:\n\u25bd\u03b8 KL[q\u03d5(z|S) \u2225 p\u03b8(z|S)]\n=\u25bd\u03b8Ez\u223cq\u03d5(z|S)[log q\u03d5(z|S)\np\u03b8(z|S)\n]\n\u2248 \u2212 \u25bd\u03b8\n1\nK\nXK\nk=1\nlog p\u03b8(z\n(k)\n|S)\n(13)\nwith samples z\n(1)\n, ..., z\n(K)\nfrom q\u03d5(z|S). Algorithm 2 shows the complete sampling process from\nq\u03d5(z|S). Specifically, we sample split points with\ncorresponding span boundaries recursively as in\nprevious work (Goodman, 1998; Finkel et al., 2006;\nKim et al., 2019b) with respect to the intermediate tree probabilities calculated during hierarchical\nencoding.\nAlgorithm 2 Top-down tree sampling for R2D2\n1: function SAMPLE(T1,n) \u25b7 Root cell\n2: Q = [T1,n]\n3: K = []\n4: while Q is not empty do\n5: T = pop(Q)\n6: i, j = T .i, T .j \u25b7 Start/end indices\n7: L = T .splits \u25b7 m splits at most\n8: \u03c4 = 0\n9: for k \u2208 1 to len(L) do\n10: wk = pe\nL[k]\ni,j \u25b7 Using Equation 2\n11: \u03c4 = \u03c4 + wk \u25b7 Sum up all wk\n12: idx \u223c Cat([w1/\u03c4, ..., wlen(L)/\u03c4 ])\n13: \u25b7 Sample a split point\n14: push(K,(L[idx], i, j))\n15: \u25b7 Keep the split point and span boundary\n16: if L[idx] > i then \u25b7 Add left child\n17: push(Q, Ti,L[idx])\n18: if L[idx] + 1 < j then \u25b7 Add right child\n19: push(Q, TL[idx]+1,j )\n20: return K\nA sequence of split points and corresponding\nspans is returned by the sampler. For the k-th sample z\n(k)\n, let p\u03b8(a\nk\nt\n|S) denote the probability of taking a\nk\nt\nas split from span (i\nk\nt\n, jk\nt\n) at the t-th step.\nFormally, we have:\np\u03b8(a\nk\nt\n|S) = e\nv\nak\nt\ne\nv\nik\nt + ... + e\nv\njk\nt\nlog p\u03b8(z\n(k)\n|S) =\nnX\u22121\nt=1\nlog p\u03b8(a\nk\nt\n|S),\n(14)\nwhere i\nk\nt\nand j\nk\nt denote the start and end of the\ncorresponding span. Please note here that the vi\nare not adjusted by span constraints.\n3.3 Downstream Tasks\nInference. In this paper, we mainly focus on classification tasks as downstream tasks. We consider\nthe root representation as representing the entire\nsentence. As we have two models pre-trained in\nour framework \u2013 an R2D2 encoder and a top-down\nparser \u2013 we have two ways of generating the representations:\na) Run forced encoding over the binary tree from\nthe top-down parser with the R2D2 encoder.\nb) Use the binary tree to guide the pruning of the\nR2D2 encoder, and take the root representation e1,n.\nIt is obvious that the first approach is much faster\nthan the latter one, as the R2D2 encoder only runs\nn \u2212 1 times in forced encoding, and can run in parallel layer by layer, e.g., we may run compositions\nat a5, a3, and a4 in parallel in Figure 3 (b). We explore both of these approaches in our experiments.\nTraining Objectives. As suggested in prior work\n(Radford et al., 2018; Howard and Ruder, 2018;\nGururangan et al., 2020), given a pretrained model,\ncontinued pretraining on an in-domain corpus with\nthe same pretraining objective can yield a better\ngeneralization ability. Thus, we simply combine\nour pretraining objectives via summation in all\ndownstream tasks. At the same time, as the downstream task may guide R2D2 to more reasonable\ntree structures, we still maintain the KL loss to\nenable the parser to continuously update. For the\ntwo inference methods, we uniformly select the\nroot representation e1,n as the representation for a\ngiven sentence followed by an MLP, and estimate\nthe cross-entropy loss, denoted as Lforced and Lcky,\nrespectively. Let LKL denote the KL loss described\nin Section 3.2 and Lbilm denote the bidirectional\nlanguage model loss described in Eq 6. The final\nloss is:\nL = Lforced + Lcky + Lbilm + LKL (15)\n5 Related Work\nMany attempts have been done in grammar induction and hierarchical encoding. Clark (2001) and\nKlein and Manning (2002) provided some of the\nfirst successful statistical approaches to grammar\ninduction. There have been multiple recent papers\n2https://github.com/huggingface/\ntransformers\n3https://github.com/alipay/\nStructuredLM_RTDT/tree/r2d2\n2816\nthat focus on structure induction based on language\nmodeling objectives (Shen et al., 2019a,b, 2021;\nKim et al., 2019a). Pollack (1990) proposed to use\nRvNN as a recursive architecture to encode text\nhierarchically, and Socher et al. (2013) showed the\neffectiveness of RvNNs with gold trees for sentiment analysis. In this work, we focus on models\nthat are capable of learning meaningful structures\nin an unsupervised way and encoding text over the\ninduced tree hierarchically.\nIn the line of work on learning a sentence representation with structures, Yogatama et al. (2017)\njointly train their shift-reduce parser and sentence\nembedding components without gold trees. As\ntheir parser is not differentiable, they have to resort\nto reinforcement training, resulting in increased\nvariance, which may easily collapse to trivial left or\nright branching trees. Gumbel-Tree-LSTMs (Choi\net al., 2018) construct trees by recursively selecting two terminal nodes to merge and learning\nthe composition probability via downstream tasks.\nCRvNN (Chowdhury and Caragea, 2021) makes\nthe entire process end-to-end differentiable and\nparallel by introducing a continuous relaxation.\nURNNG (Kim et al., 2019b) propose the first architecture to jointly pretrain parser and encoder based\non RNNG (Dyer et al., 2016). However, it has\nO(n\n3\n) complexity and remains unable to improve\nupon a right-branching baseline when punctuation\nis removed. Maillard et al. (2017) propose an alternative approach, based on a differentiable CKY\nencoding. The algorithm is differentiable due to a\nsoft-gating approach, which approximates discrete\ncandidate selection by a probabilistic mixture of\nthe constituents available in a given cell of the chart.\nWhile their work relies on annotated downstream\ntasks to learn structures, Drozdov et al. (2019) propose a novel auto-encoder-like pretraining objective based on the inside\u2013outside algorithm (Baker,\n1979; Casacuberta, 1994).\n6 Conclusion\nIn this paper, we have presented Fast-R2D2, which\nimproves the performance and inference speed of\nR2D2 by introducing a fast top-down parser to\nguide the pruning of the R2D2 encoder. Pretrained\non the same corpus, Fast-R2D2 significantly outperforms sequential Transformers with a similar\nscale of parameters on classification tasks. Experimental results show that Fast-R2D2 is a promising\nand feasible way to learn hierarchical text representations, which is different from layer stacking\nmodels and can also generate interpretable intermediate representations. As future work, we are\ninvestigating leveraging the intermediate representations in additional downstream tasks.", "SumGPT3": "Introduction\n\nThis paper presents Fast-R2D2, a novel approach designed to enhance the performance and inference speed of the Recursive Transformer based on Differentiable Trees (R2D2). The paper builds on previous efforts in grammar induction and hierarchical encoding, addressing the computational limitations of existing chart-based models and proposing a new global pruning strategy.\n\nNovelty\n\nGlobal Pruning Strategy: The authors introduce a lightweight and fast top-down parser that evaluates scores for all split points in a sentence. This parser enables the generation of a merge order for pruning the CKY encoder, improving the efficiency and accuracy of tree construction.\n\nParallel Encoding: Fast-R2D2 leverages parallel processing by scoring all split points in parallel and recursively splitting spans based on the highest-scoring split points. This significantly reduces the time complexity and allows for faster inference compared to R2D2.\n\nUnified Pretraining: The paper proposes a method to jointly pretrain the parser and encoder with linear memory cost. This approach not only enhances the grammar induction capabilities but also allows for more efficient use of pretrained components in downstream tasks.\n\nImproved Inference Speed: By using the binary tree generated by the top-down parser to guide pruning, Fast-R2D2 achieves 30 to 50 times faster inference speed than R2D2, making it more practical for large-scale applications.\n\nEnhanced Performance on Downstream Tasks: The experiments demonstrate that Fast-R2D2 outperforms sequential Transformers of similar parameter sizes in single-sentence classification tasks, highlighting its effectiveness in learning hierarchical text representations.\n\nConclusion\n\nFast-R2D2 represents a significant advancement in recursive neural network architectures for language modeling. By introducing a fast top-down parser and parallel encoding strategy, the approach addresses the computational inefficiencies of previous models, offering substantial improvements in inference speed and performance. The proposed method not only enhances grammar induction but also provides a promising direction for future research in hierarchical text representation and its applications in various downstream tasks.", "SumGem": "Introduction\n\nThis paper proposes Fast-R2D2, a novel neural network architecture that addresses the limitations of prior work in hierarchical text encoding, specifically focusing on grammar induction and achieving efficient inference speeds.\n\nNovelty\n\nFast-R2D2 introduces several key aspects of novelty:\n\nJointly pretrained parser and encoder: Unlike prior work that relies on annotated downstream tasks or reinforcement learning, Fast-R2D2 jointly trains a top-down parser and a differentiable CKY encoder in an unsupervised manner. This enables Fast-R2D2 to learn meaningful structures directly from the training corpus.\nFast top-down parser for pruning: The architecture incorporates a fast top-down parser that guides the pruning process of the CKY encoder. This significantly improves the inference speed compared to prior approaches like R2D2, which relies on heuristic pruning rules.\nUnsupervised hierarchical representation learning: Fast-R2D2 achieves unsupervised learning of hierarchical text representations, contrasting with previous methods that require gold trees or supervision from downstream tasks. This allows Fast-R2D2 to capture inherent structural information within the text data.\nConclusion\n\nFast-R2D2 demonstrates superior performance and efficiency compared to sequential Transformers with similar parameter scales on text classification tasks. The ability to learn hierarchical representations and generate interpretable intermediate outputs makes Fast-R2D2 a promising approach for various NLP applications. Future work explores leveraging these intermediate representations for further downstream tasks.", "SumGPT4": "### Introduction\nThe paper discusses advancements in compositional, hierarchical, and recursive processing in language models, specifically through the development of a new model termed Fast-R2D2. This model enhances the traditional Recursive Transformer based on Differentiable Trees (R2D2) by introducing a global pruning strategy that integrates a top-down parser to optimize tree construction, thus aiming to address the scalability issues of previous chart-based models due to their cubic time complexity.\n\n### Novelty\nThe key novelties presented in the paper are:\n1. **Global Pruning Strategy**: Unlike the local heuristic pruning of R2D2, Fast-R2D2 employs a global pruning strategy with a lightweight and fast top-down parser. This parser evaluates all possible split points in a sentence simultaneously and uses these scores to guide the recursive construction of syntax trees, thereby enabling more efficient parsing and encoding processes.\n2. **Integration of Bidirectional LSTMs with a Top-Down Parser**: Fast-R2D2 utilizes a bi-directional LSTM to initially encode the sentence, which is then used by a multi-layer perceptron to score split points. This approach allows for the parallel scoring of split points, contrasting with the sequential dependency of previous models.\n3. **Enhanced Inference Speed**: By leveraging the trees generated by the top-down parser, Fast-R2D2 significantly accelerates inference speed, achieving improvements of 30 to 50 times faster than its predecessor. This is enabled by its ability to run compositional operations in parallel.\n4. **Robustness through Tree Sampling**: The model incorporates robustness in its learning process by sampling tree structures during training, using a method inspired by the Gumbel-Max trick, which facilitates exploration of different tree configurations.\n\n### Conclusion\nFast-R2D2 presents a substantial improvement over previous hierarchical encoding models by integrating a novel global pruning strategy with a top-down parser that significantly enhances processing efficiency and model scalability. It demonstrates superior performance in single sentence classification tasks compared to traditional sequential Transformers of similar parameter size, thereby providing a promising approach for learning hierarchical text representations in a more interpretable and efficient manner. Future work will explore the potential of these intermediate representations in other downstream tasks.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "Clarity and Organization: This summary is also well-organized and covers key points under clear headings (Introduction, Novelty, Conclusion).\nCoverage of Key Points: It provides a thorough overview of the innovations introduced by Fast-R2D2, including global pruning strategy, parallel encoding, unified pretraining, and improved inference speed.\nDetail and Precision: While it covers all major points, it lacks some of the specific technical details that Summary 3 includes, such as the use of bi-directional LSTMs and the Gumbel-Max trick.\nTechnical Depth: The summary is clear and precise but could benefit from a bit more technical detail to better illustrate the methods and their implications.", "expGem": "Clarity and Organization: This summary is less structured compared to the other two. It presents the content in a less organized manner, making it harder to follow.\nCoverage of Key Points: Although it mentions several novel aspects, such as the jointly pretrained parser and encoder and the fast top-down parser for pruning, it does not clearly highlight the global pruning strategy or the specific methods used for parallel encoding.\nDetail and Precision: The summary is more general and lacks specific details that would help in understanding the technical innovations and their impact.\nTechnical Depth: It is less detailed and precise, which makes it less effective in conveying the full scope of the paper\u2019s contributions.", "expGPT4": "Clarity and Organization: This summary is well-organized, with clear sections for Introduction, Novelty, and Conclusion. Each section provides a focused and concise explanation of the paper\u2019s content.\nCoverage of Key Points: It effectively captures all major aspects of the paper, including the introduction of the global pruning strategy, the use of bi-directional LSTMs, enhanced inference speed, and robustness through tree sampling.\nDetail and Precision: The summary goes into sufficient detail to explain the novel contributions and how they compare to previous methods. It mentions specific techniques (e.g., Gumbel-Max trick) and provides a clear understanding of the improvements in efficiency and performance.\nTechnical Depth: It balances technical depth with readability, making it accessible without oversimplifying."}, "6": {"title": "Larger-Context Language Modelling with Recurrent Neural Network", "url": "https://aclanthology.org/P16-1125", "text": "1 Introduction\nThe goal of language modelling is to estimate the\nprobability distribution of various linguistic units,\ne.g., words, sentences (Rosenfeld, 2000). Among\nthe earliest techniques were count-based n-gram\nlanguage models which intend to assign the probability distribution of a given word observed af-\n\u2217Recently, (Ji et al., 2015) independently proposed a similar approach.\nter a fixed number of previous words. Later Bengio et al. (2003) proposed feed-forward neural\nlanguage model, which achieved substantial improvements in perplexity over count-based language models. Bengio et al. showed that this neural language model could simultaneously learn the\nconditional probability of the latest word in a sequence as well as a vector representation for each\nword in a predefined vocabulary.\nRecently recurrent neural networks have become one of the most widely used models in language modelling (Mikolov et al., 2010). Long\nshort-term memory unit (LSTM, Hochreiter and\nSchmidhuber, 1997) is one of the most common\nrecurrent activation function. Architecturally, the\nmemory state and output state are explicitly separated by activation gates such that the vanishing gradient and exploding gradient problems described in Bengio et al. (1994) is avoided. Motivated by such gated model, a number of variants\nof RNNs (e.g. Cho et al. (GRU, 2014b), Chung\net al. (GF-RNN, 2015)) have been designed to easily capture long-term dependencies.\nWhen modelling a corpus, these language models assume the mutual independence among sentences, and the task is often reduced to assigning a probability to a single sentence. In\nthis work, we propose a method to incorporate\ncorpus-level discourse dependency into neural language model. We call this larger-context language model. It models the influence of context by defining a conditional probability in the\nform of P(wn|w1:n\u22121, S), where w1, ..., wn are\nwords from the same sentence, and S represents\nthe context which consists a number of previous\nsentences of arbitrary length.\nWe evaluated our model on four different corpora (IMDB, BBC, Penn TreeBank, and Fil9).\nOur experiments demonstrate that the proposed\nlarger-context language model improve perplex1319\nity for sentences, significantly reducing per-word\nperplexity compared to the language models without context information. Further, through Part-OfSpeech tag analysis, we discovered that content\nwords, including nouns, adjectives and verbs, benefit the most from increasing number of context\nsentences. Such discovery led us to the conclusion that larger-context language model improves\nthe unconditional language model by capturing the\ntheme of a document.\nTo achieve such improvement, we proposed a\nlate fusion approach, which is a modification to\nthe LSTM such that it better incorporates the discourse context from preceding sentences. In the\nexperiments, we evaluated the proposed approach\nagainst early fusion approach with various numbers of context sentences, and demonstrated the\nlate fusion is superior to the early fusion approach.\nOur model explores another aspect of contextdependent recurrent language model. It is novel\nin that it also provides an insightful way to feed\ninformation into LSTM unit, which could benefit\nall encoder-decoder based applications.\n2 Statistical Language Modelling with\nRecurrent Neural Network\nGiven a document D = (S1, S2, . . . , SL) which\nconsists of L sentences, statistical language modelling aims at computing its probability P(D).\nIt is often assumed that each sentence in the\nwhole document is mutually independent from\neach other:\nP(D) \u2248\nY\nL\nl=1\nP(Sl). (1)\nWe call this probability (before approximation) a\ncorpus-level probability. Under this assumption of\nmutual independence among sentences, the task of\nlanguage modelling is often reduced to assigning\na probability to a single sentence P(Sl).\nA sentence Sl = (w1, w2, . . . , wTl\n) is a\nvariable-length sequence of words or tokens. By\nassuming that a word at any location in a sentence\nis largely predictable by preceding words, we can\nrewrite the sentence probability into\nP(S) = Y\nTl\nt=1\np(wt\n|w<t), (2)\nwhere w<t denotes all the preceding words. We\ncall this a sentence-level probability.\nThis rewritten probability expression can be either directly modelled by a recurrent neural network (Mikolov et al., 2010) or further approximated as a product of n-gram conditional probabilities such that\nP(S) \u2248\nY\nTl\nt=1\np(wt\n|w\nt\u22121\nt\u2212(n\u22121)), (3)\nwhere w\nt\u22121\nt\u2212(n\u22121) = (wt\u2212(n\u22121), . . . , wt\u22121). The\nlatter is called n-gram language modelling.\nA recurrent language model is composed of two\nfunctions\u2013transition and output functions. The\ntransition function reads one word wt and updates\nits hidden state such that\nht = \u03c6 (wt\n, ht\u22121), (4)\nwhere h0 is an all-zero vector. \u03c6 is a recurrent\nactivation function. For more details on widelyused recurrent activation units, we refer the reader\nto (Jozefowicz et al., 2015; Greff et al., 2015).\nAt each timestep, the output function computes\nthe probability over all possible next words in the\nvocabulary V . This is done by\np(wt+1 = w\n0\n|w\nt\n1\n) \u221d exp (gw0(ht)). (5)\ng is commonly an affine transformation:\ng(ht) = Woht + bo,\nwhere Wo \u2208 R\n|V |\u00d7d\nand bo \u2208 R\n|V |\n.\nThe whole model is trained by maximizing the\nlog-likelihood of a training corpus often using\nstochastic gradient descent with backpropagation\nthrough time (see, e.g., Rumelhart et al., 1988).\nThis conventional approach to statistical language modelling often treats every sentence in a\ndocument to be independent from each other This\nis often due to the fact that downstream tasks, such\nas speech recognition and machine translation, are\ndone sentence-wise. In this paper, we ask how\nstrong an assumption this is, how much impact this\nassumption has on the final language model quality and how much gain language modelling can get\nby making this assumption less strong.\nLong Short-Term Memory Here let us briefly\ndescribe a long short-term memory unit which is\nwidely used as a recurrent activation function \u03c6\n(see Eq. (4)) for language modelling (see, e.g.,\nGraves, 2013).\n1320\nA layer of long short-term memory (LSTM)\nunit consists of three gates and a single memory\ncell. They are computed by\nit =\u03c3 (Wixt + Uiht\u22121 + bi)\not =\u03c3 (Woxt + Uoht\u22121 + bo)\nft =\u03c3 (Wfxt + Ufht\u22121 + bf ),\nwhere \u03c3 is a sigmoid function. xt\nis the input at\ntime t. The memory cell is computed by\nct = ft \f ct\u22121 + it \f tanh (Wcx + Ucht\u22121 + bc),\nwhere \f is an element-wise multiplication. This\nadaptive leaky integration of the memory cell allows the LSTM to easily capture long-term dependencies in the input sequence.\nThe output, or the activation of this LSTM layer,\nis then computed by ht = ot \f tanh(ct).\n3 Larger-Context Language Modelling\nIn this paper, we aim not at improving the\nsentence-level probability estimation P(S) (see\nEq. (2)) but at improving the corpus-level probability P(D) from Eq. (1) directly. One thing we\nnoticed at the beginning of this work is that it is not\nnecessary for us to make the assumption of mutual\nindependence of sentences in a corpus. Rather,\nsimilarly to how we model a sentence probability,\nwe can loosen this assumption by\nP(D) \u2248\nY\nL\nl=1\nP(Sl\n|S\nl\u22121\nl\u2212n\n), (6)\nwhere S\nl\u22121\nl\u2212n = (Sl\u2212n, Sl\u2212n+1, . . . , Sl\u22121). n decides on how many preceding sentences each conditional sentence probability conditions on, similarly to what happens with a usual n-gram language modelling.\nFrom the statistical modelling\u2019s perspective, estimating the corpus-level language probability in\nEq. (6) is equivalent to build a statistical model\nthat approximates\nP(Sl\n|S\nl\u22121\nl\u2212n\n) = Y\nTl\nt=1\np(wt\n|w<t, Sl\u22121\nl\u2212n\n), (7)\nsimilarly to Eq. (2). One major difference from the\nexisting approaches to statistical language modelling is that now each conditional probability of\na next word is conditioned not only on the preceding words in the same sentence, but also on the\nn \u2212 1 preceding sentences.\nA conventional, count-based n-gram language\nmodel is not well-suited due to the issue of data\nsparsity. In other words, the number of rows in the\ntable storing n-gram statistics will explode as the\nnumber of possible sentence combinations grows\nexponentially with respect to both the vocabulary\nsize, each sentence\u2019s length and the number of\ncontext sentences.\nEither neural or recurrent language modelling\nhowever does not suffer from this issue of data\nsparsity. This makes these models ideal for modelling the larger-context sentence probability in\nEq. (7). More specifically, we are interested in\nadapting the recurrent language model for this.\nIn doing so, we answer two questions in the\nfollowing subsections. First, there is a question\nof how we should represent the context sentences\nS\nl\u22121\nl\u2212n\n. We consider two possibilities in this work.\nSecond, there is a large freedom in how we build a\nrecurrent activation function to be conditioned on\nthe context sentences. We also consider two alternatives in this case.\n3.1 Context Representation\nA sequence of preceding sentences can be represented in many different ways. Here, let us describe two alternatives we test in the experiments.\nThe first representation is to simply bag all the\nwords in the preceding sentences into a single vector s \u2208 [0, 1]|V |\n. Any element of s corresponding\nto the word that exists in one of the preceding sentences will be assigned the frequency of that word,\nand otherwise 0. This vector is multiplied from\nleft by a matrix P which is tuned together with all\nthe other parameters: p = Ps. We call this representation p a bag-of-words (BoW) context.\nSecond, we try to represent the preceding context sentences as a sequence of bag-of-words.\nEach bag-of-word sj is the bag-of-word representation of the j-th context sentence, and they are put\ninto a sequence (sl\u2212n, . . . , sl\u22121). Unlike the first\nBoW context, this allows us to incorporate the order of the preceding context sentences.\nThis sequence of BoW vectors are read by\na recurrent neural network which is separately\nfrom the one used for modelling a sentence (see\nEq. (4).) We use LSTM units as recurrent activations, and for each context sentence in the sequence, we get zt = \u03c6 (xt\n, zt\u22121), for t = l \u2212\nn, . . . , l \u2212 1. We set the last hidden state zl\u22121 of\nthis context recurrent neural network as the con1321\ntext vector p.\nAttention-based Context Representation The\nsequence of BoW vectors can be used in a bit different way from the above. Instead of a unidirectional recurrent neural network, we first use a\nbidirectional recurrent neural network to read the\nsequence. The forward recurrent neural network\nreads the sequence as usual in a forward direction,\nand the reverse recurrent neural network in the opposite direction. The hidden states from these two\nnetworks are then concatenated for each context\nsentence in order to form a sequence of annotation\nvectors (zl\u2212n, . . . , zl\u22121).\nUnlike the other approaches, in this case, the\ncontext vector p differs for each word wt\nin the\ncurrent sentence, and we denote it by pt\n. The context vector pt for the t-th word is computed as the\nweighted sum of the annotation vectors:\npt =\nX\nl\u22121\nl\n0=l\u2212n\n\u03b1t,l0zl\n0,\nwhere the attention weight \u03b1t,l0 is computed by\n\u03b1t,l0 =\nexp score (zl\n0, ht)\nPl\u22121\nk=l\u2212n\nexp score (zk, ht)\n.\nht\nis the hidden state of the recurrent language\nmodel of the current sentence from Eq. (5). The\nscoring function score(zl\n0, ht) returns a relevance\nscore of the l\n0\n-th context sentence w.r.t. ht\n.\n3.2 Conditional LSTM\nEarly Fusion Once the context vector p is computed from the n preceding sentences, we need to\nfeed this into the sentence-level recurrent language\nmodel. One most straightforward way is to simply\nconsider it as an input at every time step such that\nx = E\n>wt + Wpp,\nwhere E is the word embedding matrix that transforms the one-hot vector of the t-th word into a\ncontinuous word vector. We call this approach an\nearly fusion of the context.\nLate Fusion In addition to this approach, we\npropose here a modification to the LSTM such\nthat it better incorporates the context from the preceding sentences (summarized by pt\n.) The basic idea is to keep dependencies within the sentence being modelled (intra-sentence dependencies) and those between the preceding sentences\n(a) Early Fusion\n(b) Late Fusion\nFigure 1: Proposed fusion methods\nand the current sent (inter-sentence dependencies)\nseparately from each other.\nWe let the memory cell ct of the LSTM to\nmodel intra-sentence dependencies. This simply\nmeans that there is no change to the existing formulation of the LSTM.\nThe inter-sentence dependencies are reflected\non the interaction between the memory cell ct\n,\nwhich models intra-sentence dependencies, and\nthe context vector p, which summarizes the n preceding sentences. We model this by first computing the amount of influence of the preceding context sentences as\nrt = \u03c3 (Wr (Wpp) + Wrct + br).\nThis vector rt controls the strength of each of the\nelements in the context vector p. This amount\nof influence from the n preceding sentences is\ndecided based on the currently captured intrasentence dependency structures and the preceding\nsentences.\nThis controlled context vector rt \f (Wpp) is\nused to compute the output of the LSTM layer:\nht = ot \f tanh (ct + rt \f (Wpp)).\nThis is illustrated in Fig. 1 (b).\nWe call this approach a late fusion, as the effect of the preceding context is fused together with\n1322\nthe intra-sentence dependency structure in the later\nstage of the recurrent activation.\nLate fusion is a simple, but effective way to\nmitigate the issue of vanishing gradient in corpuslevel language modelling. By letting the context\nrepresentation flow without having to pass through\nsaturating nonlinear activation functions, it provides a linear path through which the gradient for\nthe context flows easily.\n4 Related Work\nContext-dependent Language Model This\npossibility of extending a neural or recurrent\nlanguage modeling to incorporate larger context\nwas explored earlier. Especially, (Mikolov and\nZweig, 2012) proposed an approach, called\ncontext-dependent recurrent neural network\nlanguage model, very similar to the proposed\napproach here. The basic idea of their approach\nis to use a topic distribution, represented as a\nvector of probabilities, of previous n words when\ncomputing the hidden state of the recurrent neural\nnetwork each time.\nThere are three major differences in the proposed approach from the work by Mikolov and\nZweig (2012). First, the goal in this work is\nto explicitly model preceding sentences to better approximate the corpus-level probability (see\nEq. (6)) rather than to get a better context of the\ncurrent sentence. Second, Mikolov and Zweig\n(2012) use an external method, such as latent\nDirichlet allocation (Blei et al., 2003) or latent semantics analysis (Dumais, 2004) to extract a feature vector, whereas we learn the whole model, including the context vector extraction, end-to-end.\nThird, we propose a late fusion approach which\nis well suited for the LSTM units which have recently been widely adopted many works involving language models (see, e.g., Sundermeyer et al.,\n2015). This late fusion is later shown to be superior to the early fusion approach.\nDialogue Modelling with Recurrent Neural\nNetworks A more similar model to the proposed larger-context recurrent language model is\na hierarchical recurrent encoder decoder (HRED)\nproposed recently by Serban et al. (2015). The\nHRED consists of three recurrent neural networks\nto model a dialogue between two people from the\nperspective of one of them, to which we refer as a\nspeaker. If we consider the last utterance of the\nspeaker, the HRED is a larger-context recurrent\nlanguage model with early fusion.\nAside the fact that the ultimate goals differ (in\ntheir case, dialogue modelling and in our case,\ndocument modelling), there are two technical differences. First, they only test with the early fusion\napproach. We show later in the experiments that\nthe proposed late fusion gives a better language\nmodelling quality than the early fusion. Second,\nwe use a sequence of bag-of-words to represent the\npreceding sentences, while the HRED a sequence\nof sequences of words. This allows the HRED to\npotentially better model the order of the words in\neach preceding sentence, but it increases computational complexity (one more recurrent neural network) and decreases statistical efficient (more parameters with the same amount of data.)\nSkip-Thought Vectors Perhaps the most similar work is the skip-thought vector by Kiros et al.\n(2015). In their work, a recurrent neural network\nis trained to read a current sentence, as a sequence\nof words, and extract a so-called skip-thought vector of the sentence. There are two other recurrent\nneural networks which respectively model preceding and following sentences. If we only consider the prediction of the following sentence, then\nthis model becomes a larger-context recurrent language model which considers a single preceding\nsentence as a context.\nAs with the other previous works we have discussed so far, the major difference is in the ultimate goal of the model. Kiros et al. (2015) fully\nfocused on using their model to extract a good,\ngeneric sentence vector, while in this paper we\nare focused on obtaining a good language model.\nThere are less major technical differences. First,\nthe skip-thought vector model conditions only on\nthe immediate preceding sentence, while we extend this to multiple preceding sentences. Second,\nsimilarly to the previous works by Mikolov and\nZweig (2012), the skip-thought vector model only\nimplements early fusion.\nNeural Machine Translation Neural machine\ntranslation is another related approach (Forcada\nand Neco, 1997; Kalchbrenner and Blunsom, \u02dc\n2013; Cho et al., 2014b; Sutskever et al., 2014;\nBahdanau et al., 2014). In neural machine translation, often two recurrent neural networks are used.\nThe first recurrent neural network, called an encoder, reads a source sentence, represented as a\nsequence of words in a source language, to form\na context vector, or a set of context vectors. The\n1323\nother recurrent neural network, called a decoder,\nthen, models the target translation conditioned on\nthis source context.\nThis is similar to the proposed larger-context recurrent language model, if we consider the source\nsentence as a preceding sentence in a corpus. The\nmajor difference is in the ultimate application, machine translation vs. language modelling, and\ntechnically, the differences between neural machine translation and the proposed larger-context\nlanguage model are similar to those between the\nHRED and the larger-context language model.\nContext-Dependent Question-Answering Models Context-dependent question-answering is a\ntask in which a model is asked to answer a question based on the facts from a natural language\nparagraph. The question and answer are often formulated as filling in a missing word in a query\nsentence (Hermann et al., 2015; Hill et al., 2015).\nThis task is closely related to the larger-context\nlanguage model we proposed in this paper in the\nsense that its goal is to build a model to learn\np(qk|q<k, q>k, D), (8)\nwhere qk is the missing k-th word in a query Q,\nand q<k and q>k are the context words from the\nquery. D is the paragraph containing facts about\nthis query. It is explicitly constructed so that the\nquery q does not appear in the paragraph D.\nIt is easy to see the similarity between Eq. (8)\nand one of the conditional probabilities in the\nr.h.s. of Eq. (7). By replacing the context sentences S\nl\u22121\nl\u2212n\nin Eq. (7) with D in Eq. (8) and conditioning wt on both the preceding and following words, we get a context-dependent questionanswering model. In other words, the proposed larger-context language model can be used\nfor context-dependent question-answering, however, with computational overhead. The overhead\ncomes from the fact that for every possible answer\nthe conditional probability completed query sentence must be evaluated.\n5 Experimental Settings\n5.1 Models\nThere are six possible combinations of the proposed methods. First, there are two ways of representing the context sentences; (1) bag-of-words\n(BoW) and (2) a sequence of bag-of-words (SeqBoW), from Sec. 3.1. There are two separate\nways to incorporate the SeqBoW; (1) with attention mechanism (ATT) and (2) without it. Then,\nthere are two ways of feeding the context vector\ninto the main recurrent language model (RLM);\n(1) early fusion (EF) and (2) late fusion (LF), from\nSec. 3.2. We will denote them by\n1. RLM-BoW-EF-n\n2. RLM-SeqBoW-EF-n\n3. RLM-SeqBoW-ATT-EF-n\n4. RLM-BoW-LF-n\n5. RLM-SeqBoW-LF-n\n6. RLM-SeqBoW-ATT-LF-n\nn denotes the number of preceding sentences to\nhave as a set of context sentences. We test four\ndifferent values of n; 1, 2, 4 and 8.\nAs a baseline, we also train a recurrent language\nmodel without any context information. We refer\nto this model by RLM. Furthermore, we also report the result with the conventional, count-based\nn-gram language model with the modified KneserNey smoothing with KenLM (Heafield et al.,\n2013).\nEach recurrent language model uses 1000\nLSTM units and is trained with Adadelta (Zeiler,\n2012) to maximize the log-likelihood; L(\u03b8) =\n1\nK\nPK\nk=1 log p(Sk|S\nk\u22121\nk\u2212n\n). We early-stop training\nbased on the validation log-likelihood and report\nthe perplexity on the test set using the best model\naccording to the validation log-likelihood.\nWe use only those sentences of length up to 50\nwords when training a recurrent language model\nfor the computational reason. For KenLM, we\nused all available sentences in a training corpus.\n5.2 Datasets\nWe evaluate the proposed larger-context language\nmodel on three different corpora. For detailed\nstatistics, see Table 1.\nIMDB Movie Reviews A set of movie reviews\nis an ideal dataset to evaluate many different\nsettings of the proposed larger-context language\nmodels, because each review is highly likely of a\nsingle theme (the movie under review.) A set of\nwords or the style of writing will be well determined based on the preceding sentences.\nWe use the IMDB Movie Review Corpus\n(IMDB) prepared by Maas et al. (2011).1 This corpus has 75k training reviews and 25k test reviews.\n1http://ai.stanford.edu/\u02dcamaas/data/\nsentiment/\n1324\n(a) IMDB (b) Penn Treebank\n(c) BBC (d) Fil9\nFigure 2: Corpus-level perplexity on (a) IMDB, (b) Penn Treebank, (c) BBC and (d) Fil9. The countbased 5-gram language models with Kneser-Ney smoothing respectively resulted in the perplexities of\n110.20, 148, 127.32 and 65.21, and are not shown here.\nWe use the 30k most frequent words in the training\ncorpus for recurrent language models.\nBBC Similarly to movie reviews, each new article tends to convey a single theme. We use the\nBBC corpus prepared by Greene and Cunningham\n(2006).2 Unlike the IMDB corpus, this corpus\ncontains news articles which are almost always\nwritten in a formal style. By evaluating the proposed approaches on both the IMDB and BBC\ncorpora, we can tell whether the benefits from\nlarger context exist in both informal and formal\nlanguages. We use the 10k most frequent words in\nthe training corpus for recurrent language models.\nBoth with the IMDB and BBC corpora, we did\nnot do any preprocessing other than tokenization.3\nPenn Treebank We evaluate a normal recurrent\nlanguage model, count-based n-gram language\nmodel as well as the proposed RLM-BoW-EF-n\nand RLM-BoW-LF-n with varying n = 1, 2, 4, 8\non the Penn Treebank Corpus. We preprocess the\n2http://mlg.ucd.ie/datasets/bbc.html\n3https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ntokenizer/tokenizer.perl\ncorpus according to (Mikolov et al., 2011) and use\na vocabulary of 10k words from the training corpus.\nFil9 Fil9 is a cleaned Wikipedia corpus, consisting of approximately 140M tokens, and is provided on Matthew Mahoney\u2019s website.4 We tokenized the corpus and used the 44k most frequent\nwords in the training corpus for recurrent language\nmodels.", "SumGPT3": "Introduction\n\nThe paper introduces a larger-context recurrent language model designed to improve language modeling by incorporating preceding sentences as context. This approach aims to enhance the model's ability to predict future words more accurately by considering a broader context compared to traditional methods. The paper evaluates different methods of context representation and integration within the recurrent language model.\n\nNovelty\n\nLate Fusion vs. Early Fusion:\n\nThe paper proposes and demonstrates the superiority of the late fusion approach over the early fusion approach for integrating contextual information into the language model. Previous works, such as those by Mikolov and Zweig (2012) and Serban et al. (2015), primarily focused on early fusion.\nSequence of Bag-of-Words (SeqBoW):\n\nInstead of representing preceding sentences as a sequence of words, the paper introduces a sequence of bag-of-words (SeqBoW). This method reduces computational complexity and increases statistical efficiency compared to the hierarchical recurrent encoder-decoder (HRED) model by Serban et al. (2015), which uses a sequence of sequences of words.\nAttention Mechanism with SeqBoW:\n\nThe paper explores the use of an attention mechanism in conjunction with SeqBoW, which allows the model to focus on relevant parts of the context when predicting the next word. This is an enhancement over the basic SeqBoW representation.\nComparison with Skip-Thought Vectors:\n\nUnlike the skip-thought vectors by Kiros et al. (2015), which condition only on the immediate preceding sentence, the proposed model extends to multiple preceding sentences, providing a broader context for prediction.\nAdaptation to Different Datasets:\n\nThe paper tests the proposed models on diverse datasets (IMDB Movie Reviews, BBC News Articles, Penn Treebank, and Fil9), demonstrating the model's adaptability to both formal and informal language styles.\nConclusion\n\nThe paper presents a novel larger-context recurrent language model that incorporates multiple preceding sentences using a sequence of bag-of-words with a late fusion approach, outperforming traditional early fusion methods. The introduction of an attention mechanism further refines the model's predictive capabilities. Evaluations across different corpora show the model's effectiveness and versatility in various language styles.", "SumGem": "Introduction\n\nThis paper proposes a novel recurrent neural network based language model that incorporates information from preceding sentences, which the authors call a larger-context language model. This model aims to improve the corpus-level probability estimation compared to conventional language models that treat sentences as independent units.\n\nNovelty\n\nThe paper proposes a method to incorporate document-level discourse dependency into a recurrent neural network language model. This is achieved by conditioning the probability of a word not only on the preceding words in the same sentence but also on a certain number of preceding sentences.\n\nTwo different ways to represent the context sentences are explored: bag-of-words (BoW) and sequence of bag-of-words (SeqBoW). SeqBoW allows incorporating the order of the preceding sentences. An attention mechanism is also investigated for SeqBoW representation.\n\nThe paper introduces a late fusion approach to mitigate the vanishing gradient problem and improve the effectiveness of incorporating context sentences into the language model. This approach keeps the dependencies within the sentence and those between preceding sentences separate before fusing them in the later stage.\n\nConclusion\n\nThe experimental results demonstrate that the proposed larger-context language model with late fusion achieves better perplexity on various corpora compared to the baseline models that do not consider preceding sentences. This suggests that incorporating context information from preceding sentences can improve the quality of language models. The paper also discusses the relationship between the proposed model and other related works in neural machine translation, context-dependent question answering, and dialogue modeling.", "SumGPT4": "**Introduction:**\nThis paper presents advancements in the field of natural language processing (NLP), specifically focusing on improving language models through larger-context integration. Traditional language models typically operate under the assumption of sentence independence, which limits their ability to predict and contextualize words based on broader textual relations. This research introduces a novel approach that incorporates discourse-level context into neural language models, which they term \"larger-context language model\".\n\n**Novelty:**\n1. **Corpus-Level Language Modeling:** Unlike traditional models that predict sentence-level probabilities, this approach emphasizes corpus-level probabilities by conditioning the probability of each sentence on multiple preceding sentences.\n2. **Integration of Discourse Context in LSTM:** The paper proposes modifications to Long Short-Term Memory (LSTM) units to better integrate discourse context, contrasting with conventional approaches that typically focus on word or sentence-level input sequences.\n3. **Late Fusion Technique:** The introduction of a late fusion method to integrate context at later stages of LSTM processing. This differs from the common early fusion technique and is demonstrated to handle vanishing gradients more effectively, maintaining a stronger gradient flow through the network.\n4. **Empirical Evaluation on Diverse Datasets:** The model's effectiveness is validated across various corpora, including IMDB, BBC, Penn TreeBank, and Fil9, showcasing its versatility and superior performance in handling different types of text, from movie reviews to formal news articles.\n\n**Conclusion:**\nThe paper marks a significant step forward in language modeling by addressing the limitations of traditional sentence-level models and enhancing them with corpus-wide contextual understanding. The proposed larger-context language model, through its innovative use of late fusion and modified LSTM architecture, not only captures the thematic essence of documents more accurately but also demonstrates marked improvements in model perplexity across diverse datasets. This approach paves the way for more context-aware NLP applications, potentially enhancing performance in tasks such as document summarization, topic modeling, and dialogue systems.", "rankGPT3": 1, "rankGem": 3, "rankGPT4": 2, "expGPT3": "Comprehensive Coverage: This summary provides a detailed overview of the key contributions and novelty of the paper. It effectively highlights the comparison between late fusion and early fusion approaches, the introduction of SeqBoW, and the use of attention mechanisms.\nClear Novelty and Contribution: It clearly distinguishes the proposed methods from previous works, citing specific models and methods used by other researchers, which helps to contextualize the advancements.\nDataset Adaptability: The summary explicitly mentions the testing of models on diverse datasets, emphasizing the model\u2019s versatility.\nStructured Presentation: It follows a logical structure, making it easy to follow the progression from the introduction of the problem to the proposed solutions and their evaluation.", "expGem": "Focused on Key Innovations: This summary succinctly covers the novel aspects of the research, such as corpus-level language modeling, modifications to LSTM, and late fusion technique.\nEmpirical Validation: It mentions the empirical evaluation on various datasets, which is crucial for demonstrating the model's effectiveness.\nConcise and Clear: The summary is concise yet comprehensive, making it easier to grasp the core contributions of the paper without unnecessary details.\nLacks Some Specifics: Compared to Summary 1, it lacks detailed comparisons with previous works and specific methods like SeqBoW and attention mechanisms, which are important for understanding the full scope of the research.", "expGPT4": "Adequate but Less Detailed: This summary provides an adequate overview but is less detailed compared to the other summaries. It mentions key concepts like BoW, SeqBoW, and the late fusion approach but doesn\u2019t elaborate much on their significance or how they improve upon previous methods.\nComparative Analysis: While it mentions the late fusion approach and its benefits, it does not compare as extensively with prior works as Summary 1.\nFlow and Clarity: The flow is less structured, and it seems more technical without providing enough context for each concept introduced, which might make it harder for readers to fully understand the innovations.\nLess Emphasis on Evaluation: It briefly touches on experimental results but does not detail the range of datasets or the specific improvements in perplexity, which are important for validating the model's performance."}, "7": {"title": "Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification", "url": "https://aclanthology.org/D17-1134", "text": "1 Introduction\nThe task of implicit discourse relation (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse\nmarker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to\nbe a crucial step for discourse analysis and language generation and helpful to many downstream\nNLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc.\nWith the release of PDTB 2.0 (Prasad et al.,\n2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009;\nWang et al., 2010; Zhou et al., 2010; Braud and\nDenis, 2015; Fisher and Simmons, 2015) with the\nuse of traditional NLP linguistically informed features and machine learning algorithms. Recently,\nmore and more researchers resorted to neural networks for implicit discourse recognition (Zhang\net al., 2015; Chen et al., 2016; Liu et al., 2016b;\nQin et al., 2016a; Liu and Li, 2016; Braud and\nDenis, 2016; Wu et al., 2016). Meanwhile, to\nalleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition\neither in traditional machine learning framework\n(Collobert and Weston, 2008; Lan et al., 2013) or\nrecently in neural network framework (Wu et al.,\n2016; Liu et al., 2016b).\nIn this work, we present a novel multi-task\nattention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation\nlearning at the same time. An attention-based neural network conducts discourse relationship representation learning through interaction between two discourse arguments. Meanwhile, a multi-task\nlearning framework leverages knowledge from\nauxiliary task to enhance the performance of main\ntask. Furthermore, these two types of learning\nare integrated into one neural network framework\nand work together to maximize the overall performance.\nThe contributions of this work are listed as follows.\n\u2022 We propose a multi-task attention-based neural network model to address implicit discourse relationship representation and recognition, which benefits from both the interaction between discourse arguments and the interaction between different learning tasks;\n\u2022 Our method achieves the best results on two\nbenchmark corpora in comparison with the\nstate-of-the-art systems so far.\nThe organization of this work is as follows.\nSection 2 describes the proposed novel multi-task\nneural network. Section 3 introduces the exper1299\nimental settings in detail. Section 4 reports the\ncomprehensive experimental results on two benchmark corpora. Section 5 summarized related work.\nFinally, Section 6 concludes this work.\n2 Multi-task Attention-based Neural\nNetworks Models\n2.1 Motivation\nThe idea of learning two types of interactive\nknowledge from arguments and from multi-tasks\nis motivated by the following observations and\nanalysis.\nOn the one hand, to recognize the discourse relationships, our system needs to understand the\nmeaning of each argument and infer the discourse\nsense transferred between two arguments (denoted\nas Arg-1 and Arg-2). Learning the semantic representation of each argument (sentence) has been\nstudied with the use of many neural network models and their variants (e.g., CNN, RNN, LSTM,\nBi-LSTM, ect). However, learning the complicated and various types of discourse relationships between arguments may not be performed by simply\nsumming up or concatenating two argument representations. We analyze the discourse with contrast\nrelationship and find that the contrast information\nmay result from different parts of sentence, e.g.,\ntenses (e.g., previous vs. now), entities (their vs\nour), or even the whole arguments, etc. Therefore, in order to learn the relationship representation between two arguments, we propose an attention mechanism that can select out the most important part from two arguments and perform the\ninformation interaction between two arguments.\nOn the other hand, one common issue involved\nin implicit discourse relationship identification is\nthe lack of labeled data. In this work, we state\nthat the relevant information from unlabelled data might be helpful and we present a novel multitask learning framework. In contrast with previous multi-task learning framework in traditional\nmachine learning, we improve multi-task learning\nframework with representation learning for better\ndiscourse relationship representation.\nInspired by the above considerations, we\npresent a novel multi-task attention-based neural\nnetwork model by integrating attention mechanism with multi-task learning for information interaction between arguments and between tasks.\n2.2 Learning Discourse Representation\nTo learn the semantic representation of each argument in discourse, a lot of neural network models and their variants have been proposed, such\nas, convolutional neural network (CNN), recurrent neural network (RNN) and so on. As a variant\nof RNN, long-short term memory (LSTM) neural\nnetwork specifically addresses the issue of learning long-term dependencies and is good at modeling over a sequence of words with consideration\nof the contextual information. Therefore, in this\nwork we adopt LSTM to model discourse argument.\n2.2.1 LSTM for Argument Representation\nFigure 1 shows the traditional LSTM model for\nrepresentation learning of arguments. First of alword embedding word embedding\n+\nArg-1 Arg-2\nsoftmax\nLSTM LSTM\nLoss\nRArg1 RArg2\nFigure 1: LSTM for discourse argument pair representation learning.\nl, through the embedding layer, we associate each\nword w in the vocabulary with a vector representation xw \u2208 R\ndw . Let x\n1\ni\n(x\n2\ni\n) be the i-th word\nvector in Arg-1 (Arg-2), then these two discourse\narguments are represented as:\nArg-1: [x\n1\n1\n, x\n1\n2\n, \u00b7 \u00b7 \u00b7 , x\n1\nL1\n] (1)\nArg-2: [x\n2\n1\n, x\n2\n2\n, \u00b7 \u00b7 \u00b7 , x\n2\nL2\n] (2)\nwhere Arg-1 (Arg-2) has L1 (L2) words.\nGiven the word representations of the argument\n[x1, x2, \u00b7 \u00b7 \u00b7 , xL] as the input sequence, an LSTM\ncomputes the state sequence [h1, h2, \u00b7 \u00b7 \u00b7 , hL] for\n1300\neach time step i using the following formulation:\nii = \u03c3(Wi\n[xi\n, hi\u22121] + bi) (3)\nfi = \u03c3(Wf [xi\n, hi\u22121] + bf ) (4)\noi = \u03c3(Wo[xi\n, hi\u22121] + bo) (5)\nc\u02dci = tanh(Wc[xi\n, hi\u22121] + bc) (6)\nci = ii \f c\u02dci + fi \f ci\u22121 (7)\nhi = oi \f tanh(ci) (8)\nwhere [ ] means the concatenation operation of\nvectors, \u03c3 denotes the sigmoid function and \f denotes element-wise product. Besides, ii\n, fi\n, oi\nand ci denote the input gate, forget gate, output\ngate and memory cell, respectively. Moreover, we\nalso use bidirectional LSTM (Bi-LSTM) which is\nable to capture the context from both past and future rather than LSTM which only considers the\ncontext information from the past. Therefore, at\neach position i of the sequence, we obtain two states\n\u2212\u2192h i and\n\u2190\u2212\nh i\n, where\n\u2212\u2192h i\n,\n\u2190\u2212\nh i \u2208 R\ndh . Then\nwe concatenate them to get the intermediate state,\ni.e. hi = [\u2212\u2192h i\n,\n\u2190\u2212\nh i\n]. After that, we sum up the\nsequence states [h1, h2, \u00b7 \u00b7 \u00b7 , hL] to get the representations of Arg-1 and Arg-2 respectively as follows:\nRArg1 =\nX\nL1\ni=1\nh\n1\ni\n(9)\nRArg2 =\nX\nL2\ni=1\nh\n2\ni\n(10)\nFinally we concatenate the two argument representations RArg1\nand RArg2\nas the argument pair\nrepresentation, i.e., Rpair = [RArg1\n, RArg2\n].\nClearly, in this way, there is no any correlation and interaction between the two arguments.\nThat is, whatever the types of discourse relationship they hold, the argument pair representation\nRpair is independent from RArg1\nor RArg2\n.\n2.2.2 Attention Neural Network for\nRelationship Representation\nIn order to effectively capture the complicated and\nvarious types of relationships between arguments,\nwe proposed a novel attention-based neural network model shown in Figure 2.\nTo do it, we first compute the match between\nRArg1\n(RArg2\n) and each state h\n2\ni\n(h\n1\ni\n) of Arg-2\n(Arg-1) by taking the inner product followed by a\nword embedding word embedding\nx x x x x\nArg-1 Arg-2\nx x x x\n+\nsoftmax\nLSTM LSTM\nLoss\nRArg1 RArg2\np\n1\np\n2\nR\u2032\nArg1 R\u2032\nArg2\nFigure 2: Attention Neural Network for representation learning of arguments.\nsoftmax as follows:\np\n1\ni = Softmax(RT\nArg2\nh\n1\ni\n) (11)\np\n2\ni = Softmax(RT\nArg1\nh\n2\ni\n) (12)\nwhere Softmax(zi) = e\nzi/\nP\nj\ne\nzi\n. Here p is an\nattention (probability) vector over the inputs and\ncan be viewed as the weights of the words measuring to what degree our model should pay attention to. It is worth noting that p\n1\nand p\n2\nare determined by RArg2\nand RArg1\nrespectively, which\nmeans the representation of one argument depends\non the representation of the other.\nNext, we sum over the state hi weighted by the\nattention vector p to compute the new representations for Arg-1 and Arg-2 respectively as below:\nR0\nArg1 =\nX\nL1\ni=0\nh\n1\ni p\n1\ni\n(13)\nR0\nArg2 =\nX\nL2\ni=0\nh\n2\ni p\n2\ni\n(14)\nThe representation of Arg-2 (RArg2\n) is used to\ncompute the weights of words in Arg-1 (i.e., p\n1\n)\nand RArg1\nis used to compute the weights of\nwords in Arg-2 (i.e., p\n2\n). In this way, the new\nrepresentations of the two arguments interact with\neach other. Therefore, this attention mechanism\nenables our model to focus on specific spans in the\ntwo arguments, which is crucial to recognize the\ndiscourse relations. We then concatenate R0\nArg1\n1301\nand R0\nArg2\nto get the argument pair representation\nRpair = [R0\nArg1\n, R0\nArg2\n].\nFinally, we feed the argument pair vector Rpair\nto a fully-connected softmax layer which outputs\nthe probabilities of different classes for the classification task. Here we choose the cross-entropy\nloss between the outputs of the softmax layer and\nthe ground-truth class labels as our loss function.\n2.3 Multi-task Attention-based Neural\nNetworks\nThe model presented in Section 2.2 can perform implicit discourse relation recognition in itself. However, similar with many models in deep\nlearning, one big issue is the lack of labeled data. Therefore, we propose a multi-task attentionbased neural network by integrating the aforementioned model into a multi-task learning framework\nto address the implicit discourse relation recognition with the aid of large amount of unlabelled data. Figure 3 shows the general framework of our\nproposed multi-task attention-based neural network model.\nArg Pair\nrepresentation\nArg Pair\nrepresentation\nArg-1 Arg-2 Arg-1 Arg-2\nShare\nhidden layer\nsoftmax\nMain Task Aux Task\nLossmain Lossaux\nRmain Raux\nFigure 3: The framework of our proposed multitask attention-based neural network model.\nWe use the aforementioned attention-based\nneural network to map the argument pair into a low-dimensional vector (Rpair) denoted\nas Arg Pair representation component in Figure 3. Under the multi-task learning\nframework, the parameters of the Arg Pair\nrepresentation components are shared between the main task and the auxiliary tasks. We\ndenote Rmain and Raux as the representations of\nargument pair for main and auxiliary tasks, respectively. And we add a hidden layer after Rmain and\nRaux to learn the task-specific representations followed by the softmax layers used to compute the\nloss of the main task (Lossmain) and the loss of\nthe auxiliary task (Lossaux), respectively.\nRegarding the strategy of sharing knowledge\nlearnt from auxiliary to main task, we propose the\nfollowing three methods.\n2.3.1 Equal Share\nA simple and straightforward way is to equally\nshare the knowledge learned from main task and\nauxiliary task. Therefore, the total loss of the\nmulti-task neural network is calculated as:\nLoss = Lossmain + Lossaux (15)\nwhere Lossaux has the same weight as Lossmain.\n2.3.2 Weighted Share\nAnother method is to give different weights to the\nmain and auxiliary task as below:\nLoss = Lossmain + w \u2217 Lossaux (16)\nwhere w \u2208 (0, 1] is a weight parameter. Clearly, a\nlower value of w means less importance of auxiliary task.\n2.3.3 Sigmoid (Gated) Interaction\nThe above two ways of sharing knowledge actually have no deep interaction between the main and\nauxiliary tasks. They only share equal or weighted\ncontributions from tasks to final result. Therefore,\nwe propose a model that can perform interaction\nbetween tasks, which is shown in Figure 4.\nWe introduce two important parameters\nWinter \u2208 R\ndpair\u00d7dpair and binter \u2208 R\ndpair\n(dpair is the length of the argument pair representation vector Rpair) to fulfil the interaction\nbetween main and auxiliary tasks. As shown in\nthe following Formula (17) and (18), the new\nrepresentation of argument pair R0\nmain is updated\nby the combination of Winter and Raux using a\nSigmoid function.\nR0\nmain = Rmain \f \u03c3(WinterRaux + binter)\n(17)\nR0\naux = Raux \f \u03c3(WinterRmain + binter)\n(18)\nWinter and the Sigmoid function (\u03c3) work together to make information interacted between two tasks and select useful relevant information out\nof the opposite tasks as well. Clearly, Winter is\n1302\nArg Pair\nrepresentation\nArg Pair\nrepresentation\nArg-1 Arg-2 Arg-1 Arg-2\nShare\nhidden layer\nsoftmax\nMain Task Aux Task\nRmain W Raux\nR\u2032\naux\nR\u2032\nmain\nLossmain Lossaux\nFigure 4: Sigmoid (Gated) interaction shared in\nmulti-task framework (GShare).\na parameter to be trained. This mechanism acts as a gate to determine how much the information would pass through to the final result. Therefore, under the framework of multi-task and gated\nmechanism, the main and auxiliary tasks are capable of not only sharing the parameters of learning\nargument pair representation but also interacting\nthe representations learning from each other.\n2.4 Parameter Learning\nWe tried various settings of word embeddings\ntrained on the BLLIP corpus with different dimensions dW E = [50, 100, 150, 200] by word2vec\ntool1\nand finally set dimensionality as 50 based\non the results on development set. we also explored the hidden state dh = [50, 100, 150, 200]\nand the size of hidden layer in multi-task framework dmulti\u2212task = [50, 80, 120, 150]. Finally,\nfor binary classification and four way classification on PDTB, we chose dh = 50 and dmulti\u2212task =\n80. For multi-class classification on CoNLL-2016,\nwe chose dh = 100 and dmulti\u2212task = 120. We\napplied dropout to the penultimate layer and set\nthe dropout rate as 0.5. These parameters remain\nthe same in experiments except the share weight\nw varies which will be discussed later. We chose\nthe cross-entropy loss as loss function and adopted AdaGrad (Duchi et al., 2011) with a learning\nrate of 0.001 and a minibatch size of 64 to train\nthe model.\n5 Related Work\n5.1 Implicit Discourse\nWith the release of PDTB 2.0, a number of studies\nperformed discourse relation recognition on natural (i.e., genuine) discourse data with the use of\ntraditional NLP techniques to extract linguistically\ninformed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009;\nWang et al., 2010; Braud and Denis, 2015; Fisher\nand Simmons, 2015).\nLater, to make a full use of unlabelled data, several studies performed multi-task or unsupervised\nlearning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford\nand Xue, 2015).\nRecently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015;\nChen et al., 2016; Liu et al., 2016b; Qin et al.,\n2016a; Liu and Li, 2016; Braud and Denis, 2016;\nWu et al., 2016).\n5.2 Multi-task learning\nMulti-task learning framework adopts traditional\nmachine learning with human-selected effective\nknowledge and the shared part is integrated into\nthe cost function to prefer the main task learning.\n(Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sharing (sharing the word\nembeddings with tasks). (Liu et al., 2016a) proposed the multi-task neural network by modifying\nthe recurrent neural network for text classification\ntasks. (Lan et al., 2013) present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained\nsynthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu\net al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification.\n5.3 Deep learning with Attention\nRecently deep learning with attention has been\nwidely adopted by NLP researchers. (Zhou et al.,\n2016) proposed an attention-based Bi-LSTM for\nrelation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level\nsentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer\nMatching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive\nconvolutional neural network for QA.\n6 Concluding Remarks\nWe present a novel multi-task attention-based neural network model for implicit discourse relationship representation and identification. Our method\ncaptures both the discourse relationships through\ninteractions between discourse arguments and the\ncomplementary knowledge through interactions\nbetween annotated and unannotated data. The experimental results showed that our proposed model outperforms the state-of-the-art systems on two\nbenchmark corpora.", "SumGPT3": "Introduction:\nThe paper addresses the task of implicit discourse relation identification, crucial for discourse analysis and language generation, benefiting various NLP applications. While traditional methods relied on linguistically informed features and machine learning algorithms, the paper introduces a novel multi-task attention-based neural network approach.\n\nNovelty:\n\nIntegrated Representation Learning: The model simultaneously learns two types of representation: interaction between discourse arguments and between different learning tasks. It employs an attention-based neural network for discourse relationship representation learning, enhancing performance through multi-task learning.\nAttention Mechanism: Recognizing complex discourse relationships requires capturing nuanced cues from text. The paper proposes an attention mechanism that selects important parts from arguments, enabling precise interaction between them.\nMulti-task Learning Framework: Addressing the challenge of limited labeled data, the paper leverages unannotated data through a multi-task learning framework. It enhances discourse relationship representation and recognition by sharing knowledge across tasks.\nGated Interaction: Unlike traditional multi-task learning, the model incorporates a gated mechanism for interaction between tasks. This enables information exchange between the main and auxiliary tasks, improving overall performance.\nConclusion:\nThe proposed multi-task attention-based neural network presents a comprehensive solution for implicit discourse relationship representation and identification. By integrating attention mechanisms and multi-task learning, it outperforms existing systems, offering advancements in discourse analysis and NLP applications.", "SumGem": "Introduction\n\nImplicit discourse relationship recognition refers to the task of identifying the logical connection between two text spans without an explicit connective word. This paper proposes a novel multi-task attention-based neural network model to address this task.\n\nNovelty\n\nThe paper introduces several novel aspects to the task of implicit discourse relationship recognition:\n\nAttention-based neural network for relationship representation: The model incorporates an attention mechanism to capture the intricate relationships between discourse arguments. This mechanism goes beyond simply concatenating argument representations and allows the model to focus on the most relevant parts of each argument for relation identification.\n\nMulti-task learning framework: The model leverages a multi-task learning framework to address the scarcity of labeled data. This framework allows the model to learn from both labeled and unlabeled data, enhancing its performance on the main task of discourse relation recognition. The paper explores three methods for knowledge sharing between the main and auxiliary tasks: equal share, weighted share, and sigmoid (gated) interaction.\n\nIntegration of attention mechanism and multi-task learning: The paper integrates the attention mechanism with the multi-task learning framework. This integration allows the model to benefit from both the interaction between discourse arguments and the interaction between different learning tasks, ultimately leading to superior performance.\n\nConclusion\n\nThe experimental results demonstrate that the proposed multi-task attention-based neural network model achieves state-of-the-art performance on two benchmark datasets for implicit discourse relationship recognition. This success can be attributed to the model's ability to effectively capture discourse relationships through argument interaction and to leverage knowledge from unlabeled data through the multi-tasking framework.", "SumGPT4": "**Introduction**\nThe scientific paper addresses the challenge of implicit discourse relation identification, which involves determining the logical connections between adjacent text spans without explicit discourse markers. This task is essential for advancing discourse analysis and enhancing various natural language processing (NLP) applications such as question answering, machine translation, sentiment analysis, and more. The study introduces a novel multi-task attention-based neural network designed to improve the recognition and representation of implicit discourse relationships.\n\n**Novelty**\nThe paper presents several innovative aspects in the field of NLP:\n1. **Multi-Task Attention-Based Neural Network**: The proposed model integrates attention mechanisms with multi-task learning to facilitate dual learning processes. It enhances discourse relationship recognition by simultaneously learning from the interaction between discourse arguments and across different auxiliary tasks.\n2. **Attention Mechanism for Discourse Argument Interaction**: Unlike traditional models that may aggregate or concatenate argument representations, this model employs an attention mechanism to selectively focus on and interact with the most relevant parts of each argument. This approach allows for a more nuanced understanding and representation of discourse relationships.\n3. **Enhanced Multi-Task Learning Framework**: The study advances beyond traditional multi-task learning frameworks by embedding the attention mechanism within the learning process. This innovation not only leverages unlabeled data but also optimizes the interaction between main and auxiliary tasks, improving the model's performance on identifying implicit discourse relationships.\n4. **Integration of Discourse Representation and Task Interaction**: The model uniquely combines representation learning with task interaction, providing a synergistic effect that enhances the overall performance in discourse analysis.\n\n**Conclusion**\nThe paper successfully demonstrates the efficacy of a multi-task attention-based neural network in improving the identification and representation of implicit discourse relationships. By incorporating both attention mechanisms and a novel multi-task learning approach, the model outperforms current state-of-the-art systems on benchmark corpora, offering significant advancements in the field of NLP. This approach not only enhances the understanding of discourse dynamics but also contributes valuable insights for further research and application in related NLP tasks.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "Strengths:\n\nComprehensive Coverage: It covers the main aspects of the paper, including the introduction of the model, the attention mechanism, and the multi-task learning framework.\nConceptual Clarity: Explains the significance of capturing nuanced cues from text and leveraging unannotated data through a multi-task learning framework.\nGated Interaction: The mention of gated interaction for task exchange is a unique point that highlights an advanced technique used in the model.\nWeaknesses:\n\nLess Structured: The summary could benefit from better structuring. It flows continuously without clear section breaks, which can make it harder to parse quickly.\nDepth on Novelty: While it mentions several novel aspects, it doesn\u2019t delve deeply into each one compared to the third summary.", "expGem": "Strengths:\n\nKey Points: It mentions key points such as the attention mechanism and the multi-task learning framework.\nKnowledge Sharing Methods: It outlines the three methods for knowledge sharing between tasks, providing a unique detail not extensively covered in the other summaries.\nWeaknesses:\n\nRepetition and Redundancy: The summary contains some repetition, especially in the introduction and novelty sections, which reduces clarity.\nLess Integration: It lacks the cohesive integration of how the attention mechanism and multi-task learning work together, which is better articulated in the other summaries.\nLess Specificity: Compared to the other summaries, it provides fewer specifics on the implementation and impact of the proposed methods.", "expGPT4": "Strengths:\n\nClarity and Structure: This summary is well-organized, clearly segmented into Introduction, Novelty, and Conclusion sections, making it easy to follow.\nDetailed Explanation: It provides detailed and specific explanations of the novel contributions, such as the attention mechanism for discourse argument interaction and the enhanced multi-task learning framework.\nIntegration of Concepts: The summary effectively explains how the integration of attention mechanisms with multi-task learning benefits the model, highlighting the synergy between discourse representation and task interaction.\nSpecific Innovations: It highlights specific innovations like the nuanced understanding of discourse relationships and the unique combination of representation learning with task interaction.\nWeaknesses:\n\nTechnical Depth: While it is detailed, it could benefit from a bit more technical depth on how the attention mechanism and multi-task learning are implemented."}, "8": {"title": "S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation", "url": "https://aclanthology.org/2022.aacl-main.12", "text": "1 Introduction\nEmotion recognition in conversation (ERC), which\naims to identify the emotion of each utterance in\na conversation, is a task arousing increasing interests in many fields. With the prevalence of social\nmedia and intelligent assistants, ERC has great\npotential applications in several areas, such as emotional chatbots, sentiment analysis of comments\nin social media and healthcare intelligence, for\nunderstanding emotions in the conversation with\nemotion dynamics and generating emotionally coherent responses. ERC problem still remains a\nchallenge. Both lexicon-based (Wu et al., 2006;\nShaheen et al., 2014) and deep learning-based (Colneric and Dem\u0161ar \u02c7 , 2018) text emotion recognition methods that treat each utterance individu-\n\u2217 Corresponding author.\nA good friend of mine passed\naway the other day. [sad]\nSpeaker A Speaker B\nOh he had some pretty\nprogressive cancer. [sad]\nYeah, he was pretty awesome\nand supportive.\nOh no, I'm so sorry. What\nhappened? [sad]\nDid you know him well?\n[neutral]\n[happy] [sad]\nFigure 1: A dialogue from IEMOPCAP, in which the\nemotion of the last utterance by speaker A will be\nwrongly classified if the dialogue context is not taken\ninto consideration.\nally fail in this task as these works ignore some\nconversation-specific characteristics.\nIn the past few years, recurrent neural network\n(RNN)-based solutions, such as CMN (Hazarika\net al., 2018b), ICON (Hazarika et al., 2018a) and\nDialogueRNN (Majumder et al., 2019), have dominated this field due to the sequential nature of\nconversational context. Nonetheless, they share\nsome inherent limitations: 1) RNN model performs\npoorly in grasping distant contextual information;\n2) RNN-based methods are not capable of handling\nlarge-scale multiparty conversations.\nWith the rise of graph neural network (GNN)\n(Wu et al., 2020) in many natural language processing (NLP) tasks, researchers pay increasing\nattention to GNN-based ERC methods recently. Instead of modeling only sequential data recurrently\nin RNN, GNN is designed to capture all kinds of\ngraph structure information via various aggregation\nalgorithms. Existing GNN-based ERC methods,\nsuch as DialogueGCN (Ghosal et al., 2019), RGAT\n(Ishiwatari et al., 2020) and DAG-ERC (Shen et al.,\n2021), which are the state of the art, have demonstrated the superiority of GNN in modeling conversational structure information. A directed graph\nis constructed on each dialogue in these methods,\nwhere the nodes denote the individual utterances,\n149\nand the edges indicate relationships between utterances. However, we notice that the relative position\nand speaker dependency information are mostly\nencoded together in one weight matrix according\nto the edge type in these methods, which can not\nexploit these conversation structure information\nsufficiently.\nOn the other hand, these methods do not work\nwell on modeling speaker-specific context, which\nis also important in the ERC task. For example,\nin Figure 1 the third utterance spoken by speaker\nA is more influenced by speaker A\u2019s prior utterances rather than the second utterance spoken by\nspeaker B, even though the latter is closer. Thus,\nin contextual modeling, we should consider both\nthe emotional influence that speakers have on themselves during a conversation, i.e., self-speaker context, and context on the entire conversation flow,\ni.e., inter-speaker context, as well as the interaction\nbetween them.\nIn this paper, we propose a novel Speaker and\nPosition-Aware GNN model for ERC (S+PAGE)\nto settle the above drawbacks of existing methods.\nOur model contains three stages to fully consider\nboth contextual modeling and conversation structure modeling. Specifically, given a sequence of\nutterances in the same dialogue, we first leverage a\nTwo-Stream Conversational Transformer (TSCT)\nwith the attentive masking mechanism to get both\nself and inter-speaker contextual features. Then,\nguided by the speaker dependency, we construct a\nconversation graph. We propose an enhanced relational graph convolution network (R-GCN), called\nSPGCN, to refine the contextual features with conversation structure information. Particularly, we\nintroduce relational relative positional encoding in\nthe aggregation algorithm to make SPGCN capable\nof capturing fine-grained positional information\nin a conversation. Finally, the global transfer of\nemotion labels is modeled by a conditional random field (CRF) layer with the features from both\nTSCT and SPGCN. Experimental results demonstrate the superiority of our model compared with\nstate-of-the-art models. Ablation study illustrates\nthe effectiveness of the proposed components in\nthe model. To conclude, our contributions are as\nfollows:\n\u2022 We propose a new GNN-based ERC method,\ncalled S+PAGE, in which a novel graph neural network, namely SPGCN, is presented to\nbetter capture the conversation structure information.\n\u2022 We present a two-stream conversational Transformer architecture to extract both self and\ninter-speaker contextual features.\n\u2022 We conduct extensive experiments on four\nERC benchmark datasets, and the results\ndemonstrate that the proposed model achieves\nthe competitive performance on all of them.\n2 Related Works\n2.1 Emotion Recognition in Conversation\nEmotion recognition in conversation is a popular\narea in NLP. Many ERC datasets have been scripted\nand annotated in the past few years, such as IEMOCAP (Busso et al., 2008), MELD (Poria et al.,\n2018), DailyDialog (Li et al., 2017), EmotionLines\n(Chen et al., 2018) and EmoryNLP (Zahiri and\nChoi, 2018). IEMOCAP, MELD, and EmoryNLP\nare multimodal datasets, containing acoustic, visual and textual information, while the remaining\ntwo datasets are textual.\nIn recent years, ERC solutions are mostly deep\nlearning-based models. CMN (Hazarika et al.,\n2018b) and ICON (Hazarika et al., 2018a) utilize\ngated recurrent unit (GRU) and memory networks\nto capture the dialogue dynamics. In IAAN (Yeh\net al., 2019) and DialgueRNN (Majumder et al.,\n2019), attention mechanisms are applied to interact\nbetween the party state and global state. With the\nrise of Transformer and graph neural networks in\nNLP tasks, many works have also introduce them\ninto the ERC task. (Zhong et al., 2019) propose\nKET, which is a structure of hierarchical Transformers assisted by external commonsense knowledge. DialogueXL (Shen et al., 2020) applies\ndialogue-aware self-attention to deal with the multiparty structures. In DialogueGCN (Ghosal et al.,\n2019) and RGAT (Ishiwatari et al., 2020), GCN\n(Kipf and Welling, 2016) and GAT (Velickovi \u02c7 c\u00b4\net al., 2017) are applied to refine the features with\nspeaker dependencies and temporal information.\nDAG-ERC (Shen et al., 2021) applies a directed\nacyclic graph for conversation representation and\nit achieves the state-of-the-art performance on multiple ERC datasets.\n2.2 Transformer\n(Vaswani et al., 2017) first propose Transformer\nfor machine translation task, whose success subsequently has been proved in various down-stream\n150\nNLP tasks. Self-attention mechanisms endow\nTransformer with the ability of capturing longerrange dependency among elements of an input sequence than the RNN structure. (Beltagy et al.,\n2020) propose a novel self-attention mechanism for\nfeature extraction of long documents. Pre-trained\nmodels such as BERT (Devlin et al., 2018) and\nGPT (Brown et al., 2020) use Transformer encoder\nand decoder respectively to learn representations\non large-scale datasets.\n2.3 Graph Neural Network\nGraph neural network has attracted a lot of attention in recent years, which learns a target node\u2019s\nrepresentation by propagating neighbor information in the graph. (Kipf and Welling, 2016) propose\na simple and well-behaved layer-wise propagation\nrule for neural network models and demonstrate\nits effectiveness in semi-supervised classification\ntasks. Better aggregation methods for large graphs\nare proposed in GAT (Velickovi \u02c7 c et al. \u00b4 , 2017) and\nGraphSage (Hamilton et al., 2017). (Schlichtkrull\net al., 2018) propose R-GCN to deal with the highly\nmulti-relational data characteristic by assigning different aggregation structures for each relation type.\n3 Methodology\nThe framework of our model is shown in Figure\n2. We decompose the emotion classification procedure into three stages, i.e., contextual modeling, speaker dependency modeling, and global consistency modeling. In the first stage, we present\na conversation-specific Transformer to get both\nself and inter-speaker contextual features. Then,\na graph neural network is proposed to refine the\nfeatures with conversation structure information,\nincluding the speaker dependency and relative position of each utterance. Subsequently, we employ conditional random field as the output layer to\nmodel the context of global consistency of emotion\nlabels.\n3.1 Problem Definition\nThe ERC task is to predict emotion labels (e.g.,\nHappy, Sad, Neutral, Angry, Excited, and Frustrated) for utterances {u1; u2; \u00b7 \u00b7 \u00b7 ; uN }, where N\ndenotes the number of utterances in a conversation. Let S be the number of speakers in a given\ndataset. P is a mapping function, and s = P(ui)\ndenotes utterance ui uttered by speaker s, where\ns \u2208 {1, \u00b7 \u00b7 \u00b7 , S}.\n3.2 Utterance Encoding\nFollowing previous works (Ghosal et al., 2019; Majumder et al., 2019), we use a simple architecture\nconsisting of a single convolutional layer followed\nby a max-pooling layer and a fully connected layer\nto extract context-independent textual features of\neach utterance. The input of this network is the 300\ndimensional pre-trained 840B GloVe vectors (Pennington et al., 2014). We use the output features,\ndenoted as \u20d7ui\n, as the representation of each utterance. Notice that we do not use any pre-trained\nmodel like BERT and RoBERTa to make utterance\nencoding for fairness of comparison with the baseline methods.\n3.3 Contextual Modeling\nWe present a Two-Stream Conversational\nTransformer (TSCT) to better extract the\ncontextual representation of each utterance\nin a conversation, which is also capable of\nhandling multi-party conversations efficiently.\nThe collection of utterance representations\nU = { \u20d7u1; \u20d7u2; \u00b7 \u00b7 \u00b7 ; \u20d7uN } is taken as the input. We\ndesign a multi-head self-attention mechanism,\ncomposed of two streams, i.e., the inter-speaker\nself-attention stream and the intra-speaker\nself-attention stream.\n3.3.1 Inter-Speaker Self-Attention\nThe inter-speaker self-attention is same with the\nself-attention in vanilla Transformer, in which each\nutterance can attend to all positions in the dialogue\nas shown in Figure 3(a). It is calculated as:\nq\nt\ni\n, kt\ni\n, vt\ni = h\nt\u22121\ni Wt\niq, ht\u22121\ni Wt\nik, ht\u22121\ni Wt\niv (1)\nz\nt\ni = sof tmax(\nq\nt\ni\n(k\nt\ni\n)\nT\n\u221a\nd\n)v\nt\ni\n(2)\nwhere Wt\niq, Wt\nik and Wt\niv are three learnable\nweight matrices for attention head i at layer t.\n3.3.2 Intra-Speaker Self-Attention\nThe intra-speaker self-attention models speakerspecific contextual information by only computing\nattention on the same speaker\u2019s utterances in a dialogue. In this way, the model is able to capture\nthe emotional influence that speakers have on themselves during the conversation. It is implemented\nby the attentive masking strategy as illustrated in\nFigure 3(b) and formulated as:\n151\n\ufffd!\n(#)\n\ufffd%\n(#)\n\ufffd&\n(')\n\ufffd(\n(')\n\ufffd)\n(#)\n\ufffd*\n(#)\nGraph Layer \ufffd\n\ufffd! \ufffd\" \ufffd# \ufffd$ \ufffd% \ufffd&\nTSCT Layer\nTSCT Layer\n\u210e! \u210e\" \u210e# \u210e$ \u210e% \u210e&\n\ufffd!\n(%)\n\ufffd%\n(%)\n\ufffd&\n(%)\n\ufffd(\n(%)\n\ufffd)\n(%) \ufffd*\n(%)\nGraph Layer 1\nContextual Modeling Speaker Dependency Modeling Consistency Modeling\n\ufffd%\n(')\n\u210e!\u2026\u2026 \u2026\u2026\n\u2a01\n\ufffd*\n(')\n\u210e&\n\u2a01\nC\nR\nF\nLabels \u2026\u2026 \u2026\u2026\nSpeaker 1\nSpeaker 2\nFigure 2: The overall framework of S+PAGE. First, contextualized representation of each utterance is obtained\nby contextual modeling part. Subsequently, we employ SPGCN to model the speaker dependency and position\ninformation. Finally, the CRF layer applied to model the consistency using information from the previous parts. \u2295\ndenotes the concatenation operation. L is the total number of graph layers.\nQ K, V\nInter-Speaker Attention\n\u210e!\n(#) \u210e%\n(#) \u210e&\n(#) \u210e'\n(#) \u210e(\n(#)\n\ufffd!\n(!)\n(a)\n(b)\nAttention Mask\nAttention Mask\nQ K, V\nIntra-Speaker Attention\n\u210e!\n(#) \u210e%\n(#) \u210e&\n(#) \u210e'\n(#) \u210e(\n(#)\n\ufffd\u0303\n!\n(!)\nFigure 3: (a) Inter-speaker self-attention: the attention\namong all speakers, same with vanilla Transformer.(b)\nIntra-speaker self-attention: the attention only on the\nutterances spoke by the current speaker.\nze\nt\ni = sof tmax(\nq\nt\ni\n(k\nt\ni\n)\nT\n\u221a\nd\n+ m)v\nt\ni\n(3)\nwhere m \u2208 R\nN\u00d7N is the attentive masking matrix.\nThe elements of m are set as below:\nmij =\n\u001a\n\u2212\u221e P(ui) \u0338= P(uj )\n0 otherwise (4)\nwhere P(\u00b7) is the function that maps the utterance\nand its corresponding speaker.\nEach attention head i of the t-th layer in TSCT,\ndenoted as headt\ni\n, is the concatenation of the zi and\nzei\n, and the output of the multi-head attention can\nbe formulated as follows:\nMultiHeadt\ni = \u2225\nM\ni=1headt\ni\n(5)\nwhere \u2225 denotes concatenation operation. M is the\nnumber of attention heads, while 1 \u2264 i \u2264 M.\nFollowing the structure of the original Transformer, the output of the TSCT layer can be generated by passing MultiHeadt\ni\nthrough a FF (feedforward network):\nh\nt = LayerNorm(FF(MultiHeadt\ni\n)) (6)\n3.4 Speaker Dependency Modeling\nAfter extracting the contextual features, we introduce a novel graph neural network, named SPGCN,\nto propagate structure-aware utterance features.\nSpecifically, in SPGCN, speaker dependency and\nposition information are modeled by edge types\nand edge weights respectively, and are combined\nin the aggregation function to update the features.\n3.4.1 SPGCN\nGraph Architecture We construct a directed\ngraph, G = (V, E, R, W), for each dialogue\nwith N utterances. The nodes in the graph are\nthe utterances in the conversation, i.e., V =\n{v1; v2; \u00b7 \u00b7 \u00b7 , vN }. (vi\n, vj , rij ) \u2208 E denotes a labeled edge (relation), where rij \u2208 R is a relation\ntype, defined according to speaker identity and relative distance. W represents the set of edge weights.\nNodes Feature vector gi of each node vi\nis initialized as the output of the TSCT layer, i.e., hi\n. gi\nis\nmodified by the aggregation algorithm through the\nstacked graphical layers in GNN. The output feature is described as g\nl\ni\n, where l denotes the number\nof layers.\n152\nSpeaker 1\nSpeaker 2\n\ufffd!\n\ufffd\"\n\ufffd#\n\ufffd$\n\ufffd& \ufffd%\n\ufffd!\n\ufffd&\n\ufffd#\n\ufffd$\n\ufffd \ufffd% \"\nFigure 4: An example of incoming edges for nodes v3\n(left) and v2 (right) in the dialogue graph. Different\ntypes of arrows denote different edge types. Nodes\nshare the same edge types if they are spoke by the same\nspeaker. v3, v1 and v5 are spoke by speaker1, thus the\nedge between v3, v1 and the edge between v3, v5 belong\nto the same edge type.\nEdges Instead of only focusing on past utterances, we take converse influence into account\n(Ghosal et al., 2019). We construct edges E with\na sliding window for each utterance. The window\nsizes p and f denote the number of past and future\nutterances from the target utterance. Each utterance\nnode vi has an edge with p utterances of the past:\n{vi\u22121, vi\u22122, ..., vi\u2212p}, f utterances of the future:\n{vi+1, vi+2, ..., vi+f }, and itself.\nEdge Types The relation type r \u2208 R is determined by the speaker identity. Assuming there are\nS distinct speakers in a dialogue, there should be\nNe = S\n2\nrelation types in the constructed graph\nG. Two utterances share the same edge type only\nif they are uttered by the same speaker. For example, in Figure 4 the incoming edges v1 \u2192 v3 and\nv5 \u2192 v3 share the same edge type, and v4 \u2192 v3 is\na different edge type.\nEdge Weights Edge weight \u03b1ij \u2208 W is computed by an attention mechanism. The particular\nattentional setup in our model closely follows the\nwork of GAT (Velickovi \u02c7 c et al. \u00b4 , 2017). The input of\nthe attention module is a set of node features from\nthe last layer. Motivated by (Shaw et al., 2018),\nwhich shows that absolute positional encoding is\nnot effective for the model to capture the information of relative word order, we inject relative\npositional encoding into the attention mechanism.\n\u03b2ij = Ep(o(vj ) \u2212 o(vi)) (7)\n\u0393ij = LReLU \u0010\n\u20d7aT\nh\nW gl\u22121\ni\n\u2225(W gl\u22121\nj + \u03b2ij )\ni\u0011\n(8)\n\u03b1ij = P\nexp \u0393ij\nk\u2208N i exp \u0393ik\n(9)\n\u03b2ij denotes the signed relative position representation between utterance i and utterance j in a dialogue, which is encoded by a trainable embedding\nmatrix Ep. o(\u00b7) is a mapping function between\nutterance and its absolute position in the dialogue\nsequence. LReLU denotes the activation function\nLeakyReLU. W is a weight matrix applied to every node. Ni\nis the number of nodes linked with\nnode i. \u20d7a is a parametrized weight vector. \u00b7\nT\nrepresents transposition, and \u2225 is the concatenation\noperation.\nAggregation Function Inspired by R-GCN\n(Schlichtkrull et al., 2018), we define the following\naggregation algorithm to calculate the forward-pass\nupdate of a node in the graph:\ngei\nl = \u03c3\n\uf8eb\n\uf8ed\nX\nr\u2208R\nX\nj\u2208Nr\ni\n\u03b1\nl\nij\nci,r\nWl\nr\ng\nl\u22121\ni + \u03b1\nl\niiWl\no\ng\nl\u22121\ni\n\uf8f6\n\uf8f8\n(10)\nwhere gei\nl\nis the aggregated state of node i in the l-th\nlayer. Nr\ni\ndenotes the set of neighbors of utterance\ni under the edge type r \u2208 R. ci,r is a normalization\nconstant, and we set ci,r = |Nr\ni\n| in our experiment.\nWl\nr\nand Wl\no\nare learnable weight matrices, and \u03c3(\u00b7)\nis an activation function, such as the ReLU. Different from R-GCN, we use edge weights calculated\nby Equation 9 to involve fine-grained positional\ninformation in a conversation.\nAfter the aggregation, we employ a gate fusion\nfunction to make gei\nl\ninteract with its hidden state\nat the previous layer. Finally, the representation at\nthe l-th layer is formulated as:\ng\n\u2032\n= [gei\nl\n; g\nl\u22121\ni\n; gei\nl\n\u2217 g\nl\u22121\ni\n; gei\nl \u2212 g\nl\u22121\ni\n] (11)\n\u03f5 = sigmoid \u0010\nWf g\n\u2032\n+ bf\n\u0011\n(12)\ngi\nl = \u03f5 \u2217 gei\nl + (1 \u2212 \u03f5) \u2217 g\nl\u22121\ni\n(13)\nwhere l \u2265 1, and Wf and bf are trainable parameters. g\n\u2032\nis the concatenation of the four vectors.\n3.5 Consistency Modeling\nInstead of directly using a softmax function in the\noutput layer, we employ conditional random field\n(CRF) to yield final emotion tags of each utterance.\n153\nOur motivation is to model the emotional consistency in a conversation, i.e., the emotion transfer.\nUsing the CRF layer enables the model to take into\naccount the dependency between emotion tags in\nneighborhoods and choose the globally best tag\nsequence for the entire conversation at once.\nFollowing the describe by Lample et al., for an\ninput set of utterances U = {u1, u2, ..., uN } and\na sequence of tag predictions y = {y1, y2, .., yN },\nyi \u2208 1, \u00b7 \u00b7 \u00b7 , K (K is number of emotion tags), the\nscore of the sequence is defined as,\nscore(U, y) = Xn\ni=0\nDyi,yi+1 +\nXn\ni=1\nBi,yi\n(14)\nwhere D \u2208 R\nK\u00d7K is the matrix of transition,\nB \u2208 R\nn \u00d7 K is the output score of the prepended\nclassification model. The model is trained to maximize the log-probability of the correct tag sequence:\nlog(p(y | U)) =\nscore(U, y) \u2212 log\n\uf8eb\n\uf8ed\nX\ny\u02dc\u2208Y\ne\nscore(U,y\u02dc)\n\uf8f6\n\uf8f8\n(15)\nwhere Y is set of all possible tag sequences. Equation 15 is computed using dynamic programming,\nwhile Viterbi applied applied to get most likely\nsequence following the work of Rabiner et al. (Rabiner, 1989).\n4 Experiments\n4.1 Datasets and Baselines\nWe evaluate our S+PAGE model on four widelyused benchmark datasets \u2013 IEMOCAP (Busso\net al., 2008), which is a audiovisual dataset consisting of dyadic conversations where actors perform\nimprovisations or scripted scenarios, MELD (Poria et al., 2018) and EmoryNLP (Zahiri and Choi,\n2018), both of which are multi-modal and multiparty datasets created from scripts of the Friends\nTV series, and DailyDialog (Li et al., 2017), which\nis a human-written dyadic dataset covering various topics about our daily life. For this work, we\nonly consider emotion recognition based on textual\nfeatures, and thus some recent ERC solutions on\nmulti-modal features (Chudasama et al., 2022; Hu\net al., 2022) are not selected as our baselines for\nfairness. The statistic of them is shown in Table 1.\nDataset # Conversations # Uterrances\nTrain Val Test Train Val Test\nIEMOCAP 120 31 5810 1623\nMELD 1038 114 280 9989 1109 2610\nDailyDialog 11118 1000 1000 87170 8069 7740\nEmoryNLP 713 99 85 9934 1344 1328\nTable 1: The statistics of the datasets.\nFor a comprehensive performance evaluation,\nwe choose CNN, CNN+cLSTM (Poria et al.,\n2017), DialogueRNN (Majumder et al., 2019) as\nbaselines of CNN and RNN-based methods, KET\n(Zhong et al., 2019) as advanced Transformerbased approach with external commonsense knowledge included, DialogueGCN (Ghosal et al., 2019),\nRGAT (Ishiwatari et al., 2020) and DAG-ERC\n(Shen et al., 2021) as GNN-based approaches. Particularly, these three GNN-based models are the\nrecent state of the art. DialogueGCN applies GCN\nto model speaker dependency, but it does not contain fine-grained positional information. Similarly,\nDAG-ERC applies a directed acyclic graph for conversation representation, which lack positional information in a conversation too. RGAT encodes\nboth speaker dependency and relative positional encoding into the edge type, and use graph attention\nnetworks to make information aggregation.\nFor the evaluation metrics, we choose microaveraged F1 for DailyDialog and weighted-average\nF1 for the other datasets, following previous works\n(Ishiwatari et al., 2020; Shen et al., 2021).\n6 Conclusion\nIn this paper, we propose a novel graph neural\nnetwork-based model, named S+PAGE, for emotion recognition in conversation (ERC). Specifically, S+PAGE contains three parts, i.e., contextual modeling, speaker dependency modeling,\nand consistency modeling. In contextual modeling, we present a new Transformer structure\nwith two-stream attention mechanism to better capture the self and inter-speaker contextual features.\nIn speaker dependency modeling, we introduce a\nnovel GNN model, named SPGCN, to refine the\nfeatures with the conversation structure information including speaker dependency and relative position information. Furthermore, we use a CRF\nlayer to model emotion transfer in the consistency\nmodeling part. Experimental results on four ERC\nbenchmark datasets demonstrate the superiority of\nour model.\n", "SumGPT3": "Introduction:\nThe paper delves into Emotion Recognition in Conversation (ERC), a burgeoning field with applications in emotional chatbots, sentiment analysis, and healthcare intelligence. Despite advancements, existing methods often overlook conversation-specific characteristics, hindering accurate emotion recognition.\n\nNovelty:\n\nContextual Modeling Enhancement: The paper introduces a Two-Stream Conversational Transformer (TSCT) to extract both self and inter-speaker contextual features, a departure from previous methods.\nSpeaker Dependency Modeling: A novel graph neural network (GNN), SPGCN, is proposed to refine features with conversation structure information, including speaker dependency and relative position, addressing limitations in prior approaches.\nConsistency Modeling with CRF: Instead of conventional softmax, a Conditional Random Field (CRF) is employed to model emotional consistency in conversations, allowing for a global understanding of emotion transfer.\nConclusion:\nThe proposed model, S+PAGE, showcases superiority over existing methods in ERC by integrating contextual, speaker dependency, and consistency modeling. Experimental results on four benchmark datasets affirm its effectiveness, underscoring its potential in advancing ERC research.\n", "SumGem": "Introduction\n\nEmotion recognition in conversation (ERC) is a task that aims to identify the emotions expressed in each utterance during a conversation. This paper proposes a novel approach, S+PAGE,  for ERC tasks.\n\nNovelty\n\nThe proposed S+PAGE model addresses the limitations of existing ERC methods by incorporating several novel aspects:\n\nTwo-Stream Conversational Transformer: This Transformer architecture captures both self-context (emotional influence within a speaker's utterances) and inter-speaker context (emotional influence between speakers) through separate attention mechanisms.\nSpeaker and Position-Aware Graph Convolutional Network (SPGCN): This GNN model refines contextual features by considering speaker dependency and relative position information within a conversation. Relative positional encoding is introduced to address the ineffectiveness of absolute positional encoding in capturing conversation order.\nConclusion\n\nThe S+PAGE model achieves superior performance compared to existing state-of-the-art models on four benchmark ERC datasets. This demonstrates the effectiveness of the proposed two-stream attention mechanism for contextual modeling and SPGCN for incorporating speaker dependency and relative position information.", "SumGPT4": "Introduction\nThe paper focuses on Emotion Recognition in Conversation (ERC), which is increasingly important across various applications such as emotional chatbots, social media sentiment analysis, and healthcare. ERC aims to identify the emotions in each utterance within a conversation, a task that is challenging due to the dynamic nature of dialogue and the need for contextual understanding. The traditional models, primarily based on recurrent neural networks (RNNs), have struggled with capturing distant contextual information and handling large-scale, multi-party conversations.\n\nNovelty\nThe paper introduces several novel aspects in the ERC field:\n\nTwo-Stream Conversational Transformer (TSCT): This new transformer architecture is designed to capture both self-context (intra-speaker) and inter-speaker contextual features within conversations. It uses a dual attention mechanism that distinguishes between utterances by the same speaker and those from different speakers, improving the model's ability to understand emotional dynamics.\nSpeaker and Position-Aware Graph Neural Network (SPGCN): The paper proposes a novel graph neural network that integrates speaker dependency and relative position information within a conversation graph. This approach allows for a more nuanced understanding of the conversation structure and the relationships between utterances.\nEnhanced Relational Graph Convolution Network (R-GCN) with Relational Relative Positional Encoding: By introducing relational relative positional encoding, the model better captures the fine-grained positional nuances of conversation, which traditional models often overlook.\nUse of Conditional Random Field (CRF) for Global Consistency Modeling: The CRF layer is employed to ensure global emotional consistency across the conversation, taking into account the dependency between successive emotional tags to produce more coherent emotional recognition.\nConclusion\nThe paper's proposed model, S+PAGE, showcases significant improvements over existing state-of-the-art models by effectively integrating both contextual features and conversation structure information. Extensive experiments on multiple ERC benchmark datasets validate the effectiveness of the new model components, demonstrating that the proposed approach achieves competitive performance. This advancement in ERC modeling could greatly enhance the capability of various real-world applications in understanding and interacting with human emotions more effectively.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary is concise and effectively highlights the key innovations of the paper, such as the Two-Stream Conversational Transformer and the novel graph neural network (SPGCN). It also mentions the use of CRF for consistency modeling, which is a significant aspect of the paper. However, it lacks some of the detailed explanations and contextual background provided in Summary 3. While it is clear and to the point, it doesn't delve as deeply into the specifics of the models and the experimental validation as Summary 3 does.", "expGem": "This summary provides a solid overview of the paper's contributions but is somewhat repetitive and less detailed compared to the other summaries. It repeats some of the points about the Two-Stream Conversational Transformer and SPGCN without adding much new information. The explanation of the use of relative positional encoding and the CRF for consistency is mentioned but not as clearly or thoroughly as in the other summaries. It feels slightly less organized and comprehensive, making it the least effective of the three.", "expGPT4": "This summary provides a comprehensive and clear explanation of the paper's contributions and context. It offers detailed descriptions of the novel components such as the Two-Stream Conversational Transformer (TSCT), Speaker and Position-Aware Graph Neural Network (SPGCN), and Enhanced Relational Graph Convolution Network (R-GCN). It also explains the role of Conditional Random Field (CRF) for global consistency modeling. The summary is well-structured and captures the novelty and significance of the proposed model, S+PAGE, while mentioning the experimental validation on multiple datasets. This thoroughness makes it the most informative and complete summary."}, "9": {"title": "Neural Network Alignment for Sentential Paraphrases", "url": "https://aclanthology.org/P19-1467", "text": "1 Introduction\nMonolingual paraphrase alignment is an active\narea of research, with applications in many natural language processing tasks, such as text-to-text\ngeneration (Barzilay and Elhadad, 2003; Barzilay and McKeown, 2005), natural language inference (MacCartney et al., 2008), and recognizing\ntextual similarity (Sultan et al., 2014b). Madnani\nand Dorr (2010) identify three levels of paraphrasing. The first is lexical paraphrasing, where individual words are replaced by synonyms or hypernyms. The second, phrasal paraphrasing, involves equivalent idiomatic phrases, such as verbpreposition combinations (eg. \u201ctake over\u201d or \u201cassume control of\u201d), or syntactic transformations,\nsuch as active versus passive voice.\nIn this work, we focus on the third: sentential\nparaphrasing. Sentential paraphrasing can trivially\nbe achieved by performing lexical and phrasal\nparaphrasing on parts of a sentence, but Madnani\nand Dorr note that more interesting paraphrases,\nsuch as \u201cHe needed to make a quick decision in\nthat situation\u201d and \u201cThe scenario required him to\nmake a split-second judgment,\u201d are challenging.\nPast work has focused on lexical and short\nphrasal alignments, in part because most existing corpora consist of mostly word-level alignments. Yao et al. (2013b) report that 95% of alignments in the MSR RTE (Brockett, 2007) and Edinburgh++ (Cohn et al., 2008) corpora are singletoken, lexical paraphrases, and phrases of four or\nmore words are less than 1% of MSR RTE and 3%\nof Edinburgh++.\nIn this work, we present a monolingual aligner\nfor long phrasal and sentential paraphrases. Our\ncontributions are as follows:\n\u2022 Our pointer-network-based system aligns\nphrases of arbitrary length.\n\u2022 Our system aligns directly at the phrase level\nby composing the semantics of the words in\neach phrase into a single representation of the\nmeaning of the entire phrase.\n\u2022 We conduct experiments on aligning long\nparaphrases using the summarization corpus\nof Ouyang et al. (2017), the first use of this\ncorpus for the alignment task, as well as the\nMSR RTE corpus (Brockett, 2007).\n\u2022 We achieve significant increases in recall\n(over 75 points) while also maintaining a\nstrong lead in F-measure on aligning long\nparaphrases (involving phrases of four or\nmore words), compared with existing stateof-the-art word- and phrase-based aligners.\n2 Related Work\nThe development of monolingual alignment as an\nindependent natural language processing task began with the release of the Microsoft Research\nRecognizing Textual Entailment (MSR RTE) corpus (Brockett, 2007), which consists of 1600 sentence pairs, divided evenly into training and testing sets, annotated with alignments. To date, there\nare only five phrase-based monolingual aligners in\nexistence, not including this work.\n4725\nThe first aligner developed using the MSR RTE\ncorpus, MANLI (MacCartney et al., 2008), set\na precedent for monolingual alignment research:\nthe possible alignments in the MSR RTE were\nnot used, following conclusions drawn in machine\ntranslation research that training using possible\nalignments does not improve the performance of\nmachine translation systems. As we show in Section 4, this decision, which has been followed\nby subsequent MSR RTE systems, removed from\nconsideration nearly all of the long alignments\n(four or more words) in the corpus.\nMANLI is a phrase-based system, capable of\naligning multiple source tokens to multiple target\ntokens. However, MacCartney et al. found that\nconstraining it to align only at the word level (ie.\nsetting a maximum phrase length of 1) decreased\nthe system\u2019s F-measure by only 0.2%, suggesting\nthat this early work was not yet able to represent\nthe meanings of multi-word phrases as well as it\ncould represent the meanings of single words.\nThadani and McKeown (2011) extended\nMANLI by introducing syntactic constraints on\nalignment, improving the system\u2019s precision,\nand used integer linear programming to perform\nfaster, exact decoding, rather than the slower,\napproximate search used by the original system.\nThadani et al. (2012) added dependency arc edits\nto MANLI\u2019s phrase edits, again improving the\nsystem\u2019s performance. Interestingly, Thadani et\nal. used both the sure and possible alignments\nin the Edinburgh++ corpus (Cohn et al., 2008)\nand showed that training on both gave better performance than training only on sure alignments\non this corpus, but no subsequent monolingual\nalignment systems have taken advantage of\npossible alignments until we do so this work.\nThe current state-of-the-art phrase-based\nmonolingual alignment system is JacanaAlignphrase (Yao et al., 2013b), the phrase-based\nextension of JacanaAlign-token (Yao et al.,\n2013a). Yao et al. use a semi-Markov CRF to tag\neach token or sequence of tokens in the source\nsentence with the indices of aligned target token.\nTo train this system, they synthesized phrasal\nalignments by merging consecutive lexical alignments among the MSR RTE sure alignments;\nhowever, even after doing so, they found that\nlong alignments involving phrases of four or more\nwords still made up less than 1% of the corpus.\nYao et al. found that the phrase-based JacanaAlign\nperformed slightly worse than the token-based\nversion, likely due to the overwhelming majority\nof alignments in their test set being at the token\nlevel and the token-based annotations in the test\nset penalizing their phrase-based alignments.\nJacanaAlign-phrase is the fastest existing\nphrase-based aligner (there are only four others: MANLI, its two extensions, and SemAligner, all described in this section), but Yao et\nal. note that it is roughtly 30-60 times slower than\nJacanaAlign-token. Of particular interest to us is\nthat the decoding time of JacanaAlign-phrase is\nO\n\nLsL\n2\ntMN2\n\n, where Ls and Lt are the maximum allowed phrase lengths, and M and N are the\nsentence lengths, for the source and target, respectively. The longer the phrases being aligned, the\nlonger Jacana-Align will need to run \u2013 we avoid\nthis dependence on phrase length in this work.\nFinally SemAligner (Maharjan et al., 2016), like\nthis work, chunks input sentences into phrases before alignment. However, it was designed for and\nevaluated on the semantic textual similarity task,\nso its published performance cannot be compared\nwith those of monolingual alignment systems.\n\n3 Models\nOur system first chunks the source and target sentences several times, at different levels of granularity, from mostly single words to phrases to whole\nclauses, then computes a chunk embedding in a\ndistributed semantic space for each chunk (Section 3.1). We call any segmentation of a sentence\ninto chunks a chunking of that sentence. We pair\neach source chunking with each target chunking\nand use a pointer-network (Vinyals et al., 2015)\nto perform a preliminary alignment of each source\nchunk to all target chunks (Section 3.2). Finally,\nwe combine the preliminary alignments from all\nsource/target chunking pairs using a voting system\nto produce the final alignment from the source sentence to the target sentence (Section 3.3). Implementation details for our model are given in Appendix A in the supplementary material.\n3.1 Chunkings and Chunk Embeddings\nWe chunk the source and target sentences using\nconstituent parsing (Bauer, 2014). We consider\nall nodes with phrase-level tags (XP) to be constituents. Beginning with the leaves, we move up\nthe tree, deleting any node that is wholly contained\nin a larger constituent but that is neither a con\n4726\nI attended a wedding which offered no dinner at the reception\nFigure 1: All potential chunk boundaries.\nS\nVP\nNP\nSBAR\nS\nPP\nNP\nthe reception\nIN\nat\nNP\nno dinner\nVBD\noffered\nWHNP\nwhich\nNP\na wedding\nVBD\nattended\nNP\nI\nFigure 2: A simplified constituent tree.\nstituent itself, nor the sibling of a constituent. Figure 2 shows a simplified constituent tree.\nConstituents and their siblings are the smallest\npossible chunks that we consider. In the example\nconstituent tree above, there are eight such small\nchunks. We can also merge any number of consecutive, small chunks to form a larger chunk: \u201coffered,\u201d \u201cno dinner,\u201d \u201cat,\u201d and \u201cthe reception,\u201d for\ninstance, can be merged to form \u201coffered no dinner at the reception.\u201d In a sentence with i of these\nsmallest chunks, there are i \u2212 1 potential chunk\nboundaries (Figure 1). Since merging two adjacent chunks is equivalent to ignoring the chunk\nboundary between them, there are 2\ni\u22121 unique\nchunkings of the sentence. Note that each token\nin the sentence is contained in only one chunk in\neach chunking of that sentence.\nFrom the example sentence above, we obtain\n128 unique chunkings. The coarsest consists of a\nsingle chunk containing the entire sentence, and\nthe most fine-grained has each leaf of the constituent tree as a separate chunk. We do not choose\na single chunking to use, but rather represent a sentence by all its possible chunkings. This allows us\nto align at any level of granularity, from mostly\nwords to full sentences. The multiple chunkings also have the practical benefit of increasing\nthe amount of training data available, with each\nchunking providing another training instance.\nTo represent the meaning of a chunk as a whole,\nwe look to recent work in composing word embeddings into phrase- or sentence-level embeddings. Since Mitchell and Lapata (2008), there has\nbeen a great deal of interest in learning phrase embeddings (Baroni and Zamparelli, 2010; Zanzotto\net al., 2010; Yessenalina and Cardie, 2011; Socher\net al., 2012; Grefenstette et al., 2013; Mikolov\net al., 2013; Yu and Dredze, 2015). In this work,\nwe generate chunk embeddings using the LSTM\nlanguage model of Hill et al. (2016)\n1\n. The model\nis trained on dictionaries: it takes as input a dictionary definition, in the form of a sequence of word\nembeddings, and produces as output the embedding of the word to which the definition belongs,\nthus learning to compose the embeddings of the\nwords into a single embedding representing the\nentire phrase or sentence. By representing each\nchunk by a single chunk embedding, we are able\nto align chunks of arbitrarily large size with only\nthe language model\u2019s run time as overhead.\n3.2 Preliminary Alignment\nFor a given source sentence chunking and target sentence chunking, we perform a preliminary alignment using a neural network aligner inspired by the pointer network of Vinyals et al.\n(2015). Most previous work on neural network\nalignment used feed-forward, recurrent, or convolutional neural networks to score source-target\nword pairs and then fed these scores to a traditional alignment model, such as an HMM or a\ngreedy aligner (Yang et al., 2013; Tamura et al.,\n2014; Legrand et al., 2016), rather than using the\nneural network itself to predict the alignments.\nThis is due to the difficulty of adapting a neural\nnetwork to the alignment task directly: two input\nsequences of unknown and often different lengths,\nas well as an output set of unknown size.\nOur neural network aligner is based on the\npointer network and learns a distribution over an\noutput dictionary of variable size. The flexibility of the output size makes the pointer network\nwell-suited to our task of aligning chunkings of\nvariable length. We fix a source chunk from the\nsource chunking under consideration and adapt the\npointer network to predict a preliminary alignment\nover the entire target chunking:\na\ni\nj = v\nT\ntanh(W1ei + W2cj )\nwhere ei\nis the embedding for chunk i in the\nsource chunking, cj is the embedding for candi1We experimented with averaging word embeddings, but\nthis approach underperformed the language model.\n4727\nWe were expecting a buffet to be set up, but there was nothing\nei\nc0 c1 c2 c4\nI attended a wedding which offered no dinner at the reception\n.14\na\ni\n0\n.09\na\ni\n1\n.67\na\ni\n2\n.10\na\ni\n3\nFigure 3: The pointer network performing preliminary\nalignment a given source chunk and target chunking.\ndate chunk j in the target chunking, and v, W1,\nand W2 are learned parameters. (For convenience,\nin subsequent sections we use ei and cj to refer to both the chunk embeddings, which are vectors, and to the chunks themselves, which are sequences of tokens.) The chunk embeddings are\ngenerated by the LSTM language model described\nin the previous section, and are fixed at training\ntime. For each source chunk i, the pointer network\nproduces a distribution over all candidate chunks\nin the target chunking. Figure 3 shows the pointer\nnetwork aligning a source/target chunking pair.\n3.3 Voting and Final Alignment\nFor a fixed source chunking and a fixed source\nchunk i, the pointer network produces one preliminary alignment for each unique chunking of\nthe target sentence. We perform this preliminary\nalignment for all source chunks in all chunkings\nof the source sentence. By aligning preliminary\nalignments for all combinations of source and target chunkings, we are able to defer deciding the\nlengths of the spans we align, instead allowing the\nvoting procedure to discover them.\nThe final output of our system is aligned token\npairs. This is due to our voting procedure, which\nis described in Figure 4. Because the preliminary\nalignments are performed on chunkings of different granularities, we must vote at the level of the\nsmallest possible chunks (the leaves in the constituent tree). Since it is not possible for the tokens\nwithin one of these smallest possible chunks to receive different amounts of votes (to do so would\nrequire the tokens to be in two different chunks\nin some chunking), and since the standard evaluation for monolingual alignment consists of precision, recall, and F-measure for token pairs \u2013 even\nfor phrase-based models \u2013 we simply vote on token pairs; each token pair inherits the preliminary\nalignment value of the source and target chunks\nInputs\n\u2022 the source sentence W\n\u2022 the target sentence U\n\u2022 the set of source sentence chunkings S\n\u2022 the set of target sentence chunkings T\nInitialize\n\u2022 set score(w, u) = 0 for tokens w \u2208 W and u \u2208 U\nRepeat for ei \u2208 s, for (s, t) \u2208 S \u00d7 T\n\u2022 predict preliminary alignment a\ni\n\u2022 add a\ni\nj\nto score(w, u) for tokens w \u2208 ei and u \u2208 cj\nRepeat for w \u2208 W\n\u2022 sum-to-one normalize score(w, u) for u \u2208 U\n\u2022 sort pairs (w, u) by score(w, u) in descending order: score(w, u1) > . . . > score(w, um)\n\u2022 select max k such that score(w, uk) > 1/(k + 1)\n\u2022 set Aw = {(w, u1), . . . ,(w, uk)}\nReturn S\nw\u2208W Aw\nFigure 4: Voting procedure for final output.\ncontaining them. The longer aligned phrases that\ncorrespond to these aligned token pairs can be\neasily constructed: following MacCartney et al.\n(2008) and Yao et al. (2013b), two tokens are\naligned if and only if the phrases containing them\nare aligned.\nIntuitively, only one chunk eis\nin a given source\nchunking s contains the token w, and only one\nchunk cjt\nin a given target chunking t contains\nthe token u. Here, is and jt\nindicate the specific\nsource and target chunks that contain the tokens\nw and u, respectively. The token-level scores are\nobtained by summing the preliminary alignment\nvalues for all source/target chunk pairs where the\nsource chunk contains w and the target chunk contains u:\nscore(w, u) = X\ns\u2208S\nX\nt\u2208T\na\nis\njt\nwhere S is the set of all source chunkings of the\nsource sentence, T is the set of all chunkings of the\ntarget sentence, and a\nis\njt\nis the preliminary alignment value described in the previous section.\nFor a fixed source token w, we normalize its\nscores to produce a probability distribution over\nall target tokens. We select the k highest-scoring\ntarget tokens such that the score of each token is\ngreater than 1/(k + 1). If we select four target tokens, for example, each has a score of at least 0.2,\nand the next-highest-scoring token has a score of\nless than 0.167. Intuitively, we are looking for a\nlarge gap in the target token scores at which to\ncut off the selected tokens from the unselected\ntokens; the sum of the scores of all unselected\ntokens is less than the score of any selected token. We select the largest possible number of tar-\n4728\nVery rarely do I get a \u201cthanks\u201d or a smile of appreciation.\nI never get any thanks.\nI had a sleep paralysis dream that I was abducted by aliens.\nI had the alien abduction dream.\nFigure 5: Examples of long alignments from Ouyang et al.\u2019s summarization corpus.\nTilda Swinton has a prominent role as the White Witch.\nTilda Swinton plays the part of the White Witch.\nFigure 6: An MSR RTE pair, slightly edited for length, with\nsure alignments bolded and possible alignments italicized.\nget tokens for which this requirement holds. This\nflexible threshold ensures that the selected tokens\nu1, . . . uk have much larger scores than the unselected tokens uk+1, . . ., um while still allowing\nany number of tokens to be selected. The selected\ntarget tokens are then aligned to the source token\nw to produce aligned token pairs. The final output of our system is the union of the aligned token\npairs for each source token in the source sentence.\nn\nWe have presented a pointer-network-based system for aligning longer paraphrases. This pointeraligner uses an LSTM language model to compose\nthe embeddings of words in a chunk into a chunk\nembedding and and then aligns these chunks. It\nis able to align arbitrarily long phrases, automatically discovering the best phrase length, from individual words to full sentences, at which to align a\ngiven input sentence pair, and it significantly outperforms existing phrase-based aligners at aligning long phrases with high semantic similarity but\nlow lexical overlap. Our system achieves high recall but suffers from imprecise alignment boundaries. In future work, we intend to refine these\nalignment boundaries and to optimize the alignment procedure for speed. We hope that this work\nwill raise more interest in developing alignment\nsystems for longer paraphrase", "SumGPT3": "Introduction\n\nMonolingual paraphrase alignment is crucial in various NLP applications, but existing methods mainly focus on lexical and short phrasal alignments, lacking efficiency in aligning longer paraphrases.\n\nNovelty\n\nPointer-Network-Based System: The paper introduces a novel approach based on pointer networks. This system aligns phrases of arbitrary length by composing the semantics of words into a single representation of the entire phrase.\n\nDirect Phrase-Level Alignment: Unlike previous methods, the system aligns directly at the phrase level, rather than performing word-level alignment and then merging to form phrases. This approach enhances alignment accuracy, especially for longer phrases.\n\nAlignment Flexibility: The system aligns at any level of granularity, from individual words to full sentences, automatically discovering the optimal phrase length for alignment. This flexibility improves alignment quality, particularly for long paraphrases with high semantic similarity but low lexical overlap.\n\nPerformance Improvement: Experimental results demonstrate significant increases in recall while maintaining a strong lead in F-measure for aligning long paraphrases compared to existing state-of-the-art aligners. This improvement indicates the effectiveness of the proposed approach in capturing meaningful paraphrase alignments.\n\nConclusion\n\nThe paper presents a novel pointer-network-based system for aligning longer paraphrases, addressing limitations of existing methods. Despite achieving high recall, the system faces challenges with precise alignment boundaries, suggesting avenues for future refinement and optimization. Overall, the study highlights the importance of developing alignment systems capable of handling longer paraphrases effectively.", "SumGem": "Introduction\n\nMonolingual paraphrase alignment is a task that aims to identify phrases in a sentence that express the same meaning as phrases in another sentence. This task is useful in many natural language processing applications, such as text summarization and natural language inference. This paper introduces a new system for monolingual paraphrase alignment that is able to align longer paraphrases than previous systems.\n\nNovelty\n\nPointer-network based aligner: This paper proposes a novel system for monolingual paraphrase alignment that utilizes a pointer network. The pointer network is able to align phrases of arbitrary length by composing the meanings of the words in each phrase into a single representation. This allows the system to avoid the limitations of previous systems that were restricted to aligning phrases of a fixed length.\nLSTM language model for chunk embedding: The system uses an LSTM language model to create a compressed representation (embedding) of each chunk. This embedding captures the meaning of the entire chunk, rather than just the individual words within the chunk. This allows the system to perform alignment based on semantics rather than just surface-level similarity.\nFlexible threshold for selecting target tokens: The system employs a flexible threshold to select the target tokens that are aligned to a source token. This threshold ensures that the selected tokens have significantly higher scores than the unselected tokens, which helps to improve the precision of the alignments.\nConclusion\n\nThe system presented in this paper is able to achieve high recall when aligning long paraphrases. However, the system also suffers from imprecise alignment boundaries. Future work will focus on refining these alignment boundaries and improving the efficiency of the alignment procedure. Overall, this paper presents a significant advancement in the field of monolingual paraphrase alignment by introducing a system that can handle longer paraphrases and achieve high semantic similarity in the alignments.", "SumGPT4": "**Introduction**\nThe paper explores the domain of monolingual paraphrase alignment, focusing specifically on sentential paraphrasing which poses significant challenges in natural language processing (NLP). Traditional approaches have predominantly concentrated on lexical and short phrasal alignments, largely due to limitations in existing corpora that mostly encompass word-level alignments. This research introduces a new system designed to handle longer, more complex phrasal and sentential paraphrases.\n\n**Novelty**\n1. **Pointer-Network-Based System**: The paper presents a novel aligner based on pointer networks, distinct from the conventional aligners that focus primarily at the word or short phrase level. This system is specifically designed to manage alignments of arbitrary lengths, from individual words to entire sentences.\n2. **Phrase-Level Alignment through Semantic Composition**: Unlike previous systems that align at the word level, this aligner composes the semantics of words within a phrase into a unified representation, enhancing the ability to recognize and align longer and semantically complex phrases.\n3. **Use of Multiple Chunkings for Alignment**: The system introduces an innovative approach of using multiple chunkings of sentences, which significantly increases the granularity of alignment and utilizes the semantic space more effectively.\n4. **Experimental Approach on New and Existing Corpora**: The aligner is tested on the summarization corpus of Ouyang et al. (2017) for the first time in alignment tasks, in addition to the MSR RTE corpus. This diversification in testing materials helps in evaluating the system's effectiveness across different types of data.\n\n**Conclusion**\nThe introduction of a pointer-network-based system for monolingual paraphrase alignment marks a significant advancement in handling longer paraphrases with high semantic similarity but low lexical overlap. This system not only achieves notable improvements in recall and F-measure over existing state-of-the-art aligners but also suggests potential in further refining alignment boundaries and optimizing performance. This study pushes the boundaries of what's possible in NLP alignments and opens up new avenues for future research and application in complex text processing tasks.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "Strengths:\n\nFocused on Novelty and Performance: This summary highlights the main innovative aspects of the proposed system, such as the pointer-network-based approach and direct phrase-level alignment.\nPerformance Metrics: It mentions the significant improvements in recall and F-measure, indicating the effectiveness of the new approach.\nFlexibility: The summary emphasizes the system's flexibility in aligning phrases of various lengths, which is a key advantage.\nWeaknesses:\n\nLess Detail on Methodology: Compared to Summary 3, it provides less detail on the methodology and specific innovations like the use of multiple chunkings.\nBrief Conclusion: The conclusion is brief and doesn't fully capture the implications of the findings or potential future work.", "expGem": "Strengths:\n\nClear Description of Techniques: This summary explains the use of pointer networks and LSTM language models for chunk embedding clearly.\nFocus on Semantic Alignment: It highlights the system's ability to perform alignment based on semantics rather than surface-level similarity, which is a significant improvement.\nWeaknesses:\n\nLack of Novelty Details: It lacks detailed explanations of some key novel aspects, such as the use of multiple chunkings or testing on different corpora.\nLess Emphasis on Performance: The summary doesn't provide detailed performance metrics or comparisons with state-of-the-art systems, which are crucial for evaluating the system's effectiveness.\nConclusion: The conclusion is less impactful and doesn't clearly state the broader implications or potential future research directions.", "expGPT4": "Strengths:\n\nComprehensive and Detailed: This summary provides a detailed explanation of the novelty, methodology, and experimental approach of the proposed system. It clearly outlines the problem, the innovative solutions offered, and the specific contributions of the paper.\nStructured Presentation: The summary is well-structured, with clear sections for the introduction, novelty, and conclusion. This helps in understanding the flow of the paper.\nSpecific Innovations Mentioned: It mentions unique aspects such as the use of multiple chunkings and the application of the aligner on new and existing corpora. This gives a deeper insight into the research's novelty and breadth.\nExperimental Approach: By highlighting the use of different corpora for testing, it shows the practical applicability and robustness of the proposed system.\nWeaknesses:\n\nComplexity: The detailed explanation might be overwhelming for readers looking for a concise overview."}, "10": {"title": "Multilingual segmentation based on neural networks and pre-trained word embeddings", "url": "https://aclanthology.org/W19-2716", "text": "1 Introduction\nThe need to understand and automatically process texts motivates the construction of discourse\nparsers. Nowadays, discourse parsing is a challenging task, essential to correctly perform other\nNLP interesting tasks such as sentiment analysis,\nquestion answering, summarization, and others.\nDiscourse parsing is usually divided into two main\nsteps: i) text segmentation (discourse segmentation) which is done automatically with a discourse\nsegmenter, and ii) relation identification linking\nthe segments using rhetorical relations (discourse\nparsing).\nAs Iruskieta and Zapirain (2015) report, segmentation proposals are based on the following\nthree basic concepts, or some combinations of\nthese basic concepts:\n\u2212 Linguistic \u201cform\u201d (or category).\n\u2217All authors contributed equally.\n\u2212 \u201cFunction\u201d (the function of the syntactical\ncomponents).\n\u2212 \u201cMeaning\u201d (the coherence relation between\npropositions).\nSome segmentation guidelines follow the same\nfunction-form based approach, in different languages. For instance, Tofiloski et al. (2009) for\nEnglish, Iruskieta et al. (2015) for Basque and\nda Cunha et al. (2012) for Spanish. Following\nthis approach, we consider an Elementary Discourse Units (EDU) to be a text span functioning as an independent unit. Under this view, only\nmain clauses and adverbial clauses1 with a verb\n(form constraint) are EDUs. Other subordinate\nclauses such as complements \u2014functioning as\nnoun phrases\u2014 and relative clauses \u2014functioning\nas noun modifiers\u2014 are not considered to be\nEDUs.\nThe first step to annotate a text is to identify\nEDUs. The aim of discourse segmentation is\nto identify all the EDUs in the text. Note that\ngranularity of an EDU is nowadays controversial\neven under the same theoretical approach (van der\nVliet, 2010) and granularity is determined in each\nannotation project.\nFrom our point of view, these are the main problems to tackle when pursuing discourse segmentation:\n\u2212 Circularity: segmenting and annotating\nrhetorical relations at the same time. It happens if we use a relation list that includes the\nATRIBUTION relation because between the\nsegmented EDUs there is no other competing\nrelation.\n\u2212 SAME-UNIT: a clause embedded in another\nclause. Discourse markers and other kind of\nsyntactic structures guide the reader, splitting\n1\nFunctioning as modifiers of verb phrases or entire\nclauses, and providing the main clause with a (discourse) thematic role.\n126\nLanguage forms considered as EDUs\nClause type Example\nIndependent sentence [Whipple (EW) gaixotasunak hesteei eragiten die bereziki.]1 GMB0503\n[Whipple\u2019s (EW) disease usually affects to the intestine.]1\nMain, part of sentence [pT1 tumoreko 13 kasuetan ez zen gongoila inbasiorik hauteman;]1 [aldiz, pT1 101 tumoretatik 19 kasutan (18.6%) inbasioa hauteman zen, eta pT1c tumoreen artetik 93 kasutan\n(32.6%).]2 GMB0703\n[In 13 cases of tumour pT1, no invasive ganglia was detected;]1 [on the other hand, 19\ninvasive pT1 tumours (18.6%) and PT1c tumours were detected in 93 cases (32.6%).]2\nFinite adjunct [Haien sailkapena egiteko hormona hartzaileen eta c-erb-B2 onkogenearen gabeziaz\nbaliatu gara,]1 [ikerketa anatomopatologikoetan erabili ohi diren zehaztapenak direlako.]2\nGMB0702\n[We have used the classification of their hormone receptors and c-erb-B2 oncogenetics]1\n[because they are the specifics used in anatomopathological studies.]2\nNon-finite adjunct [Ohiko tratamendu motek porrot eginez gero,]1 [gizentasun erigarriaren kirurgia da epe\nluzera egin daitekeen tratamendu bakarra.]2 GMB0502\n[If the usual treatment fails,]1 [the surgical treatment of graft is the only treatment that can\nbe done in the long term.]2\nNon-restrictive relative [Dublin Hiriko Unibertsitateko atal bat da Fiontar,]1 [zeinak Ekonomia, Informatika eta\nEnpresa-ikasketetako Lizentziatura ematen baitu, irlanderaren bidez.]2 TERM23\n[Fiontar is a section of the University of Dublin City,]1 [which teaches a Bachelor of Economics, Computing and Business Studies, through Ireland.]2\nTable 1: Main clause structures in Basque\nthe clause in two spans sometimes. Consequently, only one of the spans will satisfy the\nEDU constraints of form and function, making more challenging discourse segmentation\nand discourse parsing. 2\nWe present in Table 1 examples of different\nclause types in Basque (and translations) showing\nthe ones that could potentially be EDUs. This table follows the notion of hierarchical downgrading (Lehmann, 1985) that goes from independent\nstructures (EDUs) to subordinated clauses (noEDUs). This notion will be very useful to understand which is the granularity adopted by the multilingual segmenter in two language: Basque and\nSpanish.\n2 Related works\nAfter Ejerhed (1996) published the first English\nsegmenter for RST, several segmenters were built\nfor different languages.\n\u2212 For English, Le Thanh et al. (2004) developed a segmenter in the framework of the\n2Note that for example, this kind of structures is\nwidespread. For example, SAME-UNIT structure affects to\n12.67% (318 of 2,500) of the segments in the Basque RST\ntreebank.\nPDTB and Tofiloski et al. (2009) developed\nan rule based segmenter under RST.3\n\u2212 For German, Lungen et al. \u00a8 (2006) developed\na segmenter.\n\u2212 For French, Afantenos et al. (2010) developed an EDU segmenter based on machine learning techniques in the framework\nof SDRT.\n\u2212 For Brazilian Portuguese, a segmenter which\ncan be used easily online for first time,4\nwhich is the first step of the RST DiZer parser\n(Maziero et al., 2011) in RST.\n\u2212 For Dutch, van der Vliet (2010) build a rulebase segmenter in RST.\n\u2212 For Spanish, (da Cunha et al., 2012) developed a rule-based segmenter under RST.5\n\u2212 For Arabic, Keskes et al. (2012) built a\nclause-based discourse segmenter in RST.\n\u2212 For Thai language Ketui et al. (2013) developed a rule based segmenter in RST.\n3English spoken language was also studied by Passonneau\nand Litman (1993).\n4Available at http://143.107.183.175:21480/\nsegmenter/.\n5Available at: http://dev.termwatch.es/esj/\nDiSeg/WebDiSeg/.\n127\nLanguage Corpus Dataset Docs Sents Toks EDUs\nBasque eus.ert\nTrain 84 990 21,122 1,869\nDev 28 350 7,533 656\nTest 28 100 3,813 549\nSpanish\nspa.sctb\nTrain 32 304 10,249 473\nDev 9 74 2,450 103\nTest 9 100 3,813 168\nspa.rststb\nTrain 203 1,577 43,034 2,474\nDev 32 256 7,531 419\nTest 32 303 8,026 456\nPortuguese por.cstn\nTrain 110 1,595 44,808 3,916\nDev 14 232 6,233 552\nTest 12 123 3,615 265\nFrench fra.sdrt\nTrain 64 880 22,278 2,032\nDev 11 227 4,987 517\nTest 11 211 5,146 680\nEnglish eng.gum\nTrain 78 3,600 67,098 5,012\nDev 18 784 15,593 1,096\nTest 18 890 15,924 1,203\nGerman deu.pcc\nTrain 142 1,773 26,831 2,449\nDev 17 207 3,152 275\nTest 17 213 3,239 294\nTable 2: Corpus for Segmentation tasks.\n\u2212 For Basque, Iruskieta et al. (2013) created\nthe Basque RST Treebank and Iruskieta and\nZapirain (2015) developed also a rule-based\nsegmenter in RST.6\nAs mentioned before, the segmentation task\nis the first elemental stage in discourse parsing.\nSome English parsers (Joty et al., 2015; Feng and\nHirst, 2014; Ji and Eisenstein, 2014) and Portuguese parsers (Pardo and Nunes, 2004) \u2013just to\ncite some\u2013 have their segmenter. Braud et al.\n(2017) proposed a multilingual (English, Basque,\nSpanish, Portuguese, Dutch and German) discourse parser, where each analyzed language has\nits own segmenter.\n3 Resources and Methods\n3.1 Corpora\nThe segmenter has been tested on 6 languages and\n7 treebanks. Table 2 shows the information of the\nselected treebanks.7\n3.2 Features for discourse segmentation\nWe employed both lexicalized (word embeddings and character embeddings) and delexicalized (UPOS, XPOS and ATTRs) features. When\nwe refer to lexicalized features, we used external word embeddings for all languages (Basque\nincluded) and IXA team calculated word embeddings exclusively for Basque:\n1. External word embeddings: 300-dimensional\nstandard word embeddings using Facebook\u2019s\nFastText (Bojanowski et al., 2017);\n2. IXA team calculated word embeddings:\nBasque word embeddings were calculated\non the Elhuyar web Corpus (Leturia, 2012)\nusing gensim\u2019s (Reh \u02c7 u\u02da\u02c7rek and Sojka, 2010)\nword2vec skip-gram (Mikolov et al., 2013).\nThey have a dimension of 350, and we employed a window size of 5. The Elhuyar Web\ncorpus was automatically built by scraping\nthe web, and it contains around 124 million\nBasque word forms.\nWe pursued the discourse segmentation phase in\n6Available at http://ixa2.si.ehu.es/\nEusEduSeg/EusEduSeg.pl.\n7\nFor more information https://github.com/\ndisrpt/sharedtask2019#statistics.\n128\nToken WordForm Lema POS CASE Head Func. EDU\n1 Ernalketa ernalketa NOUN Case=Abs|Number=Sing 2 obl BeginSeg=Yes\n2 gertatzeko gertatu VERB Case=Loc 3 advcl\n3 espermatozoideek espermatozoide NOUN Case=Erg|Number=Plur 5 nmod BeginSeg=Yes\n4 emearen eme NOUN Case=Gen|Number=Sing 5 nmod\n5 umetoki-tronpara umetoki-tronpa NOUN Case=All|Number=Sing 6 obl\n6 heldu heldu VERB VerbForm=4Part 8 xcomp\n7 behar behar NOUN Case=Abs 8 compound\n8 dute ukan VERB Aspect=Prog|Mood=Ind 0 root\n9 , , PUNCT 8 punct\nTable 3: A training example sentence of BIZ04.\ntwo steps following the form-function approach:\n1. Preprocess the data to obtain the features\ncorresponding to each word. The preprocess\nresults in the input for BiLSTM+CRF, more\nprecisely: a) The word embedding. b) The\nPOS (if the language provided it otherwise\nCPOS). c) The syntactic relation concatenated:\n\u2013 to the case mark or the subordination\nmark (Basque and German) and\n\u2013 to the gerund mark, if the POS of the\nverb had this label (Spanish).\n2. Employ a BiLSTM+CRF to perform the actual segmentation.\nInstead of randomly initializing the embedding layer, we employed the aforementioned pretrained word embeddings.\nWe used the morphological and syntactic information provided by the Shared Task; the case and\nsubordination mark associated to each word was\nobtained using UDPipe (Straka et al., 2016).\n(1) Ernalketa gertatzeko espermatozoideek\nemearen umetoki-tronpara heldu behar dute,\nIn order to occur the fertilization, sperm\nmust reach the uterus stem of the female,\n[TRANSLATION]\nTable 3 and the dependency tree in Figure 1\nshows the information provided by the Shared\nTask Data of the Example (1).\nLSTM (Hochreiter and Schmidhuber, 1997)\nneural networks are widely used for sequential labelling where the input-output correspondence depends on the previously tagged elements. This dependency gets realized, at each time step, in the\ncorresponding LSTM cell by using as input for\neach hidden state, the output of the previously hidden state as shown in Fig 2. So, the segmentation\nprocess consists of obtaining an input sequence\nFigure 1: Dependency tree of BIZ04 with Arborator https://arborator.github.io/live.\nhtml\n(x1, x2, x3, \u00b7 \u00b7 \u00b7 , xn) and obtain the corresponding segmentation tag output (h1, h2, h3, \u00b7 \u00b7 \u00b7 , hn)\nat each time step depending not only on the information of the current input word, but of the\nalready processed input. Contrary to other algorithms (perceptron (Afantenos et al., 2010)). BiLSTMs are a special case of LSTM where two\nLSTM nets are employed, one treating the input\nsequence from left to right (forward LSTM) and\nthe other from right to left (backward LSTM).\nLSTMs use a gate-based system, to automatically\nregulate the quantity of \u201cprevious\u201d context to be\nkept and the quantity that has to be renewed. Each\nhidden state of an LSTM concentrates all relevant previous sequential context in one only vector. BiLSTM allows to combine information from\nboth directions. The CRF performs the assigment\nof the segmentation tag taking as input the hidden\nstates provided by each LSTM.\nFor this work we adopted the implementation\nby Lample et al. (2016), to accept not only the\nembeddings but additional information like POS\nor CPOS and syntactic relation concatenated to\nthe case and syntactic subordination information\nat each time step. The equations below describe a\nmemory cell formally in this implementation:\n129\nFigure 2: Graphical view of the segmenter\nit = \u03c3(Wxixt + Whiht\u22121 + Wci\nct\u22121 + bi)\nc\u02dct = tanh(Wxcxt + Whcht\u22121 + Wci\nct\u22121 + bc)\nct = (1 \u2212 it) \f ct\u22121 + it \f c\u02dct\not = \u03c3(Wxo xt + Who ht\u22121 + Wco\nct + bo)\nht = ot \f tanh(ct)\n\u2212 \u03c3 and tanh the sigmoid and hyperbolic tangent respectively, which introduce in the networl non-linearity, increasing network\u2019s predictive power.\n\u2212 t and t \u2212 1 current and previous time steps,\nrespectively.\n\u2212 ct current state of the memory cell considering how much of the previous state cell must\nbe forgotten ((1 \u2212 it) \f ct\u22121) and how much\ninformation must be updated (it \f c\u02dct).\n\u2212 it values that will get updated.\n\u2212 c\u02dct which new candidates could be added to\nthe state.\n\u2212 ot\nthrough the sigmoid (\u03c3), defines which\npart of the information stored in the cell gets\noutputed.\n\u2212 ht\nthe hidden state. Being a Bi-LSTM ht\ngets calculated by concatenation right and\nleft contexts (right to left \u2212\u2192ht and left to right \u2190\u2212ht).\n5 Conclusions and future work\nWe have conducted the DISRPT 2019 shared\ntask, cross-formalism and multilingual segmentation shared task. In this segmentation task, we\n131\nhave provided results for 6 languages: German,\nBasque, Spanish, French, Portuguese and English.\nResults were different if we take into account\nlanguages (and also a slightly different segment\ngranularity): we reported above 90% in Portuguese (92.69%), English (91.94%), German\n(91.37%) and French (90.79%); from 80% to 90%\nreported for Basque and Spanish (rststb). Moreover, we report one result under 80% for Spanish\n(sctb) (76.92%).\nBesides, we performed an error analysis of two\nlanguages (Basque and Spanish), and we underlined the different granularities in each language.\nWe think that there is still room for improvement\nby applying a post-process.\nAuthors are currently striving to achieve the following aims:\n\u2212 To design a pos-process in segmentation in\norder to improve results.\n\u2212 To include this segmenters to the Central\nUnit detectors for Spanish (Bengoetxea and\nIruskieta, 2017) and Portuguese (Bengoetxea\net al., 2018).\n", "SumGPT3": "Introduction:\nThe paper discusses the significance of discourse parsing in Natural Language Processing (NLP) tasks and outlines the two main steps involved: text segmentation and relation identification. It highlights the challenges in discourse segmentation, particularly focusing on the granularity of Elementary Discourse Units (EDUs) and the complexities posed by clauses embedded within clauses.\n\nNovelty:\n\nMultilingual Approach: The paper introduces a multilingual discourse parser, which is a novel contribution in the field. Unlike previous approaches that focused on individual languages, this parser accommodates six languages\u2014English, Basque, Spanish, Portuguese, French, and German. Each language has its own segmenter, enhancing the applicability and versatility of the parser.\n\nFeature Incorporation: In discourse segmentation, the paper integrates both lexicalized (word embeddings, character embeddings) and delexicalized features (UPOS, XPOS, ATTRs). Notably, Basque-specific word embeddings were calculated, adding a language-specific dimension to the feature set.\n\nBiLSTM+CRF Model: The paper employs a BiLSTM+CRF model for segmentation, a departure from traditional algorithms like perceptron. This model effectively utilizes sequential labeling, incorporating information from both directions through Bidirectional LSTMs, and employs Conditional Random Fields (CRF) for tag assignment, enhancing segmentation accuracy.\n\nCross-Formalism and Multilingual Shared Task: The paper presents results from the DISRPT 2019 shared task, focusing on cross-formalism and multilingual segmentation. Results demonstrate high accuracy in several languages, with variations attributed to different segment granularities across languages. The paper also identifies areas for improvement, particularly in post-processing techniques to further enhance segmentation results.\n\nConclusion:\nThe paper presents a comprehensive overview of discourse parsing, emphasizing the novel contributions in multilingualism, feature incorporation, model architecture, and shared task participation. It underscores the importance of continued research to refine segmentation techniques and integrate parsers into broader NLP applications.", "SumGem": "This paper introduces a discourse segmenter that employs a BiLSTM+CRF neural network architecture for segmenting text in multiple languages. The segmenter is based on the form-function approach, which utilizes lexicalized (word embeddings) and delexicalized (UPOS, XPOS and ATTRs) features to classify text spans into Elementary Discourse Units (EDUs).\n\nNovelty\n\nThe segmenter incorporates a BiLSTM+CRF neural network, which is a novel approach in discourse segmentation compared to previous rule-based segmentation methods employed for various languages.\nThe system is multilingual, achieving competitive results on texts in Basque, Spanish, French, Portuguese, English and German.\nConclusion\nThe paper demonstrates that a BiLSTM+CRF based discourse segmenter can achieve competitive results in segmenting text in multiple languages. The proposed system provides an alternative to rule-based segmentation methods, achieving promising results across various languages.", "SumGPT4": "### Introduction\nThe paper delves into the realm of discourse parsing, an essential task in natural language processing (NLP) that underpins many complex applications such as sentiment analysis, question answering, and summarization. Central to discourse parsing are two primary steps: text segmentation and relation identification, focusing on the segmentation of texts into Elementary Discourse Units (EDUs) using linguistic form, function, and meaning.\n\n### Novelty\n1. **Multilingual Segmentation Framework:** The paper introduces a comprehensive discourse segmenter tested across six languages (German, Basque, Spanish, French, Portuguese, and English), highlighting its adaptability and effectiveness in handling diverse linguistic structures. This multilingual approach allows for a nuanced understanding of discourse segmentation across different grammatical and syntactic systems.\n\n2. **Integration of Lexical and Delexicalized Features:** Unlike previous works that primarily focused on either lexicalized features or rule-based approaches, this paper utilizes a blend of both. It employs advanced techniques involving word embeddings and syntactic features (UPOS, XPOS, ATTRs) to improve segmentation accuracy. The use of both external and language-specific word embeddings (like those calculated exclusively for Basque) represents a significant enhancement in handling language variability.\n\n3. **BiLSTM+CRF Model Architecture:** The paper capitalizes on the strengths of BiLSTM and CRF models to handle the sequential nature of text segmentation. This dual approach leverages both forward and backward contextual information, ensuring that each segment's classification benefits from a comprehensive analysis of adjacent textual units.\n\n4. **Error Analysis and Granularity Adjustment:** An in-depth error analysis for Basque and Spanish texts underscores the challenges in segment granularity and informs adjustments in the segmentation process. This analysis is critical for refining the models and achieving higher segmentation precision.\n\n### Conclusion\nThe paper marks significant advancements in discourse segmentation, offering a robust, multilingual framework that integrates sophisticated NLP technologies and methodologies. The presented segmenter not only enhances the granularity and accuracy of discourse parsing but also sets a foundation for future research and development in multilingual NLP applications. The findings suggest promising directions for improving segmentation results through post-processing techniques and further adaptation to diverse linguistic environments.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "detailed and highlights several important aspects of the paper. It includes:\n\nIntroduction: Explains the importance of discourse parsing and the challenges faced, especially in segmentation.\nNovelty: Emphasizes the multilingual approach, feature incorporation (lexicalized and delexicalized features), and the use of the BiLSTM+CRF model. It also mentions the results from the DISRPT 2019 shared task.\nConclusion: Summarizes the overall contributions and the importance of continued research.\nWhile comprehensive, this summary is slightly less organized and doesn\u2019t break down the paper into distinct sections as clearly, making it a bit harder to follow.", "expGem": " the least detailed and organized among the three. It provides a basic overview but lacks depth and clear structure. Specifically:\n\nIntroduction and Novelty: These sections are combined and lack distinct separation, which makes it less clear. It briefly mentions the BiLSTM+CRF model and the multilingual aspect but does not go into much detail about the features or specific contributions.\nConclusion: It summarizes that the BiLSTM+CRF segmenter is competitive but doesn\u2019t provide much insight into the broader implications or future directions. does not convey the full breadth and depth of the paper\u2019s contributions as effectively as the other two summaries.", "expGPT4": "the most comprehensive and well-structured among the three. It clearly outlines the key sections of the paper (Introduction, Novelty, Conclusion) and provides detailed explanations for each. This summary:\n\nIntroduction: Concisely sets the context and importance of discourse parsing in NLP, explaining the primary tasks involved.\nNovelty: Thoroughly details the unique contributions of the paper, including the multilingual framework, integration of lexical and delexicalized features, and the innovative use of the BiLSTM+CRF model. It also mentions specific advancements like the Basque-specific embeddings and error analysis.\nConclusion: Summarizes the significant advancements and potential future directions, highlighting the robustness and implications of the research.\nThis detailed breakdown helps readers grasp the full scope and significance of the paper's contributions."}, "11": {"title": "An Insight Extraction System on BioMedical Literature with Deep Neural Networks", "url": "https://aclanthology.org/D17-1285", "text": "1 Introduction\nBiomedical literature offers a rich set of knowledge sources to discover important facts and find\nassociations among them. For instance, MEDLINE contains over 18 million references to articles published since 1946 and sourced from over\n5500 journals worldwide (Simpson and DemnerFushman, 2012). Two major processing tasks\nperformed on the biomedical text are: (1) identify and classify biomedical entities (NER) into\npredefined categories such as proteins, genes,\nor diseases, and (2) infer pair-wise relationships\namong named entities e.g., protein-protein interaction (Poon et al., 2014), gene-protein, and medical\nproblem-treatment.\nThis paper presents a system that processes\nbiomedical text to extract two specific types of relationships among biomedical entities: (a) causeeffect and (b) correlation.\nThis system is motivated by the need to better automate biomedical knowledge extraction and\nidentify important information from them, as new\nscientific findings appear across a large collection\nof publications. For instance, given user sleep patterns, existing biomedical research can be better\nutilized to provide insights: inform about potential effect (e.g., \u201cdiabetes\u201d, \u201cobesity\u201d) due to the\ncause (e.g., \u201csleep disorder\u201d) and suggest appropriate treatment.\nSince biomedical articles usually have title and\nabstract summarizing the contents of the full-text\narticle, we focus on extracting the two relationship\ntypes from them. Unfortunately, mining this summary data still poses several key challenges. Similar to full-text, this data comprises unstructured\ntext with domain-specific vocabulary, issues of\nsynonymy (e.g., \u201cheart attack\u201d vs. \u201cmyocardial\ninfarction\u201d), acronyms, abbreviations and rapidly\nevolving terminology due to new scientific discoveries. While the titles are short and informative,\nthey do not contain the key information that would\nbe contained in the abstract.\nMany of these challenges are also applicable\nfor biomedical relation extraction. Further, identifying particular relation types is challenging because relations are expressed as discontinuous\nspans of text , and the relation types are typically\napplication-specific. Finally, there is often little\nconsensus on how to best annotate relation types\nresulting in lack of high quality annotated corpora\nfor training.\nIn this study, we develop neural networks\nwith novel similarity modeling for better causality/correlation relation extraction, as we map the\nextraction task into a representational similarity\nmeasurement task in the vector space. Our approach innovates in that it explicitly measures both\nrelational and contextual similarity among representations of named entities, entity relations and\ncontexts. Our system also provides a novel combination of recognizing named entities, predicting\n2691\nrelationships (insights) between extracted entities,\nand ranking the output. We conduct human evaluations of the system to show it is able to extract insights with high human acceptance accuracy, and on a SemEval task evaluation its causality/correlation relation extraction compares favorably against previous state-of-the-art work.\n2 Related Work\nMost previous work in BioNLP focused on extraction of biomedical concepts (Craven, 1999;\nFinkel et al., 2005; Poon and Vanderwende, 2010;\nSimpson and Demner-Fushman, 2012; Liu, 2016),\nsuch as drug or protein names. We also conduct relation extraction on general named entities,\nsuch as \u201csmoking\u201d or \u201csleep quality\u201d. Kabiljo\net al. (2009) compared pattern-matching techniques against a baseline regular expression approach for gene/protein entity extraction. But existing tools for relation extraction are not as comprehensive as entity recognition tools.\nMedical dictionaries and resources are heavily\nutilized by previous work. For instance, Chen\net al. (2008) extracted disease-drug relation pairs\nwith MedLEE (Friedman et al., 2004) system for\nclinical information extraction of EHR records.\nLiu et al. (2015) developed a text-mining system\nto search for associations among human diseases,\ngenes, drugs, metabolites and toxins against large\ncollections of text-rich biological databases. Previous research efforts also lead to semantic representation program SemRep (Rindflesh and Fiszman, 2003), which exploits biomedical domain\nknowledge and linguistic analysis of biomedical\ntext. Other unconventional resource such as web\nquery logs are also utilized (Paparrizos et al.,\n2016) to provide early warnings about the presence of devastating diseases.\nFeature engineering was the dominant approach\nin most biomedical relation extraction work with\nmachine learning techniques (Dogan et al., 2011;\nYala et al., 2016); different sparse features were\nexplored. For example, word n-gram features,\nAlgorithm 1 System Overview\n1: Input: Biomedical article title and abstract\n2: Preprocess the input texts\n3: for each sentence of the input do\n4: Identify all possible named entities\n5: for each named entity pair (A, ~ B~ ) do\n6: if causality/correlation holds then\n7: Extract and Score (A, ~ B~ )\n8: end if\n9: end for\n10: end for\n11: Rank all extracted (A, ~ B~ ) pairs\n12: return top ranked entity pairs\nknowledge-based features from medical dictionaries and word position features. Our work instead\npropose neural network models that do not require\nsparse features as in most previous work.\nRecent shift from feature engineering to model\nengineering with neural networks has significantly improved accuracy on many NLP tasks.\nJagannatha and Yu (2016) adopted an LSTM\nmodel for medical entity detection given patient\nEHR records. There are recent work with the\nuse of deep reinforcement learning on healthcare study (Li, 2017). Our approach is inspired\nby recent embedding learning work to jointly\nrepresent texts and knowledge base (Toutanova\net al., 2015, 2016), previous work on embedding\ntransfer learning (Bordes et al., 2013) and noisecontrastive estimation (Rao et al., 2016). Lastly\nour work models insight extraction as a similarity measurement problem, and is inspired by similarity measurement work (He et al., 2016; He and\nLin, 2016) on pairwise word interaction modeling\nwith deep neural networks.\n3 System Overview\nWe provide a recipe to build a system for biomedical insight extraction and use it as a guide for the\nremainder of this paper (Algorithm 1).\nTo make our discussion concrete, we will use\na sample biomedical article in Example 1. Given\nthe text, at line 4 of Algorithm 1 we firstly look for\nall named entities using a shallow parser and public medical dictionaries (see details in Section 4).\nMany named entities could be found, for example,\n\u201cclinical study\u201d, \u201csleep disturbances in middleaged men\u201d and \u201cdiabetes\u201d. Next given any pair\nof previously extracted entities within a sentence,\n2692\nRESEARCH METHODS: A group of 6,599\ninitially healthy, nondiabetic middle-aged men\ntook part in a prospective, population-based\nstudy. The incidence of diabetes during a mean\nfollow-up of 14.8 years was examined in relation\nto self-reported difficulties in falling asleep.\nRESULTS: A total of 615 subjects reported either difficulties in falling asleep or use of hypnotics (seen as makers of sleep disturbances).\nAmong those, 281 of the men developed diabetes\nduring the follow-up period. The clinical study\nsuggests sleep disturbances in middle-aged men\nare likely associated with diabetes.\nExample 1: Sample Text\nat line 6 our neural network-based relation extractor checks if a valid causality/correlation relationship exists (Section 5). For example, our models can identify that the entity \u201csleep disturbances\nin middle-aged men\u201d has a correlation relationship with \u201cdiabetes\u201d but not with \u201cclinical study\u201d.\nLater each valid entity pair is scored via the ranking component at line 7 (Section 6). In the final\nstep, the system returns top ranked insight(s) to\nusers: \u201csleep disturbances in middle-aged men \u2192\ndiabetes\u201d given this example.\nFigure 1 presents the system which consists\nof three major neural network-based components: (1) a named entity extractor, (2) a causality/correlation relation extractor, and (3) an insight\nranker. Our system reads in biomedical texts, then\nprovides insights in the end. We primarily innovate in the relation extraction component. Next,\nwe describe each of these components in detail.\n4 Named Entity Extraction\nNamed entity extraction in biomedical domain is\nchallenging due to the domain-specific and rapidly\nevolving terminology. For example, \u201cDiabetes\nmellitus type 1\u201d, \u201cType 1 diabetes\u201d, \u201cIDDM\u201d, or\n\u201cjuvenile diabetes\u201d all express the same concept.\nGiven frequent evolution of entity naming for new\ndrugs, diseases or abbreviations, this task becomes\nmore complicated.\nMost existing off-the-shelf biomedical entity\nrecognizers narrowly focus on specific biomedical\nterms. Instead we aim to improve the system recall\nby extracting both specific biomedical concepts\nsuch as \u201cgene tmem230\u201d or \u201cprostate cancer\u201d as\nwell as general noun phrases such as \u201csleep qualBiomedical\nKnowledgebase\nShallow Parsing\nContextual\nSimilarity\nInsight Ranker\nModeling\nRelational\nSimilarity\nModeling\nFigure 1: Three major components of the system.\nity\u201d, \u201cdaily exercises\u201d, or \u201cmen with diabetes\u201d.\nThus the scope of the system is broader.\nWe design an entity extractor by using both\nan in-domain medical knowledge base for keyword matching, and a domain-independent neural\nnetwork-based shallow parser for entity boundary\ndetection. We present the procedure below:\n1. We firstly use a large public dictionary,\nMetathesaurus of the Unified Medical Language System (UMLS) (Bodenreider, 2004) to\nobtain in-domain biomedical terms. UMLS\nMetathesaurus is a set of dictionaries providing large collections of biomedical vocabularies. We extract over 3.3 million of biomedical\nterms from UMLS, then utilize the Aho Corasick pattern matching algorithm to create a dictionary lookup tool. Our tool can efficiently locate all UMLS terms given input texts, since it\nhas a linear complexity due to its trie tree data\nstructure.\n2. We also use a neural network-based shallow\nparser (Collobert et al., 2011) to identify boundaries of general noun phrases, which are not\nlimited to biomedical terms. Usage of shallow\nparser is to improve system recall on named entity recognition.\n3. Our named entity extraction component aims to\nlocate all entities of input texts. The result list\nis an output concatenation of both step 2 and 3,\nand is later provided to the causality/correlation\nrelation extraction component for further processing. If entity overlaps exist, only phrases\nwith longest matching sequence are extracted.\nOur insight extraction system adopts a coarseto-fine design approach. First, we focus on improving recall for the entity extraction task. Then\nwe show how the causality/correlation relation extraction component (Sec. 5) processes extracted\nnamed entities to achieve high precision.\n5 Relation Extraction as Similarity\nMeasurement\nWe first provide our model design intuition: if\na causality/correlation relationship holds between\n2693\ntwo named entities, then representations of the\ntwo entities should be semantically similar and\nclose to the representation of the relation in a lowdimensional vector space. Therefore we map the\ncausality/correlation relation extraction into a similarity measurement task in the vector space.\nOur novel approach learns representations of\nnamed entities (A, ~ B~ ), context words and the relation vector R~ , then explicitly measures two aspects\nof the similarity: 1) relational similarity between\nentities and relation (Sec. 5.2); plus, 2) contextual\nsimilarity between entities and sentence context\n(Sec. 5.3).\nThe intent of our approach is to enforce such\nstructure of the vector space: as the similarity\namong entities, relation and contexts gets stronger,\na fit of all should be observed for better causality/correlation relation extraction. We develop two\nneural network models with such property; both\nare utilized in the relation extraction component\nof the system.\nWe define input sentence representation S \u2208\nR\n`\u00d7d\nto be a sequence of ` words, each with a\nd-dimensional word embedding vector. xt \u2208 R\nd\ndenotes the embedding vector of the t-th word\n(t \u2208 [1, l]) in S. Model details are described in\nthe following sections.\n5.1 Context Modeling\nDifferent words occurring in similar contexts\nshould have a higher chance to contribute to similarity measurement and relation extraction. We\nuse bidirectional LSTMs (BiLSTM) for context\nmodeling as a basis for all following models.\nLSTM (Hochreiter and Schmidhuber, 1997)\nis a special variant of Recurrent Neural Networks (Williams and Zipser, 1989). At time step\nt, given an input word xt and previous LSTM hidden state ht\u22121, LSTM(xt\n, ht\u22121) outputs current\nhidden state ht \u2208 R\ndim. BiLSTM consists of\ntwo LSTMs that run in parallel in opposite directions. The BiLSTM hidden state h\nbi\nt \u2208 R\n2dim is a\nconcatenation of forward LSTM\u2019s h\nf or\nt\nand backward LSTM\u2019s h\nback\nt\n, representing contexts of input\nword xt\nin the sentence. We define concat operation and output sentence context representation\nHS \u2208 R\n`\u00d72dim below:\nht = LSTM(xt, ht\u22121) (1)\nh\nbi\nt = concat(h\nfor\nt\n, hback\nt ) (2)\nH\nS\n[t] = h\nbi\nt\n(3)\nFunction 1 SimiScore(A, ~ B, ~ R~ )\n1: conC = concat(A, ~ B~ )\n2: entityC = WC \u00b7 conC\n3: relationT = W D \u00b7 R~\n4: dist = Wdi\n\u00b7 tanh(entityC + relationT)\n5: return dist\nContext modeling with BiLSTM allows our following model components to be built over contexts rather than over individual words. Given\nnamed entity positions of the sentence, we get A~\nand B~ from context HS\n.\n5.2 Relational Similarity Modeling\nRelational similarity modeling focuses on interactions between named entities and relations in\nthe vector space. When the named entity A~ goes\nthrough a transformation process induced by the\nrelation R~ , our intent of relational similarity modeling is to force the transformed entity to be translated to the other named entity B~ in the same vector space so that the relation R~ holds between the\ntwo named entities.\nWe show the following objective function of our\nrelational similarity modeling:\nA~ + B~ \u2212 R~ ' 0 (4)\nTo model the transformation process in Equation 4, we need to know how to measure\nthe similarity of the triplet (A, ~ B, ~ R~ ). Therefore we develop a similarity measurement function SimiScore(A, ~ B, ~ R~ ) with learnable weights\n(W\u2217\n), the similarity function takes an input named\nentity pair of (A, ~ B~ ) and a relation R~ , returns a\nsimilarity score dist \u2208 R1\nrepresenting how semantically close (A, ~ B, ~ R~ ) are, as in Function 1.\nWe utilize a ranking approach during training\nto incorporate the constraint of Equation 4 into the\nrelational similarity model. Our goal is to learn a\nfunction SimiScore(\u00b7) so that the positive triplet\n(A, ~ B, ~ R~+) is assigned a larger score than that of\nthe negative triplet (A, ~ B, ~ R~\u2212):\nSimiScore(A, ~ B, ~ R~+) > SimiScore(A, ~ B, ~ R~\u2212) (5)\nwhere R+ denotes the positive causality/correlation relation, R\u2212 denotes a noncausality/non-correlation relation. The ranking\napproach maximizes the similarity score between\nthe entity pair (A, ~ B~ ) and a positive relation R~ +\nwhile minimizing the score with the negative R~ \u2212,\n2694\nC~ : Lung cancer is most likely caused by smoking\nNamed Entities (A, ~ B~ )\nContextual Similarity\nCausality Relation R~+ Non-Caus. Relation R~\u2212\nRelational Similarity\n>\nFigure 2: Our causality/correlation relation extraction component models both relational similarity\n(blue) and contextual similarity (red). Thicker arrows indicate stronger similarity between named\nentities (A, ~ B~ ) and relation R~ /sentence context.\nthus ensuring that the positive connection is larger\nthan the negative one as in Figure 2.\nOur relational similarity model and the ranking\ntraining approach facilitate the transformation process of (A~, B~ ) and R~ in the vector space, which\nin the end leads to better constraint satisfaction of\nobjective Equation 4.\nThe relational similarity model is placed on top\nof BiLSTM (Sec 5.1) as part of the system. We\ninitialize named entities A/~ B~ as h\nbi\nA\n/hbi\nB\nfrom the\nBiLSTM model, then initialize relation representations R~+/R~\u2212 as random vectors. During training both R~+/R~\u2212 are updated.\n5.3 Contextual Similarity Modeling\nSince not all words of a given title/abstract are created equal, important context words around named\nentities that can better contribute to the causality/correlation relation extraction deserve more\nmodel focus. We develop a contextual similarity\nmodel that can increase model weights onto important context words to better utilizing contextual\ninformation.\nFor example, given a sentence, lung cancer is\nmost likely caused by smoking, the context words\ncaused by are important clues to suggest there\nexists a causality/correlation relationship between\nthe two named entities. Clue words that require\nmodel attentions usually include, e.g. lead to, is\nassociated with, because of, while others are not\nobvious, such as promote, reflect, reduce, make.\nOur system does not require a manually prepared list of clue words, but an attention mechanism (Bahdanau et al., 2014) is utilized to better\nidentify them by conducting similarity measurement between context word representation h\nbi\nt\n(not\nincluding entity words) and extracted named entities (A, ~ B~ ) (from Sec. 4). Resulting similarity\nscores of words are accumulated in atten \u2208 R\n`\n.\nmix = Wa\n\u00b7 concat(A, ~ B~ ) (6)\nE[t] = dotProd(mix, hbi\nt ), \u2200t \u2208 [1, l] (7)\natten = softmax (E) (8)\nwhere we concatenate both entity representations\n(A, ~ B~ ), apply linear transformation with weights\nWa\nto obtain a representation mix of both entities.\nWe then use dot product dotP rod to measure the\nsimilarity between mix and each context word, finally normalize the attention weights atten[:] with\nsoftmax . The weights of atten indicate the importance of each context word with respect to both\nnamed entities.\nThe attention weights should better guide the\nfocus of the model onto important context words\nof the sentence. That is, context words that are\ncloser to entity representation mix should have\nbetter chances to be clue words. We define\nthe attention re-weighted sentence representation\nattenSen \u2208 R\n2dim:\nattenSen = atten \f HS\n(9)\nwhere \f represents element-wise multiplication.\nFigure 2 illustrates an example where representation mix of named entities attends to context words one at a time. Important context clue\nwords \u201ccaused by\u201d should receive higher attention\nweights than irrelevant neighbor words.\nThe re-weighted sentence representation\nattenSen is used together with entity representations (A, ~ B~ ) for final prediction.\nIn summary, both models described in this\nsection focus on different aspects of similarity\nmeasurement in relation extraction: the contextual similarity model utilizes context information\naround named entities, while the relational similarity model focuses on enforcing a transformation\nconstraint between entities and relation in the vector space. We adopt both models for better relation\nextraction, in the end only pairs of named entities\nthat are recognized positively by either one of the\nmodels are passed to the next stage of the system.\n2695\nFigure 3: Human annotation interface on UHRS platform. Annotators are required to identify and verify\nextracted entities and correlation/causality relations from the output of our system for evaluation.\n6 Ranking of Extracted Insights\nThe last major component of our system is to rank\nextracted relations (A, ~ B, ~ R~ ) from the output of\nthe relation extraction component, as there could\nbe many extracted relations but not all of them are\nimportant enough as insights of the article. Importance scores of extracted relations are obtained by\nfollowing a set of rules below:\n1. We utilize the output classification probability\n(\u2208 [0, 1]) of the relational similarity model as\nthe base ranking score.\n2. We use a multi-perspective convolutional neural\nnetwork model (MPCNN) (He et al., 2015) to\nmeasure the similarity (\u2208 [0, 1]) between the title of the article and extracted relation, since the\nMPCNN model has competitive performance\non multiple benchmarks for textual similarity\nmeasurement. We compare title text with \u201cA~\nleads to B~ \u201d of an extracted relation, if the similarity score is over a threshold of 0.75, we increase the extracted relation\u2019s ranking score by\n15%. If the extracted relation is from the title\ntext, we also boost its ranking score by 15% because of its location importance.\nOnce all extracted relations are scored, our system\nonly returns the top ranked insights to users.\n10 Conclusion\nWe build an end-to-end system for insight extraction on biomedical literature. We develop novel\nsimilarity measurement modeling with deep neural networks to extract causation/correlation relations. Our evaluation shows the system is able\nto extract insights with competitive human acceptance accuracy and its relation extraction component compares favorably against previous work.\n", "SumGPT3": "Introduction:\nThe paper addresses the challenge of extracting causality and correlation relationships from biomedical literature, leveraging the vast amount of available data. Traditional methods face obstacles due to the unstructured nature of text and evolving terminology. The authors propose a system designed to automate the extraction process, aiming to uncover valuable insights from biomedical articles' titles and abstracts.\n\nNovelty:\n\nNeural Network-Based Models: Unlike traditional approaches reliant on feature engineering, the system utilizes neural networks, eliminating the need for sparse features and enhancing accuracy. This departure from feature engineering aligns with recent advancements in natural language processing (NLP).\n\nRelational Similarity Modeling: The paper introduces a novel approach that views relation extraction as a similarity measurement task in a vector space. By explicitly measuring relational and contextual similarity among named entities, relations, and contexts, the system achieves more precise extraction of causality and correlation relationships.\n\nContextual Similarity Modeling: An attention mechanism is employed to identify important context words surrounding named entities, enhancing the model's ability to recognize crucial clues for relationship extraction. This attention-driven approach dynamically adjusts the model's focus based on context relevance.\n\nRanking of Extracted Insights: The system incorporates a multi-perspective convolutional neural network model to rank extracted relations based on their relevance and importance. This ranking mechanism ensures that only the most pertinent insights are presented to users, enhancing the efficiency of information retrieval.\n\nConclusion:\nThe paper presents an innovative system for extracting causality and correlation relationships from biomedical literature. By leveraging neural network-based models and advanced similarity measurement techniques, the system offers a more accurate and efficient approach to automated knowledge extraction. The incorporation of attention mechanisms and ranking strategies further enhances the system's ability to uncover valuable insights from biomedical texts.", "SumGem": "Introduction\n\nThis paper proposes a system for biomedical insight extraction from titles and abstracts of scientific articles. The system focuses on identifying causality and correlation relationships between named entities.\n\nNovelty\n\nThe system utilizes a neural network-based approach for relation extraction, as opposed to traditional feature engineering methods.\nIt leverages a combination of relational and contextual similarity modeling to capture the relationships between named entities and the context within a sentence. This is achieved through two separate models:\nRelational similarity model: Measures the similarity between entity representations and the relation representation in the vector space.\nContextual similarity model: Analyzes the importance of context words surrounding the named entities using an attention mechanism.\nThe system employs a ranking mechanism to prioritize the extracted insights based on:\nThe confidence score from the relational similarity model.\nThe similarity between the extracted relation and the title of the article using a multi-perspective convolutional neural network (MPCNN) model.\nConclusion\n\nThe authors demonstrate that their system achieves competitive accuracy in extracting insights from biomedical literature and outperforms previous relation extraction methods. This approach offers a promising direction for automating knowledge discovery from scientific publications.", "SumGPT4": "### Introduction\nThe paper introduces a novel system designed to extract cause-effect and correlation relationships from biomedical texts, particularly from titles and abstracts of scholarly articles. This approach seeks to address the challenge of efficiently utilizing biomedical literature to uncover significant insights, especially as new research continually expands the volume of available publications.\n\n### Novelty\nThe paper outlines several innovative aspects of the proposed system:\n1. **Neural Network-Based Models**: Unlike traditional methods that primarily rely on feature engineering with sparse features, this system uses neural networks that eliminate the need for manual feature selection. This shift towards model engineering enhances accuracy and efficiency in processing natural language.\n   \n2. **Representation and Similarity Measurement**: The core of the system is built on a novel methodology that transforms the task of relation extraction into a similarity measurement challenge within a vector space. This includes:\n   - **Relational Similarity**: The system models the relationship between entities and context, evaluating how closely related the entities are in terms of the predefined relation vectors.\n   - **Contextual Similarity**: Contextual cues around named entities are weighted differently based on their importance in suggesting a causal or correlational relationship. This is managed through an attention mechanism, focusing the model on significant context words.\n\n3. **Integration of Deep Learning Techniques**: The integration of techniques such as BiLSTMs for context modeling and deep reinforcement learning in other parts of the study points to a comprehensive adoption of advanced AI methods in biomedical text analysis.\n\n### Conclusion\nThe paper presents a sophisticated deep learning framework that significantly improves the extraction of biomedical insights by modeling both relational and contextual similarities. The proposed system not only enhances the accuracy of identifying relevant relationships in biomedical texts but also does so in a way that scales effectively with the growing body of literature. The evaluation results show that the system performs favorably against existing state-of-the-art methods, validating the effectiveness of the novel approaches used.", "rankGPT3": 1, "rankGem": 3, "rankGPT4": 2, "expGPT3": "Clarity and Structure: Summary 1 is well-structured, with distinct sections for Introduction, Novelty, and Conclusion. This clear separation makes it easy to follow the logical flow of the paper.\nDetailed Explanation: It provides a comprehensive explanation of the methods, including neural network-based models, relational and contextual similarity modeling, and the ranking mechanism. Each method is briefly but clearly described.\nTerminology and Innovation: It emphasizes the departure from traditional feature engineering, the use of an attention mechanism, and the implementation of a multi-perspective convolutional neural network model. These points highlight the paper\u2019s innovative aspects effectively.", "expGem": "Conciseness: This summary is concise and covers the main points of the paper, but it lacks the depth and detail found in the other two summaries.\nLess Emphasis on Innovation: It mentions the use of neural networks and the combination of relational and contextual similarity modeling, but it doesn't elaborate much on the specific techniques or their novelty.\nClear but Less Detailed: The structure is clear, and it does mention the ranking mechanism, but it doesn't go into as much detail about how the models work or why they are innovative compared to traditional methods.", "expGPT4": "Innovation and Techniques: This summary gives a good overview of the innovative techniques used, such as BiLSTMs and deep reinforcement learning. It also explains the core methodology of transforming relation extraction into a similarity measurement task.\nEmphasis on Novelty: It focuses well on the novelty of the approach, particularly the use of neural networks and the dual focus on relational and contextual similarities.\nLess Detail on Ranking: While it mentions the advanced AI methods and the attention mechanism, it doesn't explain the ranking mechanism as clearly as Summary 1, which is a slight drawback."}, "12": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "url": "https://aclanthology.org/D17-1206", "text": "1 Introduction\nThe potential for leveraging multiple levels of\nrepresentation has been demonstrated in various\nways in the field of Natural Language Processing\n(NLP). For example, Part-Of-Speech (POS) tags\nare used for syntactic parsers. The parsers are used\nto improve higher-level tasks, such as natural language inference (Chen et al., 2016) and machine\ntranslation (Eriguchi et al., 2016). These systems\nare often pipelines and not trained end-to-end.\nDeep NLP models have yet shown benefits from\npredicting many increasingly complex tasks each\nat a successively deeper layer. Existing models\noften ignore linguistic hierarchies by predicting\n\u2217 Work was done while the first author was an intern at\nSalesforce Research.\n\u2020Corresponding author.\nCHUNK\nPOS\nDEP\nRelatedness\nencoder\nRelatedness\nEntailment\nencoder\nEntailment\nword representation\nSentence1\nCHUNK\nPOS\nDEP\nRelatedness\nencoder\nEntailment\nencoder\nword representation\nSentence2\nsemantic\nlevel\nsyntactic\nlevel word level\nFigure 1: Overview of the joint many-task model\npredicting different linguistic outputs at successively deeper layers.\ndifferent tasks either entirely separately or at the\nsame depth (Collobert et al., 2011).\nWe introduce a Joint Many-Task (JMT) model,\noutlined in Figure 1, which predicts increasingly\ncomplex NLP tasks at successively deeper layers. Unlike traditional pipeline systems, our single JMT model can be trained end-to-end for POS\ntagging, chunking, dependency parsing, semantic\nrelatedness, and textual entailment, by considering linguistic hierarchies. We propose an adaptive\ntraining and regularization strategy to grow this\nmodel in its depth. With the help of this strategy we avoid catastrophic interference between\nthe tasks. Our model is motivated by S\u00f8gaard and\nGoldberg (2016) who showed that predicting two\ndifferent tasks is more accurate when performed in\ndifferent layers than in the same layer (Collobert\net al., 2011). Experimental results show that our\nsingle model achieves competitive results for all\nof the five different tasks, demonstrating that us1923\ning linguistic hierarchies is more important than\nhandling different tasks in the same layer.\n2 The Joint Many-Task Model\nThis section describes the inference procedure of\nour model, beginning at the lowest level and working our way to higher layers and more complex\ntasks; our model handles the five different tasks in\nthe order of POS tagging, chunking, dependency\nparsing, semantic relatedness, and textual entailment, by considering linguistic hierarchies. The\nPOS tags are used for chunking, and the chunking\ntags are used for dependency parsing (Attardi and\nDellOrletta, 2008). Tai et al. (2015) have shown\nthat dependencies improve the relatedness task.\nThe relatedness and entailment tasks are closely\nrelated to each other. If the semantic relatedness\nbetween two sentences is very low, they are unlikely to entail each other. Based on this observation, we make use of the information from the\nrelatedness task for improving the entailment task.\n2.1 Word Representations\nFor each word wt\nin the input sentence s of length\nL, we use two types of embeddings.\nWord embeddings: We use Skip-gram (Mikolov\net al., 2013) to train word embeddings.\nCharacter embeddings: Character n-gram embeddings are trained by the same Skip-gram objective. We construct the character n-gram vocabulary in the training data and assign an embedding for each entry. The final character embedding is the average of the unique character n-gram\nembeddings of wt\n. For example, the character ngrams (n = 1, 2, 3) of the word \u201cCat\u201d are {C, a,\nt, #B#C, Ca, at, t#E#, #B#Ca, Cat, at#E#}, where\n\u201c#B#\u201d and \u201c#E#\u201d represent the beginning and the\nend of each word, respectively. Using the character embeddings efficiently provides morphological features. Each word is subsequently represented as xt\n, the concatenation of its corresponding word and character embeddings shared across\nthe tasks.1\n2.2 Word-Level Task: POS Tagging\nThe first layer of the model is a bi-directional\nLSTM (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) whose hidden states\n1Bojanowski et al. (2017) previously proposed to train the\ncharacter n-gram embeddings by the Skip-gram objective.\nare used to predict POS tags. We use the following Long Short-Term Memory (LSTM) units for\nthe forward direction:\nit = \u03c3 (Wigt + bi), ft = \u03c3 (Wf gt + bf ),\nut = tanh (Wugt + bu),\nct = it \f ut + ft \f ct\u22121, (1)\not = \u03c3 (Wogt + bo), ht = ot \f tanh (ct),\nwhere we define the input gt as gt = [\u2212\u2192h t\u22121; xt\n],\ni.e. the concatenation of the previous hidden state\nand the word representation of wt\n. The backward\npass is expanded in the same way, but a different\nset of weights are used.\nFor predicting the POS tag of wt\n, we use the\nconcatenation of the forward and backward states\nin a one-layer bi-LSTM layer corresponding to the\nt-th word: ht = [\u2212\u2192h t\n;\n\u2190\u2212\nh t\n]. Then each ht (1 \u2264 t \u2264\nL) is fed into a standard softmax classifier with a\nsingle ReLU layer which outputs the probability\nvector y\n(1) for each of the POS tags.\n2.3 Word-Level Task: Chunking\nChunking is also a word-level classification task\nwhich assigns a chunking tag (B-NP, I-VP, etc.)\nfor each word. The tag specifies the region of major phrases (e.g., noun phrases) in the sentence.\nChunking is performed in the second bi-LSTM\nlayer on top of the POS layer. When stacking\nthe bi-LSTM layers, we use Eq. (1) with input\ng\n(2)\nt = [h\n(2)\nt\u22121\n; h\n(1)\nt\n; xt\n; y\n(pos)\nt\n], where h\n(1)\nt\nis the\nhidden state of the first (POS) layer. We define\nthe weighted label embedding y\n(pos)\nt\nas follows:\ny\n(pos)\nt =\nX\nC\nj=1\np(y\n(1)\nt = j|h\n(1)\nt\n)`(j), (2)\nwhere C is the number of the POS tags, p(y\n(1)\nt =\nj|h\n(1)\nt\n) is the probability value that the j-th POS\ntag is assigned to wt\n, and `(j) is the corresponding label embedding. The probability values are\npredicted by the POS layer, and thus no gold POS\ntags are needed. This output embedding is similar to the K-best POS tag feature which has been\nshown to be effective in syntactic tasks (Andor\net al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bidirectional hidden states h\n(2)\nt = [\u2212\u2192h\n(2)\nt\n;\n\u2190\u2212\nh\n(2)\nt\n] in\nthe chunking layer. We also use a single ReLU\nhidden layer before the softmax classifier.\n1924\n2.4 Syntactic Task: Dependency Parsing\nDependency parsing identifies syntactic relations\n(such as an adjective modifying a noun) between\nword pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs\nof words. The input vector for the LSTM includes hidden states, word representations, and\nthe label embeddings for the two previous tasks:\ng\n(3)\nt = [h\n(3)\nt\u22121\n; h\n(2)\nt\n; xt\n; (y\n(pos)\nt +y\n(chk)\nt\n)], where we\ncomputed the chunking vector in a similar fashion\nas the POS vector in Eq. (2).\nWe predict the parent node (head) for each\nword. Then a dependency label is predicted for\neach child-parent pair. This approach is related\nto Dozat and Manning (2017) and Zhang et al.\n(2017), where the main difference is that our\nmodel works on a multi-task framework. To predict the parent node of wt\n, we define a matching\nfunction between wt and the candidates of the parent node as m (t, j) = h\n(3)\nt\n\u00b7 (Wdh\n(3)\nj\n), where Wd\nis a parameter matrix. For the root, we define\nh\n(3)\nL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is\nthe parent of wt\n, the scores are normalized:\np(j|h\n(3)\nt\n) = exp (m (t, j))\nPL+1\nk=1,k6=t\nexp (m (t, k))\n. (3)\nThe dependency labels are predicted using\n[h\n(3)\nt\n; h\n(3)\nj\n] as input to a softmax classifier with\na single ReLU layer. We greedily select the parent node and the dependency label for each word.\nWhen the parsing result is not a well-formed tree,\nwe apply the first-order Eisner\u2019s algorithm (Eisner,\n1996) to obtain a well-formed tree from it.\n2.5 Semantic Task: Semantic relatedness\nThe next two tasks model the semantic relationships between two input sentences. The first task\nmeasures the semantic relatedness between two\nsentences. The output is a real-valued relatedness\nscore for the input sentence pair. The second task\nis textual entailment, which requires one to determine whether a premise sentence entails a hypothesis sentence. There are typically three classes:\nentailment, contradiction, and neutral. We use the\nfourth and fifth bi-LSTM layer for the relatedness\nand entailment task, respectively.\nNow it is required to obtain the sentence-level\nrepresentation rather than the word-level representation h\n(4)\nt\nused in the first three tasks. We compute the sentence-level representation h\n(4)\ns as the\nelement-wise maximum values across all of the\nword-level representations in the fourth layer:\nh\n(4)\ns = max \u0010\nh\n(4)\n1\n, h(4)\n2\n, . . . , h(4)\nL\n\u0011\n. (4)\nThis max-pooling technique has proven effective\nin text classification tasks (Lai et al., 2015).\nTo model the semantic relatedness between s\nand s\n0\n, we follow Tai et al. (2015). The feature\nvector for representing the semantic relatedness is\ncomputed as follows:\nd1(s, s0\n) = h\f\n\f\n\f\nh\n(4)\ns \u2212 h\n(4)\ns\n0\n\f\n\f\n\f ; h\n(4)\ns \f h\n(4)\ns\n0\ni\n, (5)\nwhere\n\f\n\f\n\f\nh\n(4)\ns \u2212 h\n(4)\ns\n0\n\f\n\f\n\f\nis the absolute values of the\nelement-wise subtraction, and h\n(4)\ns \f h\n(4)\ns\n0 is the\nelement-wise multiplication. Then d1(s, s0\n) is fed\ninto a softmax classifier with a single Maxout\nhidden layer (Goodfellow et al., 2013) to output\na relatedness score (from 1 to 5 in our case).\n2.6 Semantic Task: Textual entailment\nFor entailment classification, we also use the maxpooling technique as in the semantic relatedness\ntask. To classify the premise-hypothesis pair\n(s, s0\n) into one of the three classes, we compute the feature vector d2(s, s0\n) as in Eq. (5) except that we do not use the absolute values of\nthe element-wise subtraction, because we need\nto identify which is the premise (or hypothesis).\nThen d2(s, s0\n) is fed into a softmax classifier.\nTo use the output from the relatedness layer directly, we use the label embeddings for the relatedness task. More concretely, we compute the class\nlabel embeddings for the semantic relatedness task\nsimilar to Eq. (2). The final feature vectors that are\nconcatenated and fed into the entailment classifier\nare the weighted relatedness label embedding and\nthe feature vector d2(s, s0\n). We use three Maxout\nhidden layers before the classifier.\n3 Training the JMT Model\nThe model is trained jointly over all datasets. During each epoch, the optimization iterates over each\nfull training dataset in the same order as the corresponding tasks described in the modeling section.\n3.1 Pre-Training Word Representations\nWe pre-train word embeddings using the Skipgram model with negative sampling (Mikolov\n1925\net al., 2013). We also pre-train the character ngram embeddings using Skip-gram.2 The only difference is that each input word embedding is replaced with its corresponding average character ngram embedding described in Section 2.1. These\nembeddings are fine-tuned during the model training. We denote the embedding parameters as \u03b8e.\n3.2 Training the POS Layer\nLet \u03b8POS = (WPOS, bPOS, \u03b8e) denote the set of\nmodel parameters associated with the POS layer,\nwhere WPOS is the set of the weight matrices in\nthe first bi-LSTM and the classifier, and bPOS is\nthe set of the bias vectors. The objective function\nto optimize \u03b8POS is defined as follows:\nJ1(\u03b8POS) = \u2212\nX\ns\nX\nt\nlog p(y\n(1)\nt = \u03b1|h\n(1)\nt\n)\n+ \u03bbkWPOSk\n2 + \u03b4k\u03b8e \u2212 \u03b8\n0\nek\n2\n,\n(6)\nwhere p(y\n(1)\nt = \u03b1wt\n|h\n(1)\nt\n) is the probability value\nthat the correct label \u03b1 is assigned to wt\nin the sentence s, \u03bbkWPOSk\n2\nis the L2-norm regularization\nterm, and \u03bb is a hyperparameter.\nWe call the second regularization term \u03b4k\u03b8e \u2212\n\u03b8\n0\nek\n2\na successive regularization term. The successive regularization is based on the idea that we\ndo not want the model to forget the information\nlearned for the other tasks. In the case of POS\ntagging, the regularization is applied to \u03b8e, and \u03b8\n0\ne\nis the embedding parameter after training the final\ntask in the top-most layer at the previous training\nepoch. \u03b4 is a hyperparameter.\n3.3 Training the Chunking Layer\nThe objective function is defined as follows:\nJ2(\u03b8chk) = \u2212\nX\ns\nX\nt\nlog p(y\n(2)\nt = \u03b1|h\n(2)\nt\n)\n+ \u03bbkWchkk\n2 + \u03b4k\u03b8POS \u2212 \u03b8\n0\nPOSk\n2\n,\n(7)\nwhich is similar to that of POS tagging, and \u03b8chk is\n(Wchk, bchk, EPOS, \u03b8e), where Wchk and bchk are\nthe weight and bias parameters including those in\n\u03b8POS, and EPOS is the set of the POS label embeddings. \u03b8\n0\nPOS is the one after training the POS\nlayer at the current training epoch.\n2The training code and the pre-trained embeddings\nare available at https://github.com/hassyGo/\ncharNgram2vec.\n3.4 Training the Dependency Layer\nThe objective function is defined as follows:\nJ3(\u03b8dep) = \u2212\nX\ns\nX\nt\nlog p(\u03b1|h\n(3)\nt\n)p(\u03b2|h\n(3)\nt\n, h(3)\n\u03b1 )\n+ \u03bb(kWdepk\n2 + kWdk\n2\n) + \u03b4k\u03b8chk \u2212 \u03b8\n0\nchkk\n2\n,\n(8)\nwhere p(\u03b1|h\n(3)\nt\n) is the probability value assigned to the correct parent node \u03b1 for wt\n,\nand p(\u03b2|h\n(3)\nt\n, h(3)\n\u03b1 ) is the probability value assigned to the correct dependency label \u03b2 for\nthe child-parent pair (wt\n, \u03b1). \u03b8dep is defined as\n(Wdep, bdep, Wd, r, EPOS, Echk, \u03b8e), where Wdep\nand bdep are the weight and bias parameters including those in \u03b8chk, and Echk is the set of the\nchunking label embeddings.\n3.5 Training the Relatedness Layer\nFollowing Tai et al. (2015), the objective function\nis defined as follows:\nJ4(\u03b8rel) = X\n(s,s0)\nKL \u0010\np\u02c6(s, s0\n)\n\n\n\n\n\np(h\n(4)\ns\n, h(4)\ns\n0 )\n\u0011\n+ \u03bbkWrelk\n2 + \u03b4k\u03b8dep \u2212 \u03b8\n0\ndepk\n2\n,\n(9)\nwhere p\u02c6(s, s0\n) is the gold distribution over the defined relatedness scores, p(h\n(4)\ns , h(4)\ns\n0 ) is the predicted distribution given the the sentence representations, and KL \u0010\np\u02c6(s, s0\n)\n\n\n\n\n\np(h\n(4)\ns , h(4)\ns\n0 )\n\u0011\nis the\nKL-divergence between the two distributions. \u03b8rel\nis defined as (Wrel, brel, EPOS, Echk, \u03b8e).\n3.6 Training the Entailment Layer\nThe objective function is defined as follows:\nJ5(\u03b8ent) = \u2212\nX\n(s,s0)\nlog p(y\n(5)\n(s,s0) = \u03b1|h\n(5)\ns\n, h(5)\ns\n0 )\n+ \u03bbkWentk\n2 + \u03b4k\u03b8rel \u2212 \u03b8\n0\nrelk\n2\n,\n(10)\nwhere p(y\n(5)\n(s,s0) = \u03b1|h\n(5)\ns , h(5)\ns\n0 ) is the probability value that the correct label \u03b1 is assigned to\nthe premise-hypothesis pair (s, s0\n). \u03b8ent is defined\nas(Went, bent, EPOS, Echk, Erel, \u03b8e), where Erel is\nthe set of the relatedness label embeddings.\n4 Related Work\nMany deep learning approaches have proven to be\neffective in a variety of NLP tasks and are becoming more and more complex. They are typically\n1926\ndesigned to handle single tasks, or some of them\nare designed as general-purpose models (Kumar\net al., 2016; Sutskever et al., 2014) but applied to\ndifferent tasks independently.\nFor handling multiple NLP tasks, multi-task\nlearning models with deep neural networks have\nbeen proposed (Collobert et al., 2011; Luong et al.,\n2016), and more recently S\u00f8gaard and Goldberg\n(2016) have suggested that using different layers\nfor different tasks is more effective than using the\nsame layer in jointly learning closely-related tasks,\nsuch as POS tagging and chunking. However, the\nnumber of tasks was limited or they have very similar task settings like word-level tagging, and it\nwas not clear how lower-level tasks could be also\nimproved by combining higher-level tasks.\nMore related to our work, Godwin et al. (2016)\nalso followed S\u00f8gaard and Goldberg (2016) to\njointly learn POS tagging, chunking, and language modeling, and Zhang and Weiss (2016)\nhave shown that it is effective to jointly learn POS\ntagging and dependency parsing by sharing internal representations. In the field of relation extraction, Miwa and Bansal (2016) proposed a joint\nlearning model for entity detection and relation extraction. All of them suggest the importance of\nmulti-task learning, and we investigate the potential of handling different types of NLP tasks rather\nthan closely-related ones in a single hierarchical\ndeep model.\nIn the field of computer vision, some transfer and multi-task learning approaches have also\nbeen proposed (Li and Hoiem, 2016; Misra et al.,\n2016). For example, Misra et al. (2016) proposed\na multi-task learning model to handle different\ntasks. However, they assume that each data sample has annotations for the different tasks, and do\nnot explicitly consider task hierarchies.\nRecently, Rusu et al. (2016) have proposed a\nprogressive neural network model to handle multiple reinforcement learning tasks, such as Atari\ngames. Like our JMT model, their model is also\nsuccessively trained according to different tasks\nusing different layers called columns in their paper. In their model, once the first task is completed, the model parameters for the first task are\nfixed, and then the second task is handled with new\nmodel parameters. Therefore, accuracy of the previously trained tasks is never improved. In NLP\ntasks, multi-task learning has the potential to improve not only higher-level tasks, but also lowerlevel tasks. Rather than fixing the pre-trained\nmodel parameters, our successive regularization\nallows our model to continuously train the lowerlevel tasks without significant accuracy drops.\n7 Conclusion\nWe presented a joint many-task model to handle\nmultiple NLP tasks with growing depth in a single end-to-end model. Our model is successively\ntrained by considering linguistic hierarchies, directly feeding word representations into all layers, explicitly using low-level predictions, and applying successive regularization. In experiments\non five NLP tasks, our single model achieves the\nstate-of-the-art or competitive results on chunking, dependency parsing, semantic relatedness,\nand textual entailment.", "SumGPT3": "Introduction:\nThe paper introduces a Joint Many-Task (JMT) model for Natural Language Processing (NLP) tasks, emphasizing the potential of leveraging multiple levels of representation.\n\nNovelty:\n\nHierarchical Task Prediction: Unlike traditional pipeline systems, the JMT model predicts increasingly complex NLP tasks at successively deeper layers, considering linguistic hierarchies.\nEnd-to-End Training: The model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment.\nAdaptive Training Strategy: An adaptive training and regularization strategy is proposed to grow the model's depth, avoiding catastrophic interference between tasks.\nSuccessive Regularization: Unlike previous models where pre-trained parameters were fixed, the JMT model employs successive regularization, allowing continuous training of lower-level tasks without significant accuracy drops.\nConclusion:\nThe JMT model presents a novel approach to handling multiple NLP tasks within a single end-to-end model. By considering linguistic hierarchies, employing adaptive training strategies, and utilizing successive regularization, the model achieves state-of-the-art or competitive results on chunking, dependency parsing, semantic relatedness, and textual entailment tasks.", "SumGem": "Introduction\n\nThis paper proposes a novel deep learning approach, called Joint Many-Task (JMT), for handling multiple Natural Language Processing (NLP) tasks. Unlike traditional pipeline systems, JMT can be trained end-to-end for various tasks by considering linguistic hierarchies. The model incorporates successive regularization to grow its depth and avoid catastrophic interference between tasks.\n\nNovelty\n\nJointly tackles multiple NLP tasks with varying complexity: The JMT model addresses several NLP tasks in a single framework, including POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. This approach goes beyond previous efforts that focused on closely related tasks or a limited number of tasks.\n\nSuccessive training with linguistic hierarchy consideration: The model trains tasks successively, leveraging the information learned from lower-level tasks to improve performance on higher-level tasks. This strategy aligns with linguistic hierarchies, where tasks like POS tagging and chunking provide essential building blocks for dependency parsing.\n\nDirect word representation feeding and low-level prediction utilization: The JMT model directly feeds word representations into all layers, allowing the model to exploit word information throughout the processing pipeline. Additionally, it leverages predictions from lower-level tasks as input for higher-level tasks, facilitating the transfer of knowledge between tasks.\n\nSuccessive regularization for continuous learning: To prevent forgetting of previously learned tasks during training of new tasks, the model employs a special regularization technique called successive regularization. This approach allows the model to continuously improve all tasks throughout the training process.\n\nConclusion\n\nThe JMT model achieves state-of-the-art or competitive results on various NLP tasks, demonstrating the effectiveness of the proposed approach for joint multi-task learning. This research paves the way for developing more robust and efficient NLP models capable of handling a broader range of tasks while considering linguistic relationships.", "SumGPT4": "### Introduction\nThe paper introduces a Joint Many-Task (JMT) model in the field of Natural Language Processing (NLP) that leverages linguistic hierarchies to handle multiple complex tasks within a single model. It significantly deviates from traditional NLP systems that typically employ separate models for different tasks or integrate multiple tasks at a similar computational depth.\n\n### Novelty\nThe novelty aspects discussed in the paper include:\n1. **Integrated Multi-Task Learning Architecture**: Unlike traditional models that handle tasks separately or simultaneously at the same layer, the JMT model predicts increasingly complex NLP tasks at successively deeper layers, reflecting a true understanding of linguistic hierarchies.\n2. **End-to-End Training**: The JMT model can be trained end-to-end for tasks like POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, which is a shift from the typical pipeline approaches where components are trained separately.\n3. **Adaptive Training and Regularization Strategy**: The paper proposes a new training strategy that prevents catastrophic interference between tasks, allowing the model to grow in depth while maintaining the integrity of earlier layers\u2019 predictions.\n4. **Successive Regularization**: This method aids in mitigating the forgetting of previously learned tasks, a common issue in sequential task training, thus enhancing the model\u2019s performance across all tasks.\n5. **Utilization of Predictions from Lower Layers**: The model effectively utilizes the predictions from lower layers to enhance the performance on higher-level tasks, showing an improved integration of task-related information throughout the model.\n\n### Conclusion\nThe JMT model presented in this paper offers a significant advancement in multi-task learning within NLP. By structuring the model to handle tasks at different depths corresponding to their complexity and integrating a novel training regimen, the authors demonstrate competitive or state-of-the-art performance across multiple NLP tasks. This model not only enhances task performance but also aligns more closely with how linguistic processing might occur in a more naturally hierarchical manner.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "Strengths:\n\nGood Detail on Novelty: This summary effectively emphasizes the novelty of the JMT model, particularly focusing on hierarchical task prediction, end-to-end training, and adaptive training strategy.\nClear Conclusion: The conclusion is clear, summarizing the model's performance and its significance in a straightforward manner.\nWeaknesses:\n\nRepetitive Information: Some points are repeated between the introduction and novelty sections, which could have been more succinctly combined.\nLess Structured: It lacks clear section headings, making it slightly harder to navigate compared to Summary 3.", "expGem": "Strengths:\n\nDetailed Explanation: This summary provides detailed information about the various aspects of the JMT model, including successive regularization and direct word representation feeding.\nWeaknesses:\n\nLess Focused: It includes a lot of detail, which, while informative, makes it more cumbersome to read. The points are a bit scattered without clear section headings or a coherent flow.\nLess Emphasis on Novelty: While it mentions the model's capabilities, it does not emphasize the novelty as strongly as the other summaries.\nConclusion Blending: The conclusion is less distinct, blending into the rest of the summary rather than standing out as a summary of the paper's impact.", "expGPT4": "Strengths:\n\nClarity and Structure: This summary is very well-organized, with clear section headings (Introduction, Novelty, Conclusion) that make it easy to follow.\nComprehensive Coverage: It succinctly covers the key points of the paper, including the novel aspects, the methodology, and the results. It effectively highlights the hierarchical task prediction, end-to-end training, adaptive training strategy, and successive regularization.\nDetail and Precision: It includes precise details about how the JMT model works, such as predicting tasks at successively deeper layers and leveraging predictions from lower layers to improve higher-level task performance.\nConclusion Integration: It ties the explanation back to the conclusion, reinforcing the model\u2019s impact on achieving state-of-the-art results.\nWeaknesses:\n\nThe summary is a bit more detailed, which is generally good, but it might be slightly longer than necessary for a quick overview."}, "13": {"title": "Unsupervised Recurrent Neural Network Grammars", "url": "https://aclanthology.org/N19-1114", "text": "1 Introduction\nRecurrent neural network grammars (RNNGs)\n(Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure\nwhich is used to construct a context representation\nto be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform\nstandard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the\nhuman brain (Hale et al., 2018). However, these\nall require annotated syntactic trees for training.\nIn this work, we explore unsupervised learning of\nrecurrent neural network grammars for language\nmodeling and grammar induction.\nWork done while the first author was an intern at DeepMind.\nCode available at https://github.com/harvardnlp/urnng\nThe standard setup for unsupervised structure\nlearning is to define a generative model p\u03b8(x, z)\nover observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech\nsequence), and maximize the log marginal likelihood log p\u03b8(x) = log P\nz\np\u03b8(x, z). Successful approaches to unsupervised parsing have made\nstrong conditional independence assumptions (e.g.\ncontext-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007). These strategies imbue the learning process with inductive biases that guide the\nmodel to discover meaningful structures while allowing tractable algorithms for marginalization;\nhowever, they come at the expense of language\nmodeling performance, particularly compared to\nsequential neural models that make no independence assumptions.\nLike RNN language models, RNNGs make no\nindependence assumptions. Instead they encode\nstructural bias through operations that compose\nlinguistic constituents. The lack of independence\nassumptions contributes to the strong language\nmodeling performance of RNNGs, but make unsupervised learning challenging. First, marginalization is intractable. Second, the biases imposed by\nthe RNNG are relatively weak compared to those\nimposed by models like PCFGs. There is little\npressure for non-trivial tree structure to emerge\nduring unsupervised RNNG (URNNG) learning.\nIn this work, we explore a technique for handling intractable marginalization while also injecting inductive bias. Specifically we employ amortized variational inference (Kingma and Welling,\n2014; Rezende et al., 2014; Mnih and Gregor,\n2014) with a structured inference network. Variational inference lets us tractably optimize a lower\nbound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con-\n1106\nditional random field (CRF) constituency parser\n(Finkel et al., 2008; Durrett and Klein, 2015),\nwhich makes significant independence assumptions, acts as a guide on the generative model\nto learn meaningful trees through regularizing the\nposterior (Ganchev et al., 2010).\nWe experiment with URNNGs on English and\nChinese and observe that they perform well as\nlanguage models compared to their supervised\ncounterparts and standard neural LMs. In terms\nof grammar induction, they are competitive with\nrecently-proposed neural architectures that discover tree-like structures through gated attention\n(Shen et al., 2018). Our results, along with other\nrecent work on joint language modeling/structure\nlearning with deep networks (Shen et al., 2018,\n2019; Wiseman et al., 2018; Kawakami et al.,\n2018), suggest that it is possible learn generative\nmodels of language that model the underlying data\nwell (i.e. assign high likelihood to held-out data)\nand at the same time induce meaningful linguistic\nstructure.\n2 Unsupervised Recurrent Neural\nNetwork Grammars\nWe use x = [x1, . . . , xT ] to denote a sentence of\nlength T, and z \u2208 ZT to denote an unlabeled binary parse tree over a sequence of length T, represented as a a binary vector of length 2T \u2212 1. Here\n0 and 1 correspond to SHIFT and REDUCE actions,\nexplained below.1 Figure 1 presents an overview\nof our approach.\n2.1 Generative Model\nAn RNNG defines a joint probability distribution p\u03b8(x, z) over sentences x and parse trees\nz. We consider a simplified version of the original RNNG (Dyer et al., 2016) by ignoring constituent labels and only considering binary trees.\nThe RNNG utilizes an RNN to parameterize a\nstack data structure (Dyer et al., 2015) of partiallycompleted constituents to incrementally build the\nparse tree while generating terminals. Using the\ncurrent stack representation, the model samples\nan action (SHIFT or REDUCE): SHIFT generates\na terminal symbol, i.e. word, and shifts it onto\nthe stack,2 REDUCE pops the last two elements off\nthe stack, composes them, and shifts the composed\n1The cardinality of ZT \u2282 {0, 1}\n2T \u22121\nis given by the\n(T \u2212 1)-th Catalan number, |ZT | =\n(2T \u22122)!\nT !(T \u22121)! .\n2A better name for SHIFT would be GENERATE (as in\nDyer et al. (2016)), but we use SHIFT to emphasize similarity\nwith the shift-reduce parsing.\nFigure 1: Overview of our approach. The inference\nnetwork q\u03c6(z | x) (left) is a CRF parser which produces a distribution over binary trees (shown in dotted\nbox). Bij are random variables for existence of a constituent spanning i-th and j-th words, whose potentials\nare the output from a bidirectional LSTM (the global\nfactor ensures that the distribution is only over valid binary trees). The generative model p\u03b8(x, z) (right) is an\nRNNG which consists of a stack LSTM (from which\nactions/words are predicted) and a tree LSTM (to obtain constituent representations upon REDUCE). Training involves sampling a binary tree from q\u03c6(z | x), converting it to a sequence of shift/reduce actions, and optimizing the log joint likelihood log p\u03b8(x, z).\nrepresentation onto the stack.\nFormally, let S = [(0, 0)] be the initial stack.\nEach item of the stack will be a pair, where the first\nelement is the hidden state of the stack LSTM, and\nthe second element is an input vector, described\nbelow. We use top(S) to refer to the top pair in\nthe stack. The push and pop operations are defined imperatively in the usual way. At each time\nstep, the next action zt (SHIFT or REDUCE) is sampled from a Bernoulli distribution parameterized\nin terms of the current stack representation. Letting (hprev, gprev) = top(S), we have\nzt \u223c Bernoulli(pt), pt = \u03c3(w>hprev + b).\nSubsequent generation depend on zt\n:\n\u2022 If zt = 0 (SHIFT), the model first generates a\nterminal symbol via sampling from a categorical distribution whose parameters come from an\naffine transformation and a softmax,\nx \u223c softmax(Whprev + b).\nThen the generated terminal is shifted onto the\nstack using a stack LSTM,\nhnext = LSTM(ex, hprev),\npush(S,(hnext, ex)),\nwhere ex is the word embedding for x.\n1107\n\u2022 If zt = 1 (REDUCE), we pop the last two elements off the stack,\n(hr, gr) = pop(S), (hl\n, gl) = pop(S),\nand obtain a new representation that combines\nthe left/right constituent representations using a\ntree LSTM (Tai et al., 2015; Zhu et al., 2015),\ngnew = TreeLSTM(gl\n, gr).\nNote that we use gl and gr to obtain the new\nrepresentation instead of hl and hr.\n3 We then\nupdate the stack using gnew,\n(hprev, gprev) = top(S),\nhnew = LSTM(gnew, hprev),\npush(S,(hnew, gnew)).\nThe generation process continues until an end-ofsentence symbol is generated. The parameters \u03b8 of\nthe generative model are w, b,W, b, and the parameters of the stack/tree LSTMs. For a sentence\nx = [x1, . . . , xT ] of length T, the binary parse tree\nis given by the binary vector z = [z1, . . . , z2T \u22121].\n4\nThe joint log likelihood decomposes as a sum of\nterminal/action log likelihoods,\nlog p\u03b8(x, z) = X\nT\nt=1\nlog p\u03b8(xt\n| x<t, z<n(t)\n)\n| {z }\nlog p\u03b8(x | z)\n+\n2\nX\nT \u22121\nj=1\nlog p\u03b8(zj | x<m(j)\n, z<j )\n| {z }\nlog p\u03b8(z | x<z)\n, (1)\nwhere z<n(t)\nrefers to all actions before generating\nthe t-th word, and similarly x<m(j)\nrefers to all\nwords generated before taking the j-th action. For\nbrevity, from here on we will use log p\u03b8(x | z) to\nrefer to the first term (terminal log likelihood) and\nlog p\u03b8(z | x<z) to refer to the second term (action\nlog likelihood) in the above decomposition.5\n3The update equations for the tree LSTM (and the stack\nLSTM) also involve cell states in addition to the hidden\nstates. To reduce notational clutter we do not explicitly show\nthe cell states and instead subsume them into g. If one (or\nboth) of the inputs to the tree LSTM is a word embedding,\nthe associated cell state is taken to be zero. See Tai et al.\n(2015) for the exact parameterization.\n4As it stands, the support of z is {0, 1}\n2T \u22121\n, all binary\nvectors of length 2T \u2212 1. To restrict our distribution to ZT\n(binary vectors which describe valid trees), we constrain zt to\nbe valid at each time step, which amounts to deterministically\nchoosing zt = 0 (SHIFT) if there are fewer than two elements\n(not counting the initial zero tuple) on the stack.\n5The action log likelihood is the sum of log conditional\npriors, which is obviously different from the unconditional\nlog prior log p\u03b8(z) = log P\nx\np\u03b8(x, z).\nIn the supervised case where ground-truth z\nis available, we can straightforwardly perform\ngradient-based optimization to maximize the joint\nlog likelihood log p\u03b8(x, z). In the unsupervised\ncase, the standard approach is to maximize the log\nmarginal likelihood,\nlog p\u03b8(x) = log X\nz\n0\u2208ZT\np\u03b8(x, z\n0\n).\nHowever this summation is intractable because zt fully depends on all previous actions\n[z1, . . . , zt\u22121]. Even if this summation were\ntractable, it is not clear that meaningful latent\nstructures would emerge given the lack of explicit\nindependence assumptions in the RNNG (e.g. it is\nclearly not context-free). We handle these issues\nwith amortized variational inference.\n2.2 Amortized Variational Inference\nAmortized variational inference (Kingma and\nWelling, 2014) defines a trainable inference network \u03c6 that parameterizes q\u03c6(z | x), a variational\nposterior distribution, in this case over parse trees\nz given the sentence x. This distribution is used to\nform an evidence lower bound (ELBO) on the log\nmarginal likelihood,\nELBO(\u03b8, \u03c6; x) = Eq\u03c6(z | x)\n\u0014\nlog p\u03b8(x, z)\nq\u03c6(z | x)\n\u0015\n.\nWe maximize the ELBO with respect to both\nmodel parameters \u03b8 and inference network parameters \u03c6. The ELBO is still intractable to calculate\nexactly, but this formulation will allow us to obtain unbiased gradient estimators based on Monte\nCarlo sampling.\nObserve that rearranging the ELBO gives the\nfollowing optimization problem,\nmax\n\u03b8,\u03c6\nlog p\u03b8(x) \u2212 KL[q\u03c6(z | x) k p\u03b8(z | x)].\nThus, \u03c6 is trained to match the variational posterior q\u03c6(z | x) to the true posterior p\u03b8(z | x), but \u03b8 is\nalso trained to match the true posterior to the variational posterior. Indeed, there is some evidence to\nsuggest that generative models trained with amortized variational inference (i.e. variational autoencoders) learn posterior distributions that are close\nto the variational family (Cremer et al., 2018).\nWe can use this to our advantage with an inference network that injects inductive bias. We\npropose to do this by using a context-free model\nfor the inference network, in particular, a neural\nCRF parser (Durrett and Klein, 2015). This choice\n1108\ncan seen as a form of posterior regularization that\nlimits posterior flexibility of the overly powerful\nRNNG generative model.6,7\nThe parameterization of span scores is similar to\nrecent works (Wang and Chang, 2016; Stern et al.,\n2017; Kitaev and Klein, 2018): we add position\nembeddings to word embeddings and run a bidirectional LSTM over the input representations to\nobtain the forward [\n\u2212\u2192h 1, . . . ,\n\u2212\u2192h T ] and backward\n[\n\u2190\u2212\nh 1, . . . ,\n\u2190\u2212\nh T ] hidden states. The score sij \u2208 R\nfor a constituent spanning xi\nto xj is given by,\nsij = MLP([\u2212\u2192h j+1 \u2212\n\u2212\u2192h i\n;\n\u2190\u2212\nh i\u22121 \u2212\n\u2190\u2212\nh j ]).\nLetting B be the binary matrix representation of a\ntree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution,\nq\u03c6(B | x) = 1\nZT (x)\nexp \u0010X\ni\u2264j\nBijsij\u0011\n,\nwhere ZT (x) is the partition function,\nZT (x) = X\nB0\u2208BT\nexp \u0010X\ni\u2264j\nB\n0\nijsij\u0011\n,\nand \u03c6 denotes the parameters of the inference network (i.e. the bidirectional LSTM and the MLP).\nCalculating ZT (x) requires a summation over an\nexponentially-sized set BT \u2282 {0, 1}\nT \u00d7T\n, the set\nof all binary trees over a length T sequence. However we can perform the summation in O(T\n3\n) using the inside algorithm (Baker, 1979), shown in\n6While it has a similar goal, this formulation differs the\nfrom posterior regularization as formulated by Ganchev et al.\n(2010), which constrains the distributional family via linear\nconstraints on posterior expectations. In our case, the conditional independence assumptions in the CRF lead to a curved\nexponential family where the vector of natural parameters has\nfewer dimensions than the vector of sufficient statistics of the\nfull exponential family. This curved exponential family is a\nsubset of the marginal polytope of the full exponential family, but it is an intersection of both linear and nonlinear manifolds, and therefore cannot be characterized through linear\nconstraints over posterior expectations.\n7\nIn preliminary experiments, we also attempted to learn\nlatent trees with a transition-based parser (which does not\nmake explicit independence assumptions) that looks at the entire sentence. However we found that under this setup, the inference network degenerated into a local minimum whereby\nit always generated left-branching trees despite various optimization strategies. Williams et al. (2018) observe a similar\nphenomenon in the context of learning latent trees for classification tasks. However Li et al. (2019) find that it is possible\nuse a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization (Ganchev et al., 2010)\nbased on universal syntactic rules (Naseem et al., 2010).\nAlgorithm 1 Inside algorithm for calculating ZT (x)\n1: procedure INSIDE(s) . scores sij for i \u2264 j\n2: for i := 1 to T do . length-1 spans\n3: \u03b2[i, i] = sii\n4: for ` := 1 to T \u2212 1 do . span length\n5: for i := 1 to T \u2212 ` do . span start\n6: j = i + ` . span end\n7: \u03b2[i, j] = Pj\u22121\nk=i\nsij \u00b7 \u03b2[i, k] \u00b7 \u03b2[k + 1, j]\n8: return \u03b2[1, T] . return partition function ZT (x)\nAlgorithm 1. This computation is itself differentiable and amenable to gradient-based optimization. Finally, letting f : BT \u2192 ZT be the bijection between the binary tree matrix representation\nand a sequence of SHIFT/REDUCE actions, the inference network defines a distribution over ZT via\nq\u03c6(z | x) , q\u03c6(f\n\u22121\n(z)| x).\n2.3 Optimization\nFor optimization, we use the following variant of\nthe ELBO,\nEq\u03c6(z | x)\n[log p\u03b8(x, z)] + H[q\u03c6(z | x)],\nwhere H[q\u03c6(z | x)] = Eq\u03c6(z | x)\n[\u2212 log q\u03c6(z | x)] is\nthe entropy of the variational posterior. A Monte\nCarlo estimate for the gradient with respect to \u03b8 is\n\u2207\u03b8 ELBO(\u03b8, \u03c6; x) \u2248\n1\nK\nX\nK\nk=1\n\u2207\u03b8 log p\u03b8(x, z\n(k)\n),\nwith samples z\n(1)\n, . . . , z\n(K)\nfrom q\u03c6(z | x). Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively (Goodman, 1998; Finkel et al., 2006),\nas shown in Algorithm 2. The gradient with respect to \u03c6 involves two parts. The entropy term\nH[q\u03c6(z | x)] can be calculated exactly in O(T\n3\n),\nagain using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each\nstep of this dynamic program is differentiable, we\ncan obtain the gradient \u2207\u03c6H[q\u03c6(z | x)] using automatic differentation.9 An estimator for the gradient with respect to Eq\u03c6(z | x)\n[log p\u03b8(x, z)] is obtained via the score function gradient estimator\n(Glynn, 1987; Williams, 1992),\n\u2207\u03c6Eq\u03c6(z | x)\n[log p\u03b8(x, z)]\n= Eq\u03c6(z | x)\n[log p\u03b8(x, z)\u2207\u03c6 log q\u03c6(z | x)]\n\u2248\n1\nK\nX\nK\nk=1\nlog p\u03b8(x, z\n(k)\n)\u2207\u03c6 log q\u03c6(z\n(k)\n| x).\n8We adapt the algorithm for calculating tree entropy in\nPCFGs from Hwa (2000) to the CRF case.\n9\u2207\u03c6H[q\u03c6(z | x)] can also be computed using the insideoutside algorithm and a second-order expectation semiring (Li and Eisner, 2009), which has the same asymptotic\nruntime complexity but generally better constants.\n1109\nAlgorithm 2 Top-down sampling a tree from q\u03c6(z | x)\n1: procedure SAMPLE(\u03b2) . \u03b2 from running INSIDE(s)\n2: B = 0 . binary matrix representation of tree\n3: Q = [(1, T)] . queue of constituents\n4: while Q is not empty do\n5: (i, j) = pop(Q)\n6: \u03c4 =\nPj\u22121\nk=i\n\u03b2[i, k] \u00b7 \u03b2[k + 1, j]\n7: for k := i to j \u2212 1 do . get distribution over splits\n8: wk = (\u03b2[i, k] \u00b7 \u03b2[k + 1, j])/\u03c4\n9: k \u223c Cat([wi, . . . , wj\u22121]) . sample a split point\n10: Bi,k = 1, Bk+1,j = 1 . update B\n11: if k > i then . if left child has width > 1\n12: push(Q,(i, k)) . add to queue\n13: if k + 1 < j then . if right child has width > 1\n14: push(Q,(k + 1, j)) . add to queue\n15: z = f(B) . f : BT \u2192 ZT maps matrix representation of tree to sequence of actions.\n16: return z\nThe above estimator is unbiased but typically suffers from high variance. To reduce variance, we\nuse a control variate derived from an average of\nthe other samples\u2019 joint likelihoods (Mnih and\nRezende, 2016), yielding the following estimator,\n1\nK\nX\nK\nk=1\n(log p\u03b8(x, z\n(k)\n) \u2212 r\n(k)\n)\u2207\u03c6 log q\u03c6(z\n(k)\n| x),\nwhere r\n(k) =\n1\nK\u22121\nP\nj6=k\nlog p\u03b8(x, z\n(j)\n). This\ncontrol variate worked better than alternatives\nsuch as estimates of baselines from an auxiliary\nnetwork (Mnih and Gregor, 2014; Deng et al.,\n2018) or a language model (Yin et al., 2018).\n5 Related Work\nThere has been much work on incorporating tree\nstructures into deep models for syntax-aware language modeling, both for unconditional (Emami\nand Jelinek, 2005; Buys and Blunsom, 2015; Dyer\net al., 2016) and conditional (Yin and Neubig,\n2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017;\nEriguchi et al., 2017; Wang et al., 2018; Gu et al.,\n2018) cases. These approaches generally rely on\nannotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs.\nPrior work on combining language modeling and\nunsupervised tree learning typically embed soft,\ntree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen\net al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency\n22Many prior works that induce trees directly from words\noften employ additional heuristics based on punctuation\n(Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013;\nParikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The\nURNNG still has to learn to rely on punctuation, similar to\nrecent works such as depth-bounded PCFGs (Jin et al., 2018)\nand DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen\net al., 2018) and Ordered Neurons (Shen et al., 2019) induce\ntrees by directly training on corpus without punctuation. We\nalso reiterate that punctuation is used during training but ignored during evaluation (except in Table 4).\ntrees. Our work is also related to the recent line\nof work on learning latent trees as part of a deep\nmodel through supervision on other tasks, typically via differentiable structured hidden layers\n(Kim et al., 2017; Bradbury and Socher, 2017; Liu\nand Lapata, 2018; Tran and Bisk, 2018; Peng et al.,\n2018; Niculae et al., 2018; Liu et al., 2018), policy\ngradient-based approaches (Yogatama et al., 2017;\nWilliams et al., 2018; Havrylov et al., 2019), or\ndifferentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018).\nThe variational approximation uses amortized\ninference (Kingma and Welling, 2014; Mnih and\nGregor, 2014; Rezende et al., 2014), in which an\ninference network is used to obtain the variational\nposterior for each observed x. Since our inference\nnetwork is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014)\nand structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously\nfor unsupervised (Cai et al., 2017; Drozdov et al.,\n2019; Li et al., 2019) and semi-supervised (Yin\net al., 2018; Corro and Titov, 2019) parsing.\n6 Conclusion\nIt is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising\nintermediate attention layers with syntactic heads\nimproves semantic role labeling, while Shi et al.\n(2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as\nlatent variables does provide useful inductive biases and improves performance.\nFinally, in modeling child language acquisition,\nthe complex interaction of the parser and the grammatical knowledge being acquired is the object\nof much investigation (Trueswell and Gleitman,\n2007); our work shows that apparently grammatical constraints can emerge from the interaction of\na constrained parser and a more general grammar\nlearner, which is an intriguing but underexplored\nhypothesis for explaining human linguistic biases.", "SumGPT3": "Introduction:\nThe paper explores the realm of unsupervised learning within recurrent neural network grammars (RNNGs) for language modeling and grammar induction. Traditional methods for unsupervised structure learning in language involve generative models, which often impose strong independence assumptions or employ auxiliary objectives. However, these strategies tend to compromise language modeling performance.\n\nNovelty:\n\nAmortized Variational Inference: The paper introduces a novel approach utilizing amortized variational inference with a structured inference network, specifically a conditional random field (CRF) constituency parser. This technique addresses the challenge of intractable marginalization while injecting inductive bias, thus facilitating the learning of meaningful tree structures.\nTop-Down Sampling Algorithm: A top-down sampling algorithm is proposed for generating trees from the variational posterior distribution, employing dynamic programming to recursively sample split points. This algorithm contributes to reducing variance in gradient estimation.\nControl Variate for Gradient Estimation: To further reduce variance, the paper suggests using a control variate derived from an average of other samples' joint likelihoods, enhancing the efficiency of gradient estimation in training.\nConclusion:\nThe paper underscores the significance of incorporating syntax into neural models for language processing. It demonstrates that incorporating syntax, either explicitly or as latent variables, provides valuable inductive biases and improves language modeling performance. This approach opens avenues for future research in understanding the interplay between parsers and grammar learners, offering insights into the emergence of grammatical constraints in language acquisition.", "SumGem": "Introduction\n\nThis paper explores unsupervised learning of recurrent neural network grammars (RNNGs) for language modeling and grammar induction. Supervised RNNGs have shown success in various tasks, but they require annotated syntactic trees for training, which can be expensive to obtain. This work addresses the challenge of training RNNGs without such supervision.\n\nNovelty\n\nThe paper proposes a novel approach to unsupervised learning of RNNGs using amortized variational inference. This technique addresses two challenges:\n\nIntractable Marginalization: Computing the marginal likelihood, which is essential for training, is intractable due to the dependencies between actions in the RNNG. The paper addresses this by employing variational inference, which allows for tractable optimization of a lower bound on the marginal likelihood.\n\nWeak Inductive Bias: Standard RNNGs lack strong inductive bias, making it difficult for them to learn meaningful syntactic structures during unsupervised training. The paper introduces a structured inference network in the form of a conditional random field (CRF) constituency parser. This parser acts as a guide for the RNNG, injecting inductive bias and encouraging the model to discover non-trivial tree structures.\n\nConclusion\n\nThe paper demonstrates that the proposed approach achieves good performance on language modeling tasks compared to supervised RNNGs and standard neural language models. Additionally, the model shows competitive results in grammar induction when compared to recent neural architectures that discover tree-like structures. These findings suggest that it is possible to learn generative models of language that can both model the underlying data well and induce meaningful linguistic structures without requiring explicit supervision. This approach provides valuable insights into the role of syntax in neural language models and opens doors for further exploration in unsupervised language learning.", "SumGPT4": "### Introduction\nThe paper delves into the unsupervised learning of Recurrent Neural Network Grammars (RNNGs), specifically focusing on language modeling and grammar induction without the need for annotated syntactic trees. This represents an expansion of RNNGs' application, traditionally dependent on supervised learning with structured linguistic data.\n\n### Novelty\n1. **Unsupervised Learning Framework:** The paper introduces an unsupervised learning approach for RNNGs, employing amortized variational inference with a structured inference network. This method enables the optimization of a lower bound on the log marginal likelihood, facilitating effective grammar induction and language modeling without pre-labeled data.\n   \n2. **Structured Inference Network:** A significant novelty is the use of a Conditional Random Field (CRF) constituency parser within the inference network. This CRF parser guides the generative model to learn meaningful syntactic trees, adding a layer of structured inductive bias that encourages the emergence of non-trivial syntactic structures during learning.\n\n3. **Integration of Amortized Variational Inference:** The approach uses variational inference to address the challenges of intractable marginalization inherent in unsupervised RNNGs. This strategy allows for a tractable approximation of complex probability distributions involved in unsupervised syntactic parsing.\n\n4. **Empirical Validation:** The paper extends its theoretical contributions with empirical experiments, demonstrating that unsupervised RNNGs perform comparably to supervised counterparts and traditional neural language models in both English and Chinese. This highlights the practical viability of the proposed unsupervised approach.\n\n### Conclusion\nThe study illustrates that incorporating unsupervised learning into RNNGs can effectively model language and induce grammar without relying on annotated data. The introduction of a structured inference network and the utilization of amortized variational inference represent substantial advancements in the field of natural language processing. These innovations contribute to a better understanding of how complex syntactic structures can be learned from raw data, paralleling some aspects of human language acquisition and processing. This paper suggests that integrating syntax as latent variables in neural models offers significant benefits, improving the performance of language models by providing robust inductive biases.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary also provides a detailed overview of the paper's contributions, particularly focusing on the novelty of using amortized variational inference and the CRF constituency parser. It explains the significance of these techniques in addressing challenges in unsupervised RNNGs and mentions the practical implications for language modeling performance. However, it is slightly less structured compared to Summary 3, and some points could be more clearly articulated.\n\nExplanation:\n\nClarity and Structure: The summary is clear but less structured than Summary 3, which affects readability.\nDetail and Comprehensiveness: It adequately covers the main contributions and innovations but with less emphasis on empirical validation and broader impact.\nContext and Impact: It touches on the importance of syntax in neural models but could elaborate more on the practical implications.", "expGem": "This summary is more concise and less detailed compared to the other two. While it mentions key concepts like amortized variational inference and the CRF parser, it does not provide as much context or explanation of these terms. The summary lacks the clear structure and depth found in Summaries 1 and 3, making it harder to fully appreciate the significance of the paper's contributions.\n\nExplanation:\n\nClarity and Structure: The summary is less structured, making it harder to follow.\nDetail and Comprehensiveness: It lacks depth and detail, providing only a basic overview without thoroughly explaining key points.\nContext and Impact: It mentions important concepts but does not fully explain their significance or the broader impact of the research.", "expGPT4": "This summary provides a comprehensive and clear explanation of the paper's contributions and innovations. It effectively highlights the key aspects of the unsupervised learning approach, including the integration of amortized variational inference and the use of a structured inference network (CRF parser). It also discusses the empirical validation and the broader implications of the research in a concise manner. The structured layout with distinct sections (Introduction, Novelty, Conclusion) makes it easy to follow and understand the significance of the work.\n\nExplanation:\n\nClarity and Structure: The summary is well-organized and clearly divided into sections, making it easy to grasp the main points.\nDetail and Comprehensiveness: It covers all major innovations and contributions in detail, ensuring a thorough understanding of the paper.\nContext and Impact: It discusses both the theoretical and empirical aspects, providing a well-rounded view of the research's impact."}, "14": {"title": "Automatic Noun Compound Interpretation using Deep Neural Networks and Word Embeddings", "url": "https://aclanthology.org/W15-0122", "text": "1 Introduction\nRecent research in computational semantics has increasingly made use of vector space representations\nof words in combination with deep neural network classifiers. This recent trend builds on the earlier\nsuccesses of such representations and classifiers for morphological and syntactic NLP tasks (Collobert\net al., 2011), and now also includes semantic tasks such as word similarity, word analogy as well as\nsentiment analysis (Mikolov et al., 2013; Pennington et al., 2014). The fact that the same type of vector\nrepresentations can be initially trained for one or more NLP tasks and then be re-used and fine-tuned for\na new, seemingly unrelated task suggests that such models can provide a unified architecture for NLP\n(Collobert and Weston, 2008). The fact that the performance of word embeddings, when combined with\ndeep neural networks, improves in a multi-task learning scenario and can provide state of the art results\nfor NLP further adds to the attractiveness of such methods.\nOne of the ways to further test the viability of such models and methods is to subject them to a wider\nrange of well-studied NLP tasks and compare the results with previous studies on state-of-the-art NLP\ndatasets.\nOne such task concerns the automatic interpretation of nominal compounds, a semantic phenomenon\nthat has been widely studied in both theoretical and computational linguistics. This task consists of\nidentifying the semantic relation that holds between the constituents of a compound (e.g. WHOLE+PART_\nOR_MEMBER_OF in the case of robot arm, LOCATION in the case of hillside home). Given that noun\ncompounding is a highly productive word formation process in many natural languages, the semantic\ninterpretation of compounds constitutes an important task for a variety of NLP tasks including machine\ntranslation, information retrieval, question answering, etc. Due to the productiveness of compounding,\nan adequate NLP system for the automatic interpretation of compounds will need to be able to generalize\nwell to unseen data, i.e. to compounds that it has not been trained on. Vector space models that are based\non very large training corpora and thus have a good coverage of the lexicon of a language provide a good\nfoundation for achieving such generalization behavior. Novel compounds are typically formed of existing\nwords in the language that are recombined to form a new complex word, whose meaning is usually more\nthan the sum of the meaning of its constituents, but which are constrained by the combinatory potential\n173\nof a word. This combinatory potential, i.e. the tendencies to combine with other words, is exactly what is\ncaptured in a vector space model, since such models capture the sum of the contexts that a word typically\nappears in. Hence, vector space models and deep neural network classifiers appear to be well suited for\nexperimenting with this task. Such experiments are facilitated by the availability of a large, annotated\ndataset of English compounds that is described in Tratz and Hovy (2010); Tratz (2011) and that was used\nin machine learning experiments.\nThe present paper reports on the results of experimenting with the Tratz (2011) dataset using four\npublicly available word embeddings for English and a deep neural network classifier implemented using\nthe Torch7 scientific computing framework (Collobert et al., 2011). These experiments yield results that\nare comparable to those reported by Tratz and Hovy (2010) and by Tratz (2011), but outperform their\nsystem on unseen compounds by a large margin. The remainder of this paper is structured as follows:\nSection 2 presents previous work related to the automatic classification of compound relations. Sections\n3 and 4 present the annotated noun compounds dataset and the four word embeddings that were used in\nthe experiments. These experiments are summarized in Section 5. The paper concludes with a summary\nof the main results and an outlook towards future work.\n2 Related Work\nOne of the earliest computational approaches to the classification of compound nouns is due to Lauer\n(1995), who reports an accuracy of 47% at predicting one of 8 possible prepositions using a set of 385\ncompounds. Rosario and Hearst (2001) obtain 60% accuracy at the task of predicting one of 18 relations\nusing neural networks and a dataset of 1660 compounds. The domain-specific inventory they use was\nobtained through iterative refinement by considering a set of 2245 extracted compounds and looking\nfor commonalities among them. Girju et al. (2005) use WordNet-based models and SVMs to classify\nnouns according to an inventory containing 35 semantic relations, and obtain accuracies ranging from\n37% to 64%. Kim and Baldwin (2005) report 53% accuracy on the task of identifying one of 20 semantic relations using a WordNet-based similarity approach, given a dataset containing 2169 noun compounds. \u00d3 S\u00e9aghdha and Copestake (2013) experiment with the dataset of 1443 compounds introduced\nin \u00d3 S\u00e9aghdha (2008) and obtain 65.4% accuracy when predicting one of 6 possible classes using SVMs\nand a combination of various types of kernels. Tratz and Hovy (2010) classify English compounds using a new taxonomy with 43 semantic relations, and obtain 79.3% accuracy using a Maximum Entropy\nclassifier and 79.4% accuracy using SVMmulticlass on their dataset comprising 17509 compounds and\n63.6%(MaxEnt)/63.1%(SVMmulticlass) accuracy on the (\u00d3 S\u00e9aghdha, 2008) data.\nAll these efforts have concentrated on English compounds, despite the fact that compounding is a\npervasive linguistic phenomenon in many other languages. Recent work by Verhoeven et al. (2012)\napplied the guidelines proposed by \u00d3 S\u00e9aghdha (2008) to annotate compounds in Dutch and Afrikaans\nwith 6 category tags: BE, HAVE, IN, INST, ACTOR and ABOUT. The reported F-Scores are 47.8% on the\n1447 compounds Dutch dataset and 51.1% on the 1439 compounds Afrikaans dataset.\n4 The embeddings\nThe automatic classification experiments presented in section 5 use a selection of publicly available\nword embeddings: the CW embeddings2\n, decribed in Collobert et al. (2011), the GloVe embeddings3\n,\npresented in Pennington et al. (2014), the HPCA embeddings4\n, described in Lebret and Collobert (2014)\nand the word2vec embeddings5\nintroduced by Mikolov et al. (2013). The vector size, the dictionary\nsize, the amount of training data as well as the specific corpora used for creating each of these word\nembeddings are summarized in Table 2.\nA word embedding W : D \u2192 Rn\nis a function that assigns each word from the embedding dictionary D an n-dimensional, real-valued vector. The words in the dictionary D are embedded in a\nhigh-dimensional vector space, such that the representations of syntactically and/or semantically similar\nwords are close together in the vector space. Word embeddings are the result of training language models\non large amounts of unlabeled, textual data using various learning algorithms.\nThe CW embeddings (Collobert et al., 2011) were obtained by training a language model using\na unified neural network architecture. The initial training step used unlabeled data (plain text from the\nsupport corpora) for training individual word embeddings. The training procedure uses a context window\nof size 11. Each context window is considered a positive training example for the word in the middle of\nthe context window, which is called the target word. For each positive context window, a corresponding\nnegative context window is generated where the target word is replaced with a random word from the\ndictionary. The training objective of the neural network can be described as learning to rank the correct\ncontext windows higher than the corrupted ones. This initial training step is followed by a supervised\ntraining step where the word embeddings are further refined in the context of 4 NLP tasks: part-of-sppech\ntagging, chunking, named entity recognition and semantic role labeling.\nThe GloVe model (Pennington et al., 2014) uses statistics of word occurences in a corpus as its\nprimary source of information. It involves constructing a large co-occurence matrix X, where each entry\nXij corresponds to the number of times the word j occurs in the context of the word i. The sum of the\nelements on the i-th row of the matrix represents the number of co-occurences of the word i with any\nother word in the dictionary in a fixed-size context window (10 words to the left and 10 to the right of\nthe target word). The model uses probability ratios as a mechanism for filtering out \u201cirrelevant words\u201d\nfor a given word-word pair.\nLebret and Collobert (2014) generate the HPCA word embeddings by applying Hellinger PCA to\nthe word co-occurence matrix, a simpler and faster method than training a full neural language model.\nWord frequencies are obtained by counting each time a word w \u2208 D occurs after a context sequence of\nwords T. The co-occurence matrix of size N x |D| contains the computed frequencies for all the words\nin the dictionary given all the N possible sequences of words. The 10,000 most frequent words in the\n2The CW embeddings are part of the SENNA NLP suite which can be downloaded from http://ronan.collobert.\ncom/senna/\n3Available at http://www-nlp.stanford.edu/projects/glove/\n4Available at http://lebret.ch/words/\n5Available at https://code.google.com/p/word2vec/\n175\nCategory name Dataset percentage Example\nObjective\nOBJECTIVE 17.1% leaf blower\nDoer-Cause-Means\nSUBJECT 3.5% police abuse\nCREATOR-PROVIDER-CAUSE_OF 1.5% ad revenue\nJUSTIFICATION 0.3% murder arrest\nMEANS 1.5% faith healer\nPurpose/Activity Group\nPERFORM&ENGAGE_IN 11.5% cooking pot\nCREATE-PROVIDE-GENERATE-SELL 4.8% nicotine patch\nOBTAIN&ACCESS&SEEK 0.9% shrimp boat\nMITIGATE&OPPOSE 0.8% flak jacket\nORGANIZE&SUPERVISE&AUTHORITY 1.6% ethics authority\nPURPOSE 1.9% chicken spit\nOwnership, Experience, Employment, Use\nOWNER-USER 2.1% family estate\nEXPERIENCER-OF-EXPERIENCE 0.5% family greed\nEMPLOYER 2.3% team doctor\nUSER_RECIPIENT 1.0% voter pamphlet\nTemporal Group\nTIME-OF1 2.2% night work\nTIME-OF2 0.5% birth date\nLocation and Whole+Part/Member of\nLOCATION 5.2% hillside home\nWHOLE+PART_OR_MEMBER_OF 1.7% robot arm\nComposition and Containment Group\nCONTAIN 1.2% shoe box\nSUBSTANCE-MATERIAL-INGREDIENT 2.6% plastic bag\nPART&MEMBER_OF_COLLECTION&CONFIG&SERIES 1.8% truck convoy\nVARIETY&GENUS_OF 0.1% plant species\nAMOUNT-OF 0.9% traffic volume\nTopic Group\nTOPIC 7.0% travel story\nTOPIC_OF_COGNITION&EMOTION 0.3% auto fanatic\nTOPIC_OF_EXPERT 0.7% policy expert\nOther Complements Group\nRELATIONAL-NOUN-COMPLEMENT 5.6% eye shape\nWHOLE+ATTRIBUTE&FEATURE 0.3% earth tone\n&QUALITY_VALUE_IS_CHARACTERISTIC_OF\nAttributive and Equative\nEQUATIVE 5.4% fighter plane\nADJ-LIKE_NOUN 1.3% core activity\nPARTIAL_ATTRIBUTE_TRANSFER 0.3% skeleton crew\nMEASURE 4.2% hour meeting\nOther\nLEXICALIZED 0.8% pig iron\nOTHER 5.4% contact lense\nPersonal*\nPERSONAL_NAME 0.5% Ronald Reagan\nPERSONAL_TITLE 0.5% Gen. Eisenhower\nTable 1: Semantic relation inventory used by the Tratz dataset - abbreviated version of Table 4.5 from\nTratz (2011). Note that some relations have a slightly different name in the actual dataset than the\naforementioned table; this table lists the relation names as found in the dataset.\n176\nName Embedding size Dictionary size Training data size Support corpora\nCW 50 130,000 0.85 bn enWikipedia + Reuters RCV1\nGloVe 300 400,000 42.00 bn Common Crawl (42 bn)\nHPCA 200 178,080 1.65 bn enWikipedia + Reuters + WSJ\nword2vec 300 3,000,000 100.00 bn Google News dataset\nTable 2: Overview of embedding sizes, dictionary sizes, training data sizes and support corpora for the\nfour selected embeddings. The training data size is reported in billions of words.\ndictionary were considered as context words, and size of the word sequence T was set to 1.\nMikolov et al. (2013) uses a continuous Skip-gram model to learn a distributed vector representation\nthat captures both syntactic and semantic word relationships. The authors define the training objective\nof this model as the ability to find \u201cword representations that are useful for predicting the surrounding\nwords in a sentence or a document\u201d. The training context for a word is defined as c words to the left and\nto the right of the word.\nFor the GloVe and HPCA embeddings there are multiple sizes of word embeddings available: 50,\n100, 200, 300 (6 billion words support corpus) and 300 dimensions (42 billion words support corpus) for\nGloVe and 50, 100 and 200 dimensions for HPCA. We have experimented with all the different sizes for\neach embedding and it was always the highest dimensional embedding that gave the best results in the\ncross-validation setup. Therefore, due to space limitations we only report results for the maximum size\nof each embedding.\n5 The experiments\nThis section summarizes the experiments performed on the Tratz dataset using the four embeddings described in the previous section. Section 5.1 describes the pre-processing steps that had to be performed\non the Tratz dataset in order to make it inter-operable with the embedding dictionaries. Section 5.2 describes the architecture of the classifier used in all the experiments. Section 5.3 presents the experiments\nperformed using each of the embeddings individually as well as the best performing system that resulted\nfrom the concatenation of three out of the four selected word embeddings.\n5.1 Dataset pre-processing\nIn order to make the best use of the word embeddings described in the previous section, several preprocessing steps had to be performed. The Tratz dataset contains about 1% training examples that are\nperson names or titles starting with a capital letter, whereas such names appear in all lowercase in the\nembedding dictionaries6\n. Therefore all compound constituents of the Tratz dataset were converted to\nlowercase. This resulted in a constituent dictionary C, |C| = 5242 unique constituents for the entire\nTratz dataset of 19158 compound instances.\nThe constituent dictionary C obtained from the Tratz dataset includes complex words such as \u2019healthcare\u2019 that are themselves compounds and which appear in the dataset as parts of larger compounds such\nas \u2019health-care legislation\u2019. Moreover, such complex words are not uniform in their spelling, appearing\nsometimes as a single word (e.g. \u2019healthcare\u2019), sometimes hyphenated (e.g. \u2019health-care\u2019) and sometimes as two separate words (e.g. \u2019health care\u2019). Therefore such spelling variation had to be adapted\nto the spelling conventions used by each individual embedding. The same type of adaptation had to be\nperformed in the case of misspelled words in the Tratz dataset and singular/plural forms of the same\nlemma. In cases where a constituent appears in the embedding dictionary as two separate words we use\nthe average of the individual word embeddings as a representation for the constituent.\n6On linguistic grounds, it is highly questionable whether personal names should be included in a compound dataset. However, since they are part of the Tratz dataset, we chose not remove them.\n177\nThe Tratz dataset also contains some infrequent English words such as \u2019chintz\u2019 (in \u2019chintz skirt\u2019),\n\u2019fastbreak\u2019 (in \u2019fastbreak layup\u2019) or \u2019secretin\u2019 (in \u2019sham secretin\u2019), which are not part of the embeddings\ndictionaries. We used an unknown word embedding for representing such words. This embedding is\nalready part of the dictionary for some embeddings (e.g. the CW embedding), and was obtained by\naveraging over the embeddings corresponding to the least frequent 1000 words for the other embeddings.\nThe pre-processed Tratz dataset was then partitioned into train, dev and test splits containing 13409,\n1920 and 3829 noun compounds, respectively. The combined train and dev splits were also used to\nconstruct a 10-fold cross-validation set.\n5.2 Classifier architecture\nWe used a deep neural network classifier implemented in the Torch7 scientific computing framework\n(Collobert et al., 2011) for the automatic classification of noun compounds. The classifier is trained in a\nsupervised manner on the examples in the Tratz dataset. A training example pairs the two constituents\nof a compound (e.g. the individual words \u2018robot\u2019 and \u2018arm\u2019 in the case of \u2018robot arm\u2019) with a semantic\nrelation from the relation inventory (e.g. WHOLE+PART_OR_MEMBER_OF).\nFigure 1 displays the architecture of the network which consists of four layers: an input layer, a\nlookup table, a hidden layer and an output layer. The lookup table (Figure 1a) is a |C|xN matrix which\ncontains an N-dimensional embedding for every entry in the constituent dictionary C.\nLookup Table, size=|C| x N\n1\n2\nirobot = 3\n4\n5\n.\n.\n.\n216\niarm = 217\n218\n.\n.\n.\n|C| \u2212 1\n|C|\n(a)\n.\n.\n.\nirobot\niarm\nr1\nrk\nInput\nlayer,\nsize = 2\nLookup table\nselection,\nsize = 2 x N\nHidden\nlayer,\nsize = N\nOutput\nlayer,\nsize = k\n(b)\nFigure 1: Classifier architecture\nWhen a training example is presented to the network, the input layer and the lookup table are used\nto extract the word representations for the two constituents of the example compound. The input layer is\nused to input two numerical indices that uniquely identify each constituent in the constituent dictionary.\nThese indices are then used to select the individual word representations of the compound constituents\nfrom the lookup table. The selected representations are concatenated and the combined representation is\nfed to the subsequent hidden layer. This hidden layer is intended to capture regularities in the data that\nare relevant for selecting the correct semantic relation of the example compound. Since the hidden layer\nis fully connected to the previous layer, such regularities can be drawn from the word representations\nof both compound constituents, as well as from the relative order of the two constituents. The optimal\nsize of the hidden layer (N, which matches the size of the initial word representation) was determined\nempirically based on the dev split.\nThe resulting compound representation is then passed through a non-linearity (the logistic function,\n1\n1+exp\u2212x ) that maps it to the final output layer. The size of the output layer equals the number of semantic\nrelations in the Tratz dataset. A softmax function is used to pick the semantic class which was assigned\nthe highest score by the neural network classifier.\nThe purpose of the lookup table is to allow the generic word embeddings, which were constructed\nindependently of the compound classification task, to be fine-tuned to the current task. This fine-tuning\n178\nis made possible by having the lookup table as an intermediate layer in the network \u2013 and thus modifiable\nby backpropagation during the training process \u2013 as opposed to having the embeddings directly as the\ninput of the network. The fine-tuning of embeddings for a particular task, other than the one they have\nbeen initially trained for, has been advocated and proven effective by Collobert et al. (2011); Lebret and\nCollobert (2014) in the case of several NLP tasks like part-of-speech tagging, named entity recognition\nand sentiment analysis. In order to gauge the impact of embedding fine-tuning for the present task we\ncompared the results of training a classifier with and without fine-tuning. The classifier without finetuning consists of an input layer of size 2 x N, a hidden layer of size N, and the output layer.\nThe network was trained using the negative log likelihood criterion, using averaged stochastic gradient descent optimization (Bottou, 2012) with a batch size of 5 and an initial learning rate of 0.9. The\nlearning rate is adapted during the training phase, by setting it to 0.3 once the error on the development\nset is lower then a specified threshold. We used the dev split to choose the hyper-parameters of the model,\nwhich were used in all the reported experiments.\nAn early stopping criterion was employed in order to avoid the inherent tendency of neural networks\nto overfit the training data. We used a criterion proposed by Prechelt (1998), namely stop the training\nwhen the generalization error (i.e. the error on the dev set) has increased in s successive epochs. We set\nthe number of successive epochs in which the generalization error is allowed to fluctuate to s = 5. The\nfinal model returned by the training procedure is the model with the best generalization error that was\ndiscovered during the training procedure.\n6 Conclusion\nIn this paper we have presented a deep neural network classifier approach for the task of automatic noun\ncompound interpretation for English. We have shown that this approach achieves comparable results to\nthe state of the art system trained on a closely-related dataset and significantly outperforms this earlier\nsystem when confronted with unseen compounds. Another advantage of our approach derives from the\nuse of pre-trained word embeddings as word representations, rather than using large, manually selected\nfeature sets that are constructed and optimized for a specific task as the initial word representations.\nSince word embeddings are more generic in nature and allow for re-training in a multi-task scenario,\nthis approach has the potential of being reused, including for related tasks of semantic interpretation of\ncompounds by prepositional paraphrasing, as has been proposed by Lauer (1995), or free paraphrasing,\nwhich has been the subject of a shared SemEval task (Hendrickx et al., 2013). However, for the time\nbeing, we have to leave such re-purposing to future research. Another direction for future research is to\ntest our approach on other available noun compound datasets for English such as the one provided by\n\u00d3 S\u00e9aghdha (2008). This would allow us to directly compare our approach to earlier systems trained\non these datasets. Since the Tratz dataset is considerably larger than all the other publicly datasets, also\ntesting our system on the latter and comparing the relative performance would allow us to better estimate\nthe impact of the training data size as well as the one of the annotation scheme when training deep neural\nnetwork classifiers for automatic noun compound interpretation.\n", "SumGPT3": "Introduction:\n\nThe paper discusses recent advancements in computational semantics, particularly focusing on the utilization of vector space representations of words alongside deep neural network classifiers. It highlights the versatility of these models in handling various NLP tasks, including semantic tasks like word similarity, analogy, and sentiment analysis.\n\nNovelty:\n\nUnified Architecture: The paper introduces the idea of a unified architecture for NLP tasks, where vector representations initially trained for one or more tasks can be repurposed and fine-tuned for seemingly unrelated tasks. This approach shows promise in achieving state-of-the-art results across multiple NLP tasks.\n\nCompound Interpretation: It explores the automatic interpretation of nominal compounds, a crucial task in NLP, involving identifying semantic relations between compound constituents. Unlike previous efforts, this paper utilizes deep neural network classifiers combined with large annotated datasets to achieve significant improvements in performance, particularly on unseen compounds.\n\nEmbedding Experimentation: The paper conducts experiments using four publicly available word embeddings for English, demonstrating their effectiveness in compound interpretation tasks. By leveraging word embeddings trained on large corpora, the models exhibit robust generalization behavior to unseen compounds.\n\nConclusion:\n\nIn conclusion, the paper presents a novel approach to NLP tasks by leveraging vector space representations and deep neural network classifiers. It demonstrates significant advancements in compound interpretation tasks, outperforming previous systems and showcasing the potential of pre-trained word embeddings in multi-task learning scenarios. The findings open avenues for further research in related areas and underscore the importance of experimenting with diverse datasets and embedding techniques in computational semantics.", "SumGem": "Introduction\n\nThe paper proposes a deep neural network classifier for automatically interpreting nominal compounds in English. This task involves identifying the semantic relation between the two words that form a compound (e.g., WHOLE+PART_OR_MEMBER_OF for \"robot arm\").\n\nNovelty\n\nThe paper discusses several novel aspects of the proposed approach:\n\nUtilizing pre-trained word embeddings: Instead of relying on manually designed features, the model leverages pre-trained word embeddings, which capture semantic relationships between words based on large text corpora. This allows for more generic word representations and the potential for re-training the model on related tasks.\nFine-tuning word embeddings: The model incorporates a lookup table to fine-tune the pre-trained word embeddings for the specific task of compound interpretation. This enables the model to adapt the generic word embeddings to the task at hand.\nFocus on unseen compounds: The model demonstrates significant improvement over previous systems when tested on unseen compounds, indicating its ability to generalize beyond the training data.\nConclusion\n\nThe deep neural network classifier with fine-tuned pre-trained word embeddings shows promising results for automatic noun compound interpretation. This approach offers advantages like reusability for related tasks and potentially better generalizability compared to methods relying on manually crafted features. The paper calls for further exploration on other datasets and evaluating the impact of training data size and annotation schemes.", "SumGPT4": "**Introduction:**\nThe paper investigates the potential of vector space models combined with deep neural networks for automatic interpretation of nominal compounds in English. It builds on existing research in computational semantics that utilizes word embeddings for various NLP tasks, demonstrating that these representations can be adapted and fine-tuned for new, complex semantic tasks.\n\n**Novelty:**\n1. **Extensive Dataset Utilization**: Unlike prior studies, this paper experiments with the large Tratz dataset, allowing for robust testing and significant improvements in classifying unseen nominal compounds. The extensive dataset helps in evaluating the model's ability to generalize beyond its training data, a critical aspect for practical NLP applications.\n2. **Integration of Multiple Embeddings**: The research uniquely combines four different publicly available word embeddings\u2014CW embeddings, GloVe, HPCA, and word2vec\u2014in its experiments. This multi-embedding approach contrasts with typical single-model studies, offering insights into how different embeddings contribute to and enhance compound interpretation.\n3. **Fine-tuning Word Embeddings**: The study not only uses pre-trained embeddings but also explores fine-tuning them specifically for the task of noun compound classification. This methodological tweak is anticipated to refine the embeddings further, tailoring them to capture more nuanced semantic relationships specific to noun compounds.\n4. **Advanced Neural Network Architecture**: The application of a deep neural network classifier implemented in Torch7, specifically designed for this task, underscores a technical advancement. The architecture includes a custom lookup table and multiple layers that effectively utilize the embeddings and refine the classification process based on the semantic relations between compound constituents.\n\n**Conclusion:**\nThe paper presents a sophisticated approach to the semantic interpretation of noun compounds, showcasing improvements over previous methods primarily through the innovative use of a multi-embedding strategy and the extensive fine-tuning of these embeddings. The results significantly surpass earlier systems in handling unseen compounds, marking a promising direction for future research on scalable and adaptable NLP systems. The proposed methodology not only enhances performance on a specific task but also sets a foundation for applying similar strategies to other semantic interpretation tasks. Future work could extend this approach to other languages and datasets, further exploring the scalability and adaptability of the model.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "provides a good overview of the paper's content. It starts with an introduction to the topic and discusses the use of vector space models and deep neural networks for interpreting nominal compounds. It mentions the novelty of the approach in using pre-trained word embeddings and fine-tuning them, as well as focusing on unseen compounds. However, it lacks specific technical details and comparisons that would have strengthened its ranking compared to Summary 3.", "expGem": "the most concise but also the least detailed among the summaries provided. While it covers the basics such as the introduction of the paper, the novelty in using extensive datasets and multiple embeddings, and the fine-tuning of word embeddings, it lacks the depth and specificity of the other summaries. It could benefit from more elaboration on technical aspects and comparison with existing methods to better convey the paper's contributions.", "expGPT4": "provides a clear and structured overview of the paper. It introduces the topic, discusses the novelty of the approach in detail with specific technical aspects like dataset utilization, integration of multiple embeddings, and fine-tuning of word embeddings. It concludes by highlighting the significance of the findings and suggesting avenues for future research. This summary effectively captures the essence of the paper, its contributions, and implications."}, "15": {"title": "SupportNet: Neural Networks for Summary Generation and Key Segment Extraction from Technical Support Tickets", "url": "https://aclanthology.org/2021.ecnlp-1.20", "text": "1 Introduction\nIt is of paramount importance to AWS Support\norganization to reduce the resolution time of customer cases to ensure their business runs seamlessly without any downtime. We have a unique\nchallenge in that the customers\u2019 issues can be\ndeeply technical and require technically skilled\nagents to resolve it. There is a rapid increase in\nthe number of users of the services offered by our\ncloud company and it is important to improve\ntooling for Support Engineers (SEs) in order to\nscale. A significant portion of customers\u2019 cases\nare business critical and time-sensitive.\nCases are created by customers for several reasons such as guidance about a specific service\nor troubleshooting when a production service is\ndown. A typical customer creates a case with a\ntitle of the case (case title) and a detailed correspondence on their issue as a part of the case\n(communication text). The agents then have to\nread this case, understand the customer\u2019s problem and suggest ways for them to resolve the issues. This requires the agents to spend a lot of\ntime to completely read the case and then guide\nthe customer to solve the problem. We have developed two systems that can use the inputs from\nthe customer in the form of case text (case title +\ncommunication text) to speed up the agent\u2019s time\nto resolve a case. The first system is a summarization system that presents the customers problem\nto the agent to give them a head start in tackling\nthe case. The second system provides them snippets from similar historical cases to reduce the\ntime the agent takes to respond to the customer.\nPrior work in the domain of customer support\nhas focused on improving the time to resolve a\ncase by improving routing and detecting the customer problems into one of several predefined\ncategories (Gupta et al., 2013; Hui and Jha, 2000;\nMuni et al., 2017; Molino et al., 2018). While these\nmethods reduce average total time to resolve issues throughout the case journey, they do not\nfocus on reducing the active handle time by SEs,\ni.e., the time a SE has to invest to understand and\nrespond to the customer. In our work, we address\nthis gap by introducing two novel solutions as\npreviously described.\nSEs often look to similar previously resolved\ncases when beginning to tackle a new case according to a few internal studies. A previous similar case provides them troubleshooting resources,\nhints on root causes, and guidance material that\nthey reuse on the new case. These resources from\n165\nprevious similar cases have been found to reduce\nthe handle time by SEs, but it takes time for agent\nto browse through the results. Hence, we built\na knowledge mining system based on NLP that\nallows SEs to efficiently look up historical cases\nwithout having to read the whole case.\nTypical technical cases contain many conversations, and reading through them is a timeconsuming process. In order to solve this problem, a system was introduced - State of the Case\n(SOC) - where SEs update a summary of the current state of a case. These updates are made manually by SEs while they hand the case back to the\ncustomer or to another internal team. In addition to serving as a smooth transition between\nSEs throughout the case lifecycle, the SOC of a\ncase was intended to serve as its expert summary\nview, eliminating the vagueness and jargon that\nmay be present in customers\u2019 case text. However,\nthe additional manual effort to fill SOC resulted\nin a low adoption rate of 9.8% over time. We introduce the solution in Section 3.2 automating SOC\ngeneration based on customer communications.\nTable 1 provides a simulated example of the data\nwe have. The SEs can use the SOC to get a head\nstart in solving the case.\nCustomer case text\nCase Title: Server down because of full volume\nCase description: We had our server go down this\nevening because the @gig volume of our EC@ server\nfull. To avoid this in the future, I have two questions: -\nHow can we know the amount of free space left in the\nEc@ volume? - Is there a way we can setup alerts to\nmonitor free usage? Thank you for your help in this\nmatter. Have a good day, Instance ID(s): How can we\ntrack the storage / volume usage of our EC@ instances\nState of Case(SOC) Case Summary: how can we track\nthe free space remaining on the volume of an EC2 server\n?@- how do we set up alerts at certain thresholds\nto know to act?\nTable 1: The customer can describe their problem in\nseveral different ways. The state of the case summarizes the customer issue into something that is actionable.\nWe show that the summary version that a\nSE would write in SOC can be automatically\ngenerated using the state-of-the-art encoder\n(Bert (Devlin et al., 2018))-decoder (GPT-2 (Radford et al., 2019)) models with cross attention.\nWe compare this model against recent baseline\nmodels such as Bart (Lewis et al., 2019) and\non traditional encoder decoder models using\nLSTMs. We present findings that support using\nthis model in production when compared to a\nmore memory efficient model such as LSTM. Any\nconditional generation task requires a parallel\ndataset on which the model is trained on. Our\ndataset on the other hand has labeled data for the\nencoder text as well. This allows us to introduce\na classification loss on the encoder to obtain\nbetter encoder representations that can be fed to\nthe decoder. This also allows us to jointly train\nan encoder and a decoder with a simultaneous\nmulti-task objective. We perform this novel\nexperiment to show the efficacy of training\nthe encoder with a cross entropy loss function\nwhile the decoder is trained using Maximum\nLikelihood Estimation (MLE).\nThus, the contribution of this work can be summarized as follows:\n1. We present a model that is capable of mining technical guidance phrases from support\ncases.\n2. We present a summarization model that generates a concise summary of customer problems from the communication text. We also\ncompare several summarization models and\ndiscuss the potential impact of productionizing our model. In addition, we performed\nmulti-task learning on the encoder to determine if it can improve decoder\u2019s performance.\n3. We present the results from a human subject\nstudy to show the usefulness of our solution.\nInitial results suggest that the SEs considered\nthe summary generated by our model as a\ngood starting point to solve a case.\n2 Related Work\nAs mentioned earlier, Molino et al. (2018) built\nsystems that could categorize case issues into predefined categories. They also suggest predefined\ntemplates to SE. Specific details of the case are\nnot taken into picture while suggesting these templates to make them more appropriate. Our summary model on the other hand ingests the context\nof the case and generates a personalized problem\n166\nFigure 1: (a) Component 1 reads the contents of the incoming case and sends a summary to the support agent\nto get a head start. (b) Component 2 reads the contents of the historical cases and provides guidance excerpts\nfor the agents to respond back to the customer.\ntemplate that a SE can use to update the state of\ncase.\nPrior work (Godse et al., 2018; Chess et al., 2007;\nBrittenham et al., 2007; Pathak and Khandelwal,\n2017) has looked at providing better Information\nTechnology Service Management (ITSM) to their\ncustomers by building resources that help the\ncustomers diagnose their own issue and find a\nsolution for it. However, customers usually try to\nself-diagnose their issues before cutting a case.\nHence, our solution focuses on helping the SE\nand improve their efficiency.\nOther work (Gupta et al., 2013; Hui and Jha,\n2000; Muni et al., 2017) has looked at the use of\nthe support case text along with other metadata\nto classify the intent of the case and improve routing. We on the other hand analyze the case text to\nprovide assistance to SE in their day-to-day tasks.\n3 System and Model Overview\nFigure 1 shows the overall architecture of the proposed system. There are two major components\nthat we propose in this work. The first one uses\na Bert and GPT-2 model to allow the SE to get a\nhead start in solving the case. The second component runs on the case text of previously solved\ncases to provide the SE with guidance phrases.\nThey can use the predicted guidance phrases to\nunderstand how the case can be solved and to\nalso construct a response back to the customer.\nIn this section, we provide details of the models\nthat were built in these components and the steps\nthat we took to train them.\n3.1 Guidance Extraction Model\nSupport cases that we receive from customers\nare filled with jargon rich text that takes highly\nskilled agents to read and understand. In order to\ntrain the models that can understand this text, we\nneed large amounts of supervised data that is very\nexpensive to obtain as it requires expert annotations. However, we can train large Language Models (LMs) with the vast amounts of self-supervised\ncase text that enables the models to understand\nthis jargon filled technical domain.\nFollowing Lee et al. (2020); Beltagy et al. (2019)\nwe continue the pre-training of the model presented by Devlin et al. (2018) . We continue the\npre-training of the Bert model for another 60,000\nsteps on the support cases that we have received\nin the period of 2019\u221220. We call this model SupportBert. We show that this model outperforms\nthe Bert base model trained on English Wikipedia\nand the Book corpus (Zhu et al., 2015) on our guidance phrase prediction task. We follow the standard procedure of fine-tuning this model on a\nlabeled dataset of guidance excerpts. More about\nthis dataset is presented in Section 4. We try several variants of the models while pre-training and\nthe details of the experiments are presented in\nSection 5.\n3.2 Summarization Model\nWe use a Seq2Seq (Sutskever et al., 2014) model\nwith the cross attention as our baseline model.\nWe use a Bert encoder and a GPT-2 decoder to\nsummarize the case content. For every word Wi\nthat belongs to the case description and the communication text, we pass that word through the\n167\nFigure 2: The complete setup of our summary model is shown above. We use the output of the GPT-2 decoder\nas a summary of the case description fed into the Bert Encoder. For the multi-task experiment we use the class\nlabel to add more gradients to the encoder.\nBert encoder to obtain the contextualized representations of the case content. We use Maximum\nLikelihood Estimation (MLE) to train the decoder\non the case summary (S1...ST\n0). The case summaries entered by SEs on historical cases are used\nto train our model. Here Si represents every word\nof the summary at a time step t\n0\n. T\n0\nrepresents\nthe overall length of the summary. Our overall\narchitecture is shown in Figure 2.\nWe first pass the words(Wi) into the SupportBert model to get a contextualized representation\nof every word(henc ).\nh\nT\nenc = Ber t_encoder (W1,W2....WN ) (1)\nWe then use these hidden states as the keys to\nthe cross-attention units of the GPT-2 decoder.\nAt each stage of the GPT-2 decoder we will see a\nprobability distribution on the vocabulary(P\nV\nt\n).\nP\nV\nt = GPT _2(h\nT\nenc ,S1...t) (2)\nWe use the SOC described in Table 1 for training the decoder with MLE. We want to maximize\nthe log likelihood of the probability of the true\nword, in other words we want to minimize the\nnegative log likelihood of the probability of the\ntrue word.\nl osst = \u2212l og P(W t r ue\nt\n) (3)\nTotal decoder loss is averaged cross entropy\nloss at each time step of the decoder.\nlossdecoder =\n1\nTdecoder\nTdecoder X\nt=1\nl osst (4)\nDuring inference, we use the words generated\nby the decoder till time t and the Bert embeddings to produce the word at time t +1.\nSt+1 = arg max\nV\n[softmax(GPT _2(h\nT\nenc ,St))] (5)\n3.2.1 Multi-task training\nIn this variant of the summarization model, we\nalso predict the issue category of the case text\nalong with generating the summary. We have a\nunique corpus that has a label for the encoder\ntext to train the encoder and SOC text to train the\ndecoder. This enables us to jointly train the encoder and the decoder with both these loss functions. The encoder receives gradients from not\nonly the MLE objective of the decoder but also the\ncross-entropy loss from issue categorization (241\npre-defined categories). Multi-task learning has\nshown to improve the performance of the Bert\nbase model (Liu et al., 2019). However, each task\nhead during this training phase is trained independently as parallel labels are not available. Also,\nthere is a lack of a public corpus that enables us\nto jointly train the encoder on a classification task\nand the decoder on a text generation task. Our\ncorpus allowed us to perform this experiment.\nlossencoder =\n241\nX\ni=0\nP(X)l og (Q(X)) (6)\nloss = lossencoder +lossdecoder (7)\n7 Conclusion\nIt is important for us to scale the support business with our rapid user growth. We describe two\ncomponents of a system that aims to reduce the\ntime spent by SE in resolving a support case. The\naim of this work is to promote research at the intersection of NLP and support business. Using\nour models, we were able to achieve an F1 score\nof 0.64 on the guidance extraction problem and\nBertScore (F1) of 0.55 on the summarization problem. These promising results shows us that they\ncan be deployed in production, create impact and\nhelp SEs in their day-to-day tasks. We hope this\ncontribution can lead to better tools that can improve the tooling necessary for support agents to\nprovide a rich customer experience.", "SumGPT3": "Introduction:\nThe paper addresses the imperative need within AWS Support to diminish the resolution time of customer cases, particularly those of a highly technical nature. Traditional approaches have centered on categorizing cases and improving routing, but they often neglect the active handle time required by Support Engineers (SEs). To bridge this gap, the paper introduces two innovative solutions: a summarization system and a knowledge mining system leveraging historical cases.\n\nNovelty:\n\nAutomated Summarization: Unlike previous methods that primarily focus on categorization, the paper introduces a summarization system that automatically generates concise summaries of customer issues using state-of-the-art encoder-decoder models like Bert and GPT-2. This approach not only expedites the SEs' understanding of the case but also enhances their response time.\n\nEfficient Knowledge Mining: Traditional methods rely on SEs manually updating the State of the Case (SOC) to provide summaries of ongoing cases. However, the low adoption rate of this manual process prompted the development of an automated SOC generation system based on customer communications. This system streamlines the process for SEs, reducing the time required to browse through historical cases while still providing valuable insights.\n\nMulti-Task Learning: The paper explores the efficacy of multi-task learning by jointly training an encoder and a decoder with a simultaneous multi-task objective. This approach not only enhances the encoder's representations but also improves the decoder's performance, contributing to more accurate summarization of customer issues.\n\nConclusion:\nThe paper presents novel advancements in customer support systems, particularly in reducing the active handle time of SEs. By automating case summarization and SOC generation, leveraging advanced NLP models, and exploring multi-task learning, the proposed solutions demonstrate promising results in improving efficiency and enhancing the overall support experience for AWS customers.", "SumGem": "This paper describes a system with two components that use Natural Language Processing (NLP) techniques to improve the efficiency of Support Engineers (SEs) by reducing the time they spend resolving customer support cases.\n\nNovelty\n\nThe system utilizes customer case text and historical case data to automatically generate summaries of customer issues and identify relevant guidance phrases from similar resolved cases. This can significantly reduce the time SEs spend understanding new cases.\nThe system leverages a pre-trained Bert model (SupportBert) on a large corpus of customer support cases to improve its understanding of the technical domain and jargon used by customers.\nThe summarization model uses a Seq2Seq architecture with a Bert encoder and GPT-2 decoder with cross-attention. The model is jointly trained on two tasks: predicting the issue category of the case text and generating a summary of the customer communication. This joint training with a classification task is a novel approach not previously explored in public corpora.\nConclusion\nThe paper demonstrates that the proposed NLP system can improve the efficiency of SEs by automatically summarizing customer cases and suggesting relevant guidance phrases. The system offers promising results and paves the way for further research on the intersection of NLP and customer support.", "SumGPT4": "**Introduction:**\nThe paper outlines the advancements made by AWS Support in reducing case resolution times, crucial for maintaining seamless business operations given the technical nature of customer inquiries. It presents novel technological solutions designed to expedite the support process for Support Engineers (SEs) amidst a rapidly increasing user base of cloud services.\n\n**Novelty:**\n1. **Summarization Model Integration**: The paper introduces a novel integration of a summarization model employing a Bert encoder and a GPT-2 decoder with cross-attention mechanisms. This model automatically generates concise summaries of customer problems from detailed case communications, which is pivotal in providing SEs with a quick comprehension of the issue at hand.\n\n2. **Utilization of Historical Case Data**: Another innovation is the system's ability to provide SEs with snippets from previously resolved cases. This leverages historical data to furnish pertinent troubleshooting resources, root cause hints, and guidance, thereby decreasing the time spent by SEs in formulating responses.\n\n3. **Automated State of the Case (SOC) Generation**: The development of an automated SOC generation system is a significant advancement. It transforms detailed customer communication into a clear, actionable summary, overcoming the challenge of low adoption rates due to the manual effort previously required.\n\n4. **Multi-Task Learning Framework**: The implementation of a multi-task learning approach where the encoder benefits from additional gradients through a classification task while the decoder focuses on generating text. This structure enhances the model's ability to understand and summarize technical jargon-filled texts effectively.\n\n**Conclusion:**\nThis paper demonstrates the effective application of NLP techniques to improve the efficiency of support services in a cloud computing environment. By automating key components of the customer support process and leveraging advanced machine learning models, the system not only reduces the time SEs spend on each case but also enhances the overall quality of customer support. The novel approaches presented are shown to outperform existing models and methods, providing a strong foundation for further research and development in the intersection of NLP and customer support.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary provides a comprehensive overview similar to Summary 3 but lacks the structured sections.\nIt effectively covers the advancements in reducing case resolution times and highlights the novel integration of summarization models using Bert and GPT-2.\nIt mentions the utilization of historical case data and automated SOC generation but in a slightly less detailed manner compared to Summary 3.\nIt concludes with the promising results and potential for further research, aligning well with the content of the original paper.", "expGem": "This summary is concise and addresses the main points but lacks the depth and detail found in Summaries 1 and 3.\nIt briefly mentions the use of NLP techniques to improve efficiency without elaborating on specific innovations like Bert-GPT-2 integration or multi-task learning.\nThe conclusion emphasizes the improvement in efficiency but doesn't provide as much insight into the specific advancements or methodologies employed.\nIt provides an overview but does not delve deeply into the technical innovations or their implications, making it less informative compared to the other summaries.", "expGPT4": "This summary is structured well with clear sections (Introduction, Novelty, Conclusion), which mirrors typical scientific paper organization.\nIt effectively highlights the advancements made by AWS Support and the novel technological solutions introduced to expedite support processes.\nThe novelty section details specific innovations such as the integration of NLP models (Bert encoder, GPT-2 decoder), utilization of historical case data, automated SOC generation, and multi-task learning.\nIt concludes by summarizing the effectiveness of these innovations in improving efficiency and support quality."}, "16": {"title": "Representation of Linguistic Form and Function in Recurrent Neural Networks", "url": "https://aclanthology.org/J17-4003", "text": "1. Introduction\nRecurrent neural networks (RNNs) were introduced by Elman (1990) as a connectionist architecture with the ability to model the temporal dimension. They have proved\npopular for modeling language data as they learn representations of words and larger\nlinguistic units directly from the input data, without feature engineering. Variations\nof the RNN architecture have been applied in several NLP domains such as parsing\n(Vinyals et al. 2015) and machine translation (Bahdanau, Cho, and Bengio 2015), as well\n\u2217 Tilburg Center for Cognition and Communication, Tilburg University, 5000 LE Tilburg, The Netherlands,\nE-mail: {a.kadar, g.chrupala, a.alishahi}@uvt.nl.\nSubmission received: 21 July 2016; revised version received: 5 June 2017; accepted for publication: 7 July 2017.\ndoi:10.1162/COLI a 00300\n\u00a9 2017 Association for Computational Linguistics\nPublished under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International\n(CC BY-NC-ND 4.0) license\nComputational Linguistics Volume 43, Number 4\nas in computer vision applications such as image generation (Gregor et al. 2015) and\nobject segmentation (Visin et al. 2016). RNNs are also important components of systems\nintegrating vision and language\u2014for example, image (Karpathy and Fei-Fei 2015) and\nvideo captioning (Yu et al. 2015).\nThese networks can represent variable-length linguistic expressions by encoding\nthem into a fixed-size low-dimensional vector. The nature and the role of the components of these representations are not directly interpretable as they are a complex, nonlinear function of the input. There have recently been numerous efforts to visualize deep\nmodels such as convolutional neural networks in the domain of computer vision, but\nmuch less so for variants of RNNs and for language processing.\nThe present article develops novel methods for uncovering abstract linguistic\nknowledge encoded by the distributed representations of RNNs, with a specific focus\non analyzing the hidden activation patterns rather than word embeddings and on the\nsyntactic generalizations that models learn to capture. In the current work we apply our\nmethods to a specific architecture trained on specific tasks, but also provide pointers\nabout how to generalize the proposed analysis to other settings.\nAs our case study we picked the IMAGINET model introduced by Chrupa\u0142a, Kad\u00b4 ar, \u00b4\nand Alishahi (2015). It is a multi-task, multi-modal architecture consisting of two gatedrecurrent unit (GRU) (Cho et al. 2014; Chung et al. 2014) pathways and a shared word\nembedding matrix. One of the GRUs (VISUAL) is trained to predict image vectors given\nimage descriptions, and the other pathway (TEXTUAL) is a language model, trained to\nsequentially predict each word in the descriptions. This particular architecture allows\na comparative analysis of the hidden activation patterns between networks trained\non two different tasks, while keeping the training data and the word embeddings\nfixed. Recurrent neural language models akin to TEXTUAL, which are trained to predict\nthe next symbol in a sequence, are relatively well understood, and there have been\nsome attempts to analyze their internal states (Elman 1991; Karpathy, Johnson, and Li\n2016, among others). In constrast, VISUAL maps a complete sequence of words to a\nrepresentation of a corresponding visual scene and is a less commonly encountered, but\nmore interesting, model from the point of view of representing meaning conveyed via\nlinguistic structure. For comparison, we also consider a standard standalone language\nmodel.\nWe report a thorough quantitative analysis to provide a linguistic interpretation\nof the networks\u2019 activation patterns. We present a series of experiments using a novel\nmethod we call omission score to measure the importance of input tokens to the\nfinal prediction of models that compute distributed representations of sentences. Furthermore, we introduce a more global measure for estimating the informativeness of\nvarious types of n-gram contexts for each model. These techniques can be applied to\nvarious RNN architectures such as recursive neural networks and convolutional neural\nnetworks.\nOur experiments show that the VISUAL pathway in general pays special attention\nto syntactic categories that carry semantic content, and particularly to nouns. More\nsurprisingly, this pathway also learns to treat word types differently depending on their\ngrammatical function and their position in the sequential structure of the sentence.\nIn contrast, the TEXTUAL pathway and the standalone language model are especially\nsensitive to the local syntactic characteristics of the input sentences. Further analysis of\nthe most informative n-gram contexts for each model shows that whereas the VISUAL\npathway is mostly sensitive to lexical (i.e., token n-gram) contexts, the language models\nreact more strongly to abstract contexts (i.e., dependency relation n-grams) that represent syntactic constructions.\n762\nKad\u00b4 ar, Chrupa\u0142a, and Alishahi Linguistic Form and Function in RNN\u2019s \u00b4\n2. Related Work\nThe direct predecessors of modern architectures were first proposed in the seminal\npaper by Elman (1990). He modifies the RNN architecture of Jordan (1986) by changing\nthe output-to-memory feedback connections to hidden-to-memory recurrence, enabling\nElman networks to represent arbitrary dynamic systems. Elman (1991) trains an RNN\non a small synthetic sentence data set and analyzes the activation patterns of the hidden\nlayer. His analysis shows that these distributed representations encode lexical categories, grammatical relations, and hierarchical constituent structures. Giles et al. (1991)\ntrain RNNs similar to Elman networks on strings generated by small deterministic\nregular grammars with the objective to recognize grammatical and reject ungrammatical strings, and develop the dynamic state partitioning technique to extract the learned\ngrammar from the networks in the form of deterministic finite state automatons.\nMore closely related is the recent work of Li et al. (2016a), who develop techniques\nfor a deeper understanding of the activation patterns of RNNs, but focus on models with\nmodern architectures trained on large scale data sets. More specifically, they train long\nshort-term memory networks (LSTMs) (Hochreiter and Schmidhuber 1997) for phraselevel sentiment analysis and present novel methods to explore the inner workings\nof RNNs. They measure the salience of tokens in sentences by taking the first-order\nderivatives of the loss with respect to the word embeddings and provide evidence that\nLSTMs can learn to attend to important tokens in sentences. Furthermore, they plot\nthe activation values of hidden units through time using heat maps and visualize local\nsemantic compositionality in RNNs. In comparison, the present work goes beyond the\nimportance of single words and focuses more on exploring structure learning in RNNs,\nas well as on developing methods for a comparative analysis between RNNs that are\nfocused on different modalities (language vs. vision).\nAdding an explicit attention mechanism that allows the RNNs to focus on different\nparts of the input was recently introduced by Bahdanau, Cho, and Bengio (2015) in the\ncontext of extending the sequence-to-sequence RNN architecture for neural machine\ntranslation. On the decoding side this neural module assigns weights to the hidden\nstates of the decoder, which allows the decoder to selectively pay varying degrees\nof attention to different phrases in the source sentence at different decoding timesteps. They also provide qualitative analysis by visualizing the attention weights and\nexploring the importance of the source encodings at various decoding steps. Similarly\nRocktaschel et al. (2016) use an attentive neural network architecture to perform natural \u00a8\nlanguage inference and visualize which parts of the hypotheses and premises the model\npays attention to when deciding on the entailment relationship. Conversely, the present\nwork focuses on RNNs without an explicit attention mechanism.\nKarpathy, Johnson, and Li (2016) also take up the challenge of rendering RNN\nactivation patterns understandable, but use character level language models and rather\nthan taking a linguistic point of view, focus on error analysis and training dynamics of\nLSTMs and GRUs. They show that certain dimensions in the RNN hidden activation\nvectors have specific and interpretable functions. Similarly, Li et al. (2016b) use a convolutional neural network (CNN) based on the architecture of Krizhevsky, Sutskever,\nand Hinton (2012), and train it on the ImageNet data set using different random initializations. For each layer in all networks they store the activation values produced\non the validation set of the ImageNet Large Scale Visual Recognition Competition\nand align similar neurons of different networks. They conclude that although some\nfeatures are learned across networks, some seem to depend on the initialization. Other\nworks on visualizing the role of individual hidden units in deep models for vision\n763\nComputational Linguistics Volume 43, Number 4\nsynthesize images by optimizing random images through backpropagation to maximize\nthe activity of units (Erhan et al. 2009; Simonyan, Vedaldi, and Zisserman 2014; Yosinski\net al. 2015; Nguyen, Yosinski, and Clune 2016) or to approximate the activation vectors\nof particular layers (Dosovitskiy and Brox 2015; Mahendran and Vedaldi 2016).\nWhile this paper was under review, a number of articles appeared that also investigate linguistic representations in LSTM architectures. In an approach similar to ours, Li,\nMonroe, and Jurafsky (2016) study the contribution of individual input tokens as well as\nhidden units and word embedding dimensions by erasing them from the representation\nand analyzing how this affects the model. They focus on text-only tasks and do not take\nother modalities such as visual input into account. Adi et al. (2017) take an alternative\napproach by introducing prediction tasks to analyze information encoded in sentence\nembeddings about sentence length, sentence content, and word order. Finally, Linzen,\nDupoux, and Goldberg (2016) examine the acquisition of long-distance dependencies\nthrough the study of number agreement in different variations of an LSTM model\nwith different objectives (number prediction, grammaticality judgment, and language\nmodeling). Their results show that such dependencies can be captured with very high\naccuracy when the model receives a strong supervision signal (i.e., whether the subject\nis plural or singular), but simple language models still capture the majority of test cases.\nWhereas they focus on an in-depth analysis of a single phenomenon, in our work we\nare interested in methods that make it possible to uncover a broad variety of patterns of\nbehavior in RNNs.\nIn general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters\nand pixel saliencies (Simonyan, Vedaldi, and Zisserman 2014; Mahendran and Vedaldi\n2015; Yosinski et al. 2015). These techniques have also led to improvements in model\nperformance (Eigen et al. 2014) and transferability of features (Zhou et al. 2015). To date\nthere has been much less work on such issues within computational linguistics. We aim\nto fill this gap by adapting existing methods as well as developing novel techniques to\nexplore the linguistic structure learned by recurrent networks.\n3. Models\nIn our analyses of the acquired linguist knowledge, we apply our methods to the\nfollowing models:\nr\nIMAGINET: A multi-modal GRU network consisting of two pathways,\nVISUAL and TEXTUAL, coupled via word embeddings.\nr LM: A (unimodal) language model consisting of a GRU network.\nr SUM: A network with the same objective as the VISUAL pathway of\nIMAGINET, but that uses sum of word embeddings instead of a GRU.\nThe rest of this section gives a detailed description of these models.\n3.1 Gated Recurrent Neural Networks\nOne of the main difficulties for training traditional Elman networks arises from the\nfact that they overwrite their hidden states at every time step with a new value computed from the current input xt and the previous hidden state ht\u22121\n. Similarly to LSTMs,\nGRU networks introduce a mechanism that facilitates the retention of information\n764\nKad\u00b4 ar, Chrupa\u0142a, and Alishahi Linguistic Form and Function in RNN\u2019s \u00b4\nover multiple time steps. Specifically, the GRU computes the hidden state at current\ntime step ht\n, as the linear combination of previous activation ht\u22121\n, and a new candidate\nactivation h\u02dc\nt\n:\nGRU(ht\u22121\n, xt\n) = (1 \u2212 zt\n) \f ht\u22121 + zt \f h\u02dc\nt\n(1)\nwhere \f is elementwise multiplication, and the update gate activation zt determines the\namount of new information mixed in the current state:\nzt = \u03c3s\n(Wzxt + Uzht\u22121\n) (2)\nThe candidate activation is computed as:\nh\u02dc\nt = \u03c3(Wxt + U(rt \f ht\u22121\n)) (3)\nThe reset gate rt determines how much of the current input xt\nis mixed in the previous\nstate ht\u22121\nto form the candidate activation:\nrt = \u03c3s\n(Wrxt + Urht\u22121\n) (4)\nwhere W, U, Wz\n, Uz\n, Wr and Ur are learnable parameters.\n3.2 Imaginet\nIMAGINET, introduced in Chrupa\u0142a, Kad\u00b4 ar, and Alishahi (2015), is a multi-modal GRU \u00b4\nnetwork architecture that learns visually grounded meaning representations from textual and visual input. It acquires linguistic knowledge through language comprehension, by receiving a description of a scene and trying to visualize it through predicting a\nvisual representation for the textual description, while concurrently predicting the next\nword in the sequence.\nFigure 1 shows the structure of IMAGINET. As can be seen from the figure, the model\nconsists of two GRU pathways, TEXTUAL and VISUAL, with a shared word embedding\nmatrix. The inputs to the model are pairs of image descriptions and their corresponding\nimages. The TEXTUAL pathway predicts the next word at each position in the sequence\nof words in each caption, whereas the VISUAL pathway predicts a visual representation\nof the image that depicts the scene described by the caption after the final word is\nreceived.\nFigure 1\nStructure of IMAGINET, adapted from Chrupa\u0142a, Kad\u00b4 ar, and Alishahi (2015). \u00b4\n765\nComputational Linguistics Volume 43, Number 4\nFormally, each sentence is mapped to two sequences of hidden states, one by\nVISUAL and the other by TEXTUAL:\nh\nV\nt = GRUV\n(h\nV\nt\u22121\n, xt\n) (5)\nh\nT\nt = GRUT\n(h\nT\nt\u22121\n, xt\n) (6)\nAt each time step TEXTUAL predicts the next word in the sentence S from its current\nhidden state h\nT\nt\n, and VISUAL predicts the image-vector1 \u02c6i from its last hidden representation h\nV\nt\n.\n\u02c6i = VhV\n\u03c4\n(7)\np(St+1\n|S1:t\n) = softmax(LhT\nt\n) (8)\nThe loss function is a multi-task objective that penalizes error on the visual and the\ntextual targets simultaneously. The objective combines cross-entropy loss L\nT\nfor the\nword predictions and cosine distance L\nV for the image predictions,2 weighting them\nwith the parameter \u03b1 (set to 0.1).\nL\nT\n(\u03b8) = \u2212\n1\n\u03c4\nX\u03c4\nt=1\nlog p(St\n|S1:t\n) (9)\nL\nV\n(\u03b8) = 1 \u2212\n\u02c6i \u00b7 i\nk\u02c6ikkik\n(10)\nL = \u03b1L\nT + (1 \u2212 \u03b1)L\nV\n(11)\nFor more details about the IMAGINET model and its performance, see Chrupa\u0142a, Kad\u00b4 ar, \u00b4\nand Alishahi (2015). Note that we introduce a small change in the image representation: We observe that using standardized image vectors, where each dimension is\ntransformed by subtracting the mean and dividing by standard deviation, improves\nperformance.\n3.3 Unimodal Language Model\nThe model LM is a language model analogous to the TEXTUAL pathway of IMAGINET\nwith the difference that its word embeddings are not shared, and its loss function is the\ncross-entropy on word prediction. Using this model we remove the visual objective as\na factor, as the model does not use the images corresponding to captions in any way.\n3.4 Sum of Word Embeddings\nThe model SUM is a stripped-down version of the VISUAL pathway, which does not\nshare word embeddings, only uses the cosine loss function, and replaces the GRU\nnetwork with a summation over word embeddings. This removes the effect of word\n1 Representing the full image, extracted from the pre-trained CNN of Simonyan and Zisserman (2015).\n2 Note that the original formulation in Chrupa\u0142a, Kad\u00b4 ar, and Alishahi (2015) uses mean squared error \u00b4\ninstead; as the performance of VISUAL is measured on image-retrieval (which is based on cosine\ndistances) we use cosine distance as the visual loss here.\n766\nKad\u00b4 ar, Chrupa\u0142a, and Alishahi Linguistic Form and Function in RNN\u2019s \u00b4\norder from consideration. We use this model as a baseline in the sections that focus on\nlanguage structure.\n5. Discussion\nThe goal of our article is to propose novel methods for the analysis of the encoding of\nlinguistic knowledge in RNNs trained on language tasks. We focused on developing\nquantitative methods to measure the importance of different kinds of words for the\nperformance of such models. Furthermore, we proposed techniques to explore what\nkinds of linguistic features the models learn to exploit beyond lexical cues.\nUsing the IMAGINET model as our case study, our analyses of the hidden activation\npatterns show that the VISUAL model learns an abstract representation of the information structure of a single sentence in the language, and pays selective attention to lexical\ncategories and grammatical functions that carry semantic information. In contrast, the\nlanguage model TEXTUAL is sensitive to features of a more syntactic nature. We have\nalso shown that each network contains specialized units that are tuned to both lexical\nand structural patterns that are useful for the task at hand.\n5.1 Generalizing to Other Architectures\nFor other RNN architectures such as LSTMs and their bi-directional variants, measuring\nthe contribution of tokens to their predictions (or the omission scores) can be straightforwardly computed using their hidden state at the last time step used for prediction.\nFurthermore, the technique can be applied in general to other architectures that map\nvariable-length linguistic expressions to the same fixed dimensional space and perform\npredictions based on these embeddings. This includes tree-structured RNN models\nsuch as the Tree-LSTM introduced in Tai, Socher, and Manning (2015), or the CNN\n777\nComputational Linguistics Volume 43, Number 4\narchitecture of Kim (2014) for sentence classification. However, the presented analysis\nand results regarding word positions can only be meaningful for RNNs as they compute\ntheir representations sequentially and are not limited by fixed window sizes.\nA limitation of the generalizability of our analysis is that in the case of bi-directional\narchitectures, the interpretation of the features extracted by the RNNs that process the\ninput tokens in the reversed order might be hard from a linguistic point of view.\n", "SumGPT3": "Introduction:\nThe paper discusses the analysis of linguistic knowledge encoded within Recurrent Neural Networks (RNNs), focusing particularly on the IMAGINET model. RNNs have gained popularity for their ability to model sequential data, but understanding the linguistic knowledge they encode remains a challenge.\n\nNovelty:\n\nQuantitative Analysis of Importance: The paper introduces quantitative methods to measure the importance of different words in RNN predictions, shedding light on their role beyond simple lexical cues.\nExploration of Linguistic Features: Novel techniques are proposed to explore the linguistic features learned by RNNs, going beyond traditional analyses of word embeddings to uncover deeper structural patterns.\nSpecialized Units: The analysis reveals specialized units within RNNs tuned to both lexical and structural patterns, providing insights into how these models capture linguistic information.\nConclusion:\nThe paper demonstrates that the IMAGINET model learns abstract representations of information structure in language, with the VISUAL pathway focusing on semantic information encoded in lexical categories and grammatical functions, while the TEXTUAL pathway is more sensitive to syntactic features. The findings suggest that these methods can be generalized to other RNN architectures and applied to tasks beyond language processing, providing valuable insights into the workings of deep learning models.", "SumGem": "Introduction\n\nThis paper investigates how Recurrent Neural Networks (RNNs) encode linguistic knowledge. The authors analyze the hidden activation patterns of these networks to understand what kind of linguistic information they capture. They focus on models trained on language tasks and propose methods to measure the importance of different aspects of the input for the model's performance.\n\nNovelty\n\nThe paper introduces several novel methods for analyzing RNNs:\n\nOmission Score: This metric measures the impact of removing a specific word from the input on the model's prediction. It helps identify how important each word is for the model's task.\nInformative n-gram contexts: This approach goes beyond analyzing single words and explores how the model utilizes sequences of words (n-grams) to perform its task. It can reveal whether the model pays more attention to the lexical content of the words (i.e., word order) or the abstract syntactic structure of the sentence (i.e., grammatical relationships).\nComparative Analysis: The paper applies these methods to different RNN architectures, including a multi-modal architecture that processes both text and image data. This allows for a comparison of how different models handle linguistic information and how their focus might differ depending on the task.\nConclusion\n\nThe paper demonstrates that the proposed techniques can effectively analyze the linguistic knowledge encoded by RNNs. The experiments show that the models learn to exploit various linguistic features, including:\n\nLexical categories and grammatical functions (especially for models focused on semantic representation)\nWord order and syntactic structure (especially for language models)\nThese findings provide valuable insights into how RNNs process language and what kind of linguistic information they are capable of learning. The approach can be generalized to analyze other RNN architectures and tasks, promoting a deeper understanding of how these models work.", "SumGPT4": "### Introduction\nThe scientific paper discusses advancements in understanding and analyzing Recurrent Neural Networks (RNNs), specifically focusing on the encoding of linguistic knowledge and the visualization of activation patterns within these networks. It particularly examines a dual-pathway GRU-based architecture, IMAGINET, designed for both textual and visual tasks, to elucidate how RNNs manage and represent linguistic and semantic information.\n\n### Novelty\n1. **Dual-Task Analysis**: The paper introduces IMAGINET, a multi-modal, multi-task GRU network that allows comparative analysis of activation patterns between networks trained on language and visual representation tasks. This is a step forward in understanding how different modalities influence learning in neural networks.\n   \n2. **Omission Score Methodology**: A novel method called the omission score is used to determine the importance of individual input tokens on the network's predictions. This quantitative analysis provides insights into which aspects of the input are most crucial for the network\u2019s performance.\n   \n3. **N-gram Context Informativeness**: The research introduces a global measure for assessing the importance of various types of n-gram contexts (lexical vs. dependency relation n-grams) across different models. This helps in understanding how RNNs learn and prioritize different linguistic features.\n\n4. **Comparison with Traditional Models**: By contrasting the IMAGINET model with a standard language model and a simplified sum of word embeddings model, the study delineates how the inclusion of visual data impacts the representation learning in RNNs.\n\n5. **Specialization in Activation Patterns**: It is highlighted that specific units within the network specialize in recognizing patterns that are either structurally or lexically significant, which shows an advanced level of abstraction in learning linguistic structures.\n\n### Conclusion\nThe study provides groundbreaking insights into how RNNs encode linguistic and semantic information, particularly through the lens of a dual-task, multi-modal architecture. The methodologies introduced, such as omission scores and the analysis of n-gram context informativeness, offer new tools for deeper examination of neural networks. This contributes to a more nuanced understanding of the functional and representational capabilities of RNNs in handling complex, multi-modal data, marking a significant step in the field of computational linguistics and neural network research.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "offers a detailed summary of the paper, focusing on the analysis of linguistic knowledge encoded by RNNs. It introduces the IMAGINET model and discusses its dual-pathway architecture, novel methods for analysis (omission score and n-gram context informativeness), and findings related to how RNNs process language and semantic information. It lacks some specificity but still covers the main points effectively.", "expGem": "while providing a decent summary, is more fragmented and less structured compared to the other two. It touches on the main themes of the paper such as linguistic knowledge encoding in RNNs, the use of IMAGINET, and some methodologies like omission scores and n-gram analysis. However, it lacks depth in explaining the methodologies and findings in detail, making it less effective overall.", "expGPT4": "provides a comprehensive overview of the paper, covering key aspects such as the introduction of IMAGINET, novel methodologies like omission scores and n-gram context analysis, comparisons with traditional models, and insights into specialization in activation patterns within RNNs. It effectively captures the novelty, methodology, and findings of the paper in a structured and clear manner."}, "17": {"title": "Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem", "url": "https://aclanthology.org/2020.findings-emnlp.255", "text": "1 Introduction\nLearning general functional dependency between\narbitrary input and output spaces is one of the key\nchallenges in machine learning. While many efforts\nin machine learning have mainly focused on designing flexible and powerful input representations for\nsolving classification or regression problems, many\napplications require researchers to design novel\nmodels that can deal with complex structured inputs and outputs, such as graphs, trees, sequences,\nor sets. In this paper, we consider the general problem of learning a mapping between a graph input G \u2208 G and a tree output T \u2208 T , based on\na training sample of structured input-output pairs\n(G1, T1), ...,(Gn, Tn) \u2208 G \u00d7 T drawn from some\nfixed but unknown probability distribution.\nSuch learning problems often arise in a variety\nof applications, ranging from semantic parsing, to\n\u2217\nauthors contributed equally to this research.\nSP\nText Input:\nwhat jobs are there for web developer who know \u2019c++\u2019 ?\nStructured output:\nanswer( A , ( job ( A ) , title ( A , W ) , const ( W , \u2019Web\nDeveloper\u2019 ) , language ( A , C ) , const ( C , \u2019c++\u2019 ) ) )\nMWP\nText input:\n0.5 of the cows are grazing grass . 0.25 of the cows are sleeping and 9 cows are drinking water from the pond . find the\ntotal number of cows .\nStructured output:\n( ( 0.5 * x ) + ( 0.25 * x ) ) + 9.0 = x\nTable 1: Examples of structured input and output of semantic parsing (SP) and math word problem (MWP).\nFor inputs, we consider parsing tree augmented sequences to get structural information. For outputs, they\nare naturally a hierarchical structure with some structural meaning symbols like brackets.\nmath word problem, label sequence learning, and\nsupervised grammar learning, to name just a few.\nAs shown in Fig. 1, finding the parse tree of a\nsentence involves a structural dependency among\nthe labels in the parse tree; generating a mathematical expression of a math word problem involves\na hierarchical dependency between math logical\noperations and the numbers. Conventionally, there\nhave been efforts in generalizing kernel methods to\npredict structured and inter-dependent variables in\na supervised learning setting (Tsochantaridis et al.,\n2005; Altun et al., 2004; Joachims et al., 2009).\nRecently, the celebrated Sequence-to-Sequence\ntechnique (Seq2Seq) and its numerous variants\n(Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) achieve excellent performance in\nneural machine translation. Encouraged by the success of Seq2Seq model, there is a surge of interests\nin applying Seq2Seq models to cope with other\ntasks such as developing neural semantic parser\n(Dong and Lapata, 2016) or solving math word\nproblem (Ling et al., 2017). However, the two\nsignificant challenges making a Seq2Seq model ineffective in these tasks are that, i) for the natural\n2842\ntext description input, it often entails some hidden\nsyntactic structure information such as dependency,\nconstituency tree or even semantic structure information like AMR parsing tree; ii) for the meaningful representation output, it typically contains\nabundant information in a structured object like a\nparsing tree or a mathematical equation.\nInspired by these observations, in this work, we\npropose a Graph-to-Tree neural networks, namely\nGraph2Tree consisting of a graph encoder and a\nhierarchical tree decoder, which leverages the structural information of both source graphs and target\ntrees. In particular, our Graph2Tree model learns\nthe mapping from a structured object such as a\ngraph to another structured object such as a tree. In\naddition, we also observe that the structured object\ntranslation typically follows a modular procedure,\nwhich translates the individual sub-graph in the\nsource graph into the corresponding target one in\ntarget tree output, and then compose them to form\nthe final target tree.\nTherefore, we design a workflow to align with\nthis procedure: our graph encoder first learns from\nan input graph that is constructed from the various\ninputs such as combining both a word sequence and\nthe corresponding dependency or constituency tree,\nand then our tree decoder generates the tree object\nfrom the learned graph vector representations to explicitly capture the compositional structure of a tree.\nIn particular, we present a novel Graph2tree model\nwith a separated attention mechanism to jointly\nlearn a final hidden vector of the corresponding\ngraph nodes in order to align the generation process between a heterogeneous graph input and a\nhierarchical tree output.\nTo demonstrate the effectiveness of our model,\nwe perform experiments on two important tasks \u2013\nSemantic Parsing and Math Word Problem. First,\nwe compare our approach against several neural\nnetwork approaches on the Semantic Parsing task.\nOur experimental results show that our Graph2Tree\nmodel could outperform or match the performance\nof other state-of-the-art models on three standard\nbenchmark datasets. Second, we further compare\nour approach with existing recently developed neural approaches on the math word problem and our\nresults clearly show that our Graph2Tree model\ncan achieve state-of-the-art performance compared\nto other baselines that use many task-specific techniques. We believe our Graph2Tree model is a\nsolid attempt for learning structured input-output\ntranslation.\n2 Related Works\n2.1 Graph Neural Networks\nThe graph representation learning recently attracted\na lot of attention and interest from both academia\nand industry. One of the most important research\nlines is the semantic embedding learning of graph\nnodes or edges based upon the power of graph\nneural networks (GNNs) (Li et al., 2016; Kipf and\nWelling, 2017; Velickovic et al., 2017; Gilmer et al.,\n2017; Hamilton et al., 2017).\nEncouraged by the recent success in GNNs, various Sequence-to-Graph (Peng et al., 2018) or\nGraph-to-Sequence models (Xu et al., 2018a,b,c;\nBeck et al., 2018; Chen et al., 2020) have been\nproposed to handle the structured inputs, structured\noutputs or both of them, i.e. generating AMR graph\ngeneration from the text sequence. More recently,\nsome researchers proposed the Tree-to-Tree (Chen\net al., 2018b), Graph-to-Tree (Yin et al., 2019) and\nGraph-to-Graph (Guo et al., 2018) neural networks\nfor targeted application scenarios.\nHowever, these works are designed exclusively\nfor specific downstream tasks like program translation or code edit. Compared to them, our proposed\nGraph2Tree neural network with novel design of\ngraph encoder and tree decoder does not rely on\nany specific downstream task assumption. Additionally, our Graph2Tree is the first generic neural\nnetwork translating graph inputs into tree outputs,\nwhich may have numerous applications in practice.\n2.2 Neural Semantic Parsing\nSemantic parsing is the task of translating natural language utterances into machine-interpretable\nmeaning representations like logical forms or SQL\nqueries. Recent years have witnessed a surge of interests in developing neural semantic parsers with\nsequence-to-sequence models. These parsers have\nachieved promising results (Jia and Liang, 2016;\nDong and Lapata, 2016; Ling et al., 2016). Due to\nthe fact that the meaning representations are usually structured objects (e.g. tree structures), many\nefforts have been devoted to develop structureoriented decoders, including tree decoders (Dong\nand Lapata, 2016; Alvarez-Melis and Jaakkola,\n2017), grammar constrained decoders (Yin and\nNeubig, 2017; Yin et al., 2018; Jie and Lu, 2018;\nDong and Lapata, 2018), action sequences for semantic graph generation (Chen et al., 2018a), and\n2843\nmodular decoders based on abstract syntax trees\n(Rabinovich et al., 2017). However, those approaches could potentially be further improved because they only consider the word sequence information and ignore other rich syntactic information,\nsuch as dependency or constituency tree, available\nat the encoder side.\nResearchers recently attempted to leverage of\nthe power of GNNs in various NLP tasks, including the neural machine translation (Bastings et al.,\n2017; Beck et al., 2018), conversational machine\nreading comprehension (Chen et al., 2019b), and\nAMR-to-text (Song et al., 2018). Specifically in\nthe semantic parsing field, a general Graph2Seq\nmodel (Xu et al., 2018b) is proposed to incorporate\nthese dependency and constituency trees with the\nword sequence and then create a syntactic graph as\nthe encoding input. However, this approach simply\ntreats a logical form as a sequence, neglecting the\nabundant information in a structured object like tree\nin the decoder architecture. Therefore, we present\nthe Graph2Tree model to utilize the structure information in both structured inputs and outputs.\n2.3 Math Word Problems\nThe math word problem is the task of translating\nthe short paragraph (typically consisting with multiple short sentences) into succinct mathematical\nequations. To solve a math word problem illustrated in Table 1, traditional approaches focus on\ngenerating numeric answer expressions by mapping verbs in problems text to categories (Hosseini\net al., 2014) or by generating templates from problem texts (Kushman et al., 2014). However, these\napproaches either need additional hand-crafted annotations for problem texts or are limited to a set\nof predefined equation templates.\nInspired by the great success of Seq2Seq models in Neural Machine Translation, deep-learning\nbased methods are intensively explored by researchers in the equation generation (Wang et al.,\n2017; Ling et al., 2017; Li et al., 2018, 2019; Zou\nand Lu, 2019; Xie and Sun, 2019). However, different forms of equations can be formed to solve\nthe same math problem, which often makes models fail. To resolve the equation duplication issues,\nvarious equation normalization methods are proposed in (Wang et al., 2018a, 2019) to generate a\nunique expression tree with the cost of losing the\nunderstanding of problem-solving steps in equation expressions. In contrast, we propose to use a\nGraph2Tree model to solve this task without any\nspecial mechanisms like equation normalization.\nTo the best of our knowledge, this is the first work\nto use GNN to build a math word problem solver.\n3 Problem Formulation and Structure\nObject Construction\n3.1 Graph-to-Tree Translation Task\nIn this work, we consider the problem of translating a graph input to a tree output. In particular, we consider two important tasks - Semantic Parsing and Math Word Problem. Formally,\nwe define both tasks as follows. The input side\ncontains a set of text sequences, denoted as S =\n{s1, s2, . . . , sn} \u2208 S where si\nis a text sequence\nconsisting of a sequence of word embeddings\nsi = {w1, w2, . . . , w|si|} \u2208 W, where W is a pretrained word embedding space. We then construct\na heterogeneous graph input G = (V, E) \u2208 G,\nwhere V = [V1 V2] contains all of the original\nword nodes V1 \u2208 V1 as well as the relationship\nnodes V2 \u2208 V2 from the relationships of a parsing\ntree (i.e. dependency or constituency tree), and\nE \u2208 E denotes if the two nodes are connected or\nnot. The aim is to translate a set of heterogeneous\ngraph inputs G = {g1, g2, . . . , gn} into a set of tree\noutputs T = {t1, t2, ...tn} \u2208 T where ti\nis a logic\nform or math equation consisting of a sequence of\ntree node token ti = {y1, y2, . . . , y|ti|} \u2208 Y.\n3.2 Constructing Graph Inputs and Tree\nOutputs\nTo apply GNNs, the first step is to construct a graph\ninput by combining the word sequence with their\ncorresponding hidden structure information. How\nto construct such graphs is critical to incorporate\nthe structured information and influences the final\nperformance. Similarly, how to construct the tree\noutputs from logic form or math equations also\nplay an important role in the final performance\nand model interpretability. In this section, we will\nintroduce two methods for graph construction and\none method for tree construction.\nare there ada jobs outside austin\nexpl\ncompound\nnsubj\ncase\nnmod\nDependency Feature Sentence Level Feature\nFigure 1: Dependency tree augmented text graph\n2844\nCombining Word Sequence with Dependency\nParse Tree. The dependency parse tree not only\nrepresents various grammatical relationships between pairs of text words, but also is shown to have\nan important role in transforming texts into logical forms (Reddy et al., 2016). Therefore, the first\nmethod integrates two types of features by adding\ndependency linkages between corresponding word\npairs in word sequence. Concretely, we transform\na dependency label into a node, which is linked\nrespectively with two word nodes with dependency\nrelationship. Figure 1 gives such an example of\nconstructed heterogeneous graph from a text.\nPP\nSQ\nare there ada jobs outside austin\nConstituency Feature Sentence Level Feature\nVBP EX\nND\nFW NNS IN NN\nNP\nP\nROOT\nNP\nLayer 1\nLayer 2\nLayer 3\nFigure 2: Constituency tree augmented text graph\nCombining Word Sequence with Constituency\nTree. The constituency tree contains the phrase\nstructure information which is also critical to describe the word relationships and has shown to provide useful information for translation (Gu et al. \u00af ,\n2018). Since the leaf nodes in the constituency\ntree are the word nodes in the text, this method\nmerges these nodes with the identical ones in the\nbi-directional word sequence chain to create the\nsyntactic graph. Figure 2 shows an example of\nconstructed heterogeneous graph input.\nSubtree Node Operator Node Operand Node\nS1 + 9.0 = x\nS2 S2\n0.5 * x 0.25 * x\nGraph\nEmbedding\n+\nStart Decoding Parent Feeding Sibling Feeding\nROOT\nFigure 3: A sample tree output in our decoding process\nfrom expression \u201d( ( 0.5 * x ) + ( 0.25 * x ) ) + 9.0 = x\u201d\nConstructing Tree Outputs. To effectively learn\nthe compositional nature of our structured outputs,\nwe need to firstly transform original outputs from\nlogic forms or math equations to tree structured\nobjects. Specifically, we follow the tree construction method in (Dong and Lapata, 2016), which\nis a top-down manner to generate tree-structured\noutputs. In original outputs containing structural\nmeaning symbols like brackets, we first extract subtree structures and replace these sub-tree structures\nwith sub-tree symbols. Then we grow branches\nfrom the generated sub-tree symbols until all hierarchical structures in the original sequence are\nprocessed. Figure 3 provides an example of constructed tree objects from mathematical expression.\n4 Graph2Tree Neural Networks\nWe aim to learn a mapping that translates a heterogeneous graph-structured input G and its corresponding tree-structured outputs T. We illustrate\nthe workflow of our proposed Graph2Tree model\nfor semantic parsing in Figure 4, and present each\ncomponent of the model as follows.\n4.1 Graph Encoder\nTo effectively learn graph representations from our\nconstructed heterogeneous text graph, we present a\nnovel bidirectional graph node embeddings method\n- BiGraphSAGE. The proposed BiGraphSAGE extends the widely used GraphSAGE (Hamilton et al.,\n2017) by learning forward and backward node embeddings of a graph G in an interleaved fashion.\nIn particular, consider a word node v \u2208 V1\nwith pretrained word embedding wv like GloVe\n(Pennington et al., 2014) as v\u2019s initial attributes.\nWe then generate the contextualized node embeddings av for all nodes v \u2208 V1 using Bi-directional\nLong Short Term Memory (BiLSTM) (Graves et al.,\n2013). For a relationship node v \u2208 V2, we initialize av with randomized embeddings. These feature vectors are used as initial node embeddings\nh\n0\nv = av. Then each node embedding learns its\nvector representation by aggregating information\nfrom a node local neighborhood within K hops of\nthe graph.\nh\nk\nN`(v) = M\nk\n`({h\nk\u22121\nu` , \u2200u \u2208 N`(v)}) (1)\nh\nk\nNa(v) = M\nk\na({h\nk\u22121\nua , \u2200u \u2208 Na(v)}) (2)\nwhere k \u2208 {1, ..., K} is the iteration index and N\nis the neighborhood function of node v. Mk\n`\nand\nMk\na\nare the forward and backward aggregator functions. Node v\u2019s forward (backward) representation\nh\nk\nv`\n(h\nk\nva\n) aggregates the information of nodes in\nN`(v) (Na(v)).\n2845\nDependency :\n{u'dep': u'compound', u'jobs', u'ada'},\n{u'dep': u'nsubj', u'are', u'jobs'},\n{u'dep': u'case', u'austin', u'outside'},\n\u2026\u2026\u2026\u2026\nX\"\nX#\n.\n.\n.\n.\nX$\nz\"\n. . .\nFC Layer\nPooling\nGraph\nEmbedding\nGraph Encoder\nTree Decoder\nBi-directional\nNode Embedding\nConstruct Graph\nare\nthere\nada jobs outside\naustin\nX\"\nX#\n.\n.\n.\n.\n.\nX$\n\ufffd'\u22a3\n)\nX\"\nX#\n.\n.\n.\n.\n.\nX$\n\ufffd'\u22a2\n)\nLSTM\nlanguage\nLSTM <n>\nLSTM\n,\nLSTM\njobs\nLSTM\nLSTM <n>\n,\nLSTM\n\nLSTM\nloc\nLSTM <n>\nLSTM\n</s>\nLSTM\nANS\nada\n</s>\nANS\n</s>\nANS\naustin\n</s>\nLSTM\n<n> Non-terminal\nSibling Feeding\nParent feeding\nStart decoding\nDecoder Unit\nz+ z+,# z$\nexpl\ncompound\nnsubj nmod\ncase\nHop 1\nHop 2\nHop 3\nInput : are there ada jobs outside austin\nParse\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\nX\"\nX#\n.\n.\n.\n.\nX$\n\ufffd\ufffd(\ufffd)\n\ufffd3\n\u2026 \ufffd43\n\u2026\n\ufffd\ufffd(\ufffd)\n. . .\n. . . . . .\n\ufffd3\n\u2225\nConcat\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nSeparate\nAttention\nFigure 4: Overall architecture of our Graph2Tree model. We use semantic parsing task as an example.\nConceptually, one can choose to keep these\nnode embeddings for each direction independently,\nwhich ignores interactions between two intermediate node embeddings during the training. Therefore, we fuse two intermediate unidirectional node\nembeddings at each hop as follows,\nh1 = h\nk\nN`(v)\n, h2 = h\nk\nNa(v)\n(3)\nh\nk\nN(v) = wg \f h1 + (1 \u2212 wg) \f h2, (4)\nwg = \u03c3(W~z[h1; h2; h1 \f h2; h1 \u2212 h2]) (5)\nwhere \f denotes component-wise multiplication,\n\u03c3 is a sigmoid function and wg is a gating vector.\nThe graph encoder learns node embeddings h\nk\nv\nby repeating the following process K times:\nh\nk\nv = \u03c3(Wk\n\u00b7 CONCAT(h\nk\u22121\nv , h\nk\nN(v))) (6)\nwhere Wk denotes weight matrices, \u03c3 is a nonlinearity function, K is maximum number of hops.\nThe final bi-directional node embeddings zv is\nchosen to concatenate the two unidirectional node\nembeddings at the last hop,\nzv = CONCAT(h\nK\nv`, h\nK\nva) (7)\ng = MAXPOOL(FC(z)). (8)\nAfter the bi-directional embeddings for all nodes\nz are computed, we then feed the obtained node\nembeddings into a fully-connected neural network\nand apply the element-wise max-pooling operation\non all node embeddings to compute the graph-level\nvector representation g, where other alternative\ncommutative operations such as mean or attention\nbased weighted sum can be used as well.\n4.2 Tree Decoder\nWe propose a new general tree decoder fully leveraging the outputs of our graph encoder, i.e. the bidirectional node embeddings and the graph embedding, and faithfully generating the tree-structured\ntargets like logic forms or math equations.\nInspired by the thinking paradigm of human beings, our tree decoder at high level uses a divideand-conquer strategy splitting the whole decoding\ntask into sub ones. Figure 3 illustrates an example\noutput of our tree decoder. In this example, we\nfirstly initialize the root tree node ROOT with the\ngraph embedding g, and then apply a sub-decoder\non the ROOT to generate a 1st-level coarse output\ncontaining a sub-tree node S1. This S1 is further\ndecoded with the similar sub-decoder to derive the\n2nd-level coarse output. This procedure is repeated\nto generate the 3rd-level output in which there is\nno sub-tree nodes. In this way, we get the whole\ntree output in a top-down manner.\nThis whole procedure can be summarized as follows: 1) initialize the root tree node with the graph\nembedding from our encoder and perform the first\nlevel decoding with our LSTM based sub-decoder;\n2) for each newly generated sub-tree node, a subdecoder is applied to derive the next level coarse\noutput; 3) repeat step 2 until there is no sub-tree\nnodes in the last level of tree structure.\n4.2.1 Sub-Decoder Design\nIn each of our sub-decoder task, the conditional\nprobability of the generated word at step t is calculated as follows:\n2846\np(yt|y<t, x) = fpredict(st) (9)\nwhere x denotes vectors of all input words, yt\nis the\npredicted output word at t, st\nis the decoder hidden\nstate at t, and fpredict is a non-linear function.\nThe key component of Eq. (9) is the computation of st\n. Conceptually, this value is calculated as\nst = fdecoder(yt\u22121,st\u22121), where fdecoder is usually\na RNN unit. We propose two improvements on top\nof it, parent feeding and sibling feeding, to feed\nmore information for decoding sub-tree nodes.\nParent feeding. For a sub-task in our tree decoding process, we aim to expand the sub-tree node in\nthe parent layer. Therefore, it is reasonable to take\nthe sub-tree node embedding sti\ninto consideration.\nTherefore, we add the sub-tree node embedding\nas part of our input at every time-step, in order to\ncapture the upper-layer information for decoding.\nSibling feeding. Besides the information from\nparent nodes, if two sub-tree nodes share the same\nparent node, then these two sub-tasks can also be related. Inspired by this observation, we employ the\nsibling feeding mechanism to feed the preceding\nsibling sentence embedding to the sub-task related\nto its closet neighbor sub-tree node. For example,\nimagine p1 is the parent node of c1, c2, and we feed\nboth embeddings of p1 and c1 when decoding c2.\nTherefore, our sub-decoder calculates the decoder hidden state st as follows:\nst = fdecoder(yt\u22121,st\u22121;stparent;stsibling) (10)\nwhere stparent stands for sub-tree node embedding\nfrom parent layer and stsibling is the sentence embedding of the closest preceding sibling. By fully\nutilizing the information from parent nodes and\nsibling nodes, our tree decoder can effectively generate target hierarchical outputs.\n4.3 Separate Attention Mechanism to Locate\nSource Sub-graph\nVarious attention mechanisms have been proposed\n(Bahdanau et al., 2014; Luong et al., 2015) to incorporate the hidden vectors of the inputs into account during the decoding processing. In particular, the context vector st depends on a set of bidirectional node representations of the source graph\n(z1,...,z|V |\n) to which the decoder locates the source\nsub-graph. Since our graph input is essentially\na heterogeneous graph with two different input\nsources (word nodes with relationship nodes of\na parsing tree), we propose to employ a separated\nattention mechanism over the node representations\ncorresponding to the different node types:\n\u03b1t(v) =\nexp(score(zv,st))\nexp(PV1\nk=1 score(zk,st))\n, \u2200v \u2208 V1 (11)\n\u03b2t(v) =\nexp(score(zv,st))\nexp(PV2\nk=1 score(zk,st))\n, \u2200v \u2208 V2 (12)\nwhere the score(\u00b7) function estimates the similarity\nof zv and st\n. Then, we compute the context vectors\ncv1 and cv2, respectively.\ncv1 =\nX\u03b1t(v)zv, \u2200v \u2208 V1 (13)\ncv2 =\nX\u03b2t(v)zv, \u2200v \u2208 V2 (14)\nWe concatenate the context vector cv1\n, context\nvector cv2\nand decoder hidden state st\nto compute\nthe final attention hidden state at this time step as:\ns\u02dct = tanh(Wc \u00b7 [cv1\n; cv2\n;st] + bc) (15)\nwhere Wc and bc are learnable parameters. The\nfinal context vector s\u02dct is further used for decoding\ntree structured outputs. The output probability distribution over a vocabulary at the current time step\nis calculated by:\np(yt|y1, y2, . . . , yt\u22121, g) = sof tmax(Wvs\u02dct + bv) (16)\nwhere Wv and bv are learnable parameters. Our\nmodel is then jointly trained to maximize the conditional log-probability of the target tree given a\nheterogeneous graph input g.\n6 Conclusion and Future Work\nWe presented a novel Graph2Tree model consisting of a graph encoder and a hierarchical tree decoder, for learning the translation between structured inputs and structured outputs. Studies on two\ntasks - Semantic Parsing and Math Word Problem\ndemonstrated our model consistently outperformed\nor matched the performance of the state-of-the-art.\nOur Graph2Tree model is generic and agnostic to\nthe downstream tasks and thus one of the future\nworks is to adapt it to the other NLP applications.\n", "SumGPT3": "Introduction:\n\nThe paper addresses the challenge of learning general functional dependencies between complex structured inputs and outputs in machine learning. It focuses on the task of mapping graph inputs to tree outputs, crucial for various applications like semantic parsing and math word problems.\n\nNovelty:\n\nGraph2Tree Model: The paper proposes a Graph2Tree neural network, consisting of a graph encoder and a hierarchical tree decoder. This model learns to translate structured inputs to structured outputs, effectively capturing the compositional structure of trees from graphs. Unlike previous approaches, Graph2Tree is not limited to specific downstream tasks and achieves state-of-the-art performance in tasks like semantic parsing and math word problems.\n\nBiGraphSAGE Encoder: The paper introduces BiGraphSAGE, a bidirectional graph node embeddings method. It extends GraphSAGE by learning forward and backward node embeddings in an interleaved fashion, effectively capturing information from a node's local neighborhood within a graph. This approach outperforms previous methods in learning graph representations.\n\nTree Decoder Design: The paper proposes a novel tree decoder that leverages the outputs of the graph encoder to generate tree-structured targets. This decoder utilizes a divide-and-conquer strategy, decoding the tree output in a top-down manner. It incorporates parent feeding and sibling feeding mechanisms to capture upper-layer and related information, respectively, improving the decoding process.\n\nSeparate Attention Mechanism: The paper introduces a separate attention mechanism to locate the source sub-graph during decoding. By considering different node types in the heterogeneous graph input, this mechanism enhances the model's ability to generate accurate tree outputs.\n\nConclusion:\n\nThe Graph2Tree model presented in the paper demonstrates promising results in learning structured input-output translation. Its novel components, including the BiGraphSAGE encoder, tree decoder design, and separate attention mechanism, contribute to its effectiveness in various NLP applications.", "SumGem": "This paper introduces a novel neural network model, Graph2Tree, for translating structured inputs to structured outputs. The model is specifically designed to address the challenges of learning such mappings, where both the input and output data have hierarchical structures.\n\nNovelty:\n\nGraph2Tree Architecture: The proposed Graph2Tree model consists of a graph encoder and a hierarchical tree decoder. The graph encoder leverages a novel bidirectional graph node embedding method, BiGraphSAGE, to capture information from both forward and backward node neighborhoods within the input graph. The tree decoder employs a divide-and-conquer strategy to generate the tree-structured output in a top-down manner. This decoder incorporates parent feeding and sibling feeding mechanisms to effectively utilize information from parent and sibling nodes during the decoding process.\nSeparate Attention Mechanism: The model employs a separate attention mechanism to locate the relevant source sub-graph in the input graph. This mechanism considers the different node types (word nodes and relationship nodes) present in the heterogeneous graph input.\nConclusion:\n\nThe paper demonstrates the effectiveness of Graph2Tree on two tasks: semantic parsing and math word problems. The model achieves state-of-the-art performance on both tasks, highlighting its ability to learn complex relationships between structured inputs and outputs. The generality of Graph2Tree paves the way for its application in various other NLP tasks involving structured data.", "SumGPT4": "**Introduction:**\nThe paper presents a comprehensive exploration of machine learning challenges involving the mapping of structured inputs to structured outputs, specifically focusing on graph-to-tree transformations. This study delves into applications such as semantic parsing and solving math word problems, where the complexity of inputs like graphs, trees, sequences, or sets necessitates innovative model designs. The authors introduce the Graph2Tree model, a novel neural network architecture aimed at addressing these sophisticated input-output mapping problems.\n\n**Novelty:**\nThe novelty of the paper lies in several key aspects of the Graph2Tree model:\n1. **Graph-to-Tree Architecture**: Unlike previous models that often handle simpler or less structured data, Graph2Tree is designed specifically to translate complex graph inputs into hierarchical tree outputs. This architecture is beneficial for tasks that involve deep structural dependencies.\n2. **BiGraphSAGE Encoder**: The model utilizes a novel graph encoder, BiGraphSAGE, which extends the GraphSAGE model by incorporating bidirectional node embeddings. This enhancement helps capture more nuanced relationships within the input graph by considering both forward and backward context, which is critical for generating accurate tree structures.\n3. **Hierarchical Tree Decoder with Modular Decoding**: The tree decoder in Graph2Tree employs a modular, divide-and-conquer strategy that mirrors human cognitive processes in breaking down complex problems into simpler sub-problems. This approach allows for more efficient and accurate tree generation.\n4. **Separate Attention Mechanism**: The model introduces a separate attention mechanism for different node types in the graph (word nodes and relationship nodes), allowing for more precise targeting of relevant parts of the input graph during the decoding phase. This specificity enhances the fidelity of the output structures.\n\n**Conclusion:**\nThe Graph2Tree model represents a significant advancement in the field of machine learning for structured data translation tasks. By innovatively addressing the translation from graphs to trees, the model not only demonstrates superior performance on benchmark datasets for semantic parsing and math word problems but also establishes a framework that could be adapted to a broader range of natural language processing tasks. This research opens up new possibilities for handling complex structured data in various applications, paving the way for further innovations in the domain.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary is ranked second because it also covers the essential elements of the paper but is slightly less structured compared to the best summary. It introduces the Graph2Tree model and its components, discusses its applications, and mentions its performance in semantic parsing and math word problems. While it effectively conveys the main points, it could benefit from a bit more clarity in connecting different sections and ensuring a smoother transition between the introduction and conclusion.", "expGem": "This summary is ranked the lowest primarily due to its lack of clarity and organization. While it mentions key aspects like the graph-to-tree architecture, BiGraphSAGE encoder, hierarchical tree decoder, and separate attention mechanism, the presentation feels disjointed and repetitive. It doesn't flow as smoothly from one idea to the next compared to the other summaries. Additionally, some parts are overly technical without enough context or explanation, making it harder to follow for someone unfamiliar with the topic.", "expGPT4": "This summary is ranked the highest because it provides a clear and structured overview of the paper. It effectively introduces the problem statement (mapping structured inputs to structured outputs), highlights the significance of the Graph2Tree model for addressing this challenge, and succinctly describes the key novelties of the model. It covers essential aspects such as the architecture, the BiGraphSAGE encoder, hierarchical tree decoder, and separate attention mechanism, all while concluding with the model's demonstrated effectiveness across specific tasks. The language is precise, and the summary flows logically from introduction to conclusion."}, "18": {"title": "HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization", "url": "https://aclanthology.org/2022.coling-1.545", "text": "1 Introduction\nDocument summarization is one of the central\nproblems in NLP, which aims to rewrite a single\ndocument or multi documents under a shorter version with preserving the main information. There\nare two major approaches such as extractive and\n\u2217\ncorresponding author\n1\nhttps://github.com/tuananhphan97vn/HeterGraphLongSum\nabstractive summarization. Abstractive models\nare more sophisticated abilities that require wellcomprehensive reading text and generating highquality text. Specifically, most of the existing architectures have been built based on sequence-tosequence (Seq2Seq) techniques in different ways\nsuch as Recurrent Neural Network (RNN) (Nallapati et al., 2017), Pointer-Generator-Network(See\net al., 2017), or Transformer-based models(Zhang\net al., 2020; Xiao and Carenini, 2020). Furthermore, the external information, for instance, pretrained model BERTSum(Liu and Lapata, 2019)\nand topic modeling (Wang et al., 2020b; Nguyen\net al., 2021)) are incorporated to improve performances. Nevertheless, this approach requires a\ncomplicated neural network that consists of millions of learnable parameters, which is the cause\nof raising significant costs in both terms of computation time perplexity and resources. Therefore,\nextractive models still gain much attention. Particularly, extractive document summarization (EDS)\ntakes a document in the form list of sentences and\nchooses several best candidates from the original\ndocument, then combine them to create the summarization. Recent models trend to turn EDS into\nthe sequential binary-labeling task (Nallapati et al.,\n2017; Cheng and Lapata, 2016; Zhou et al., 2018).\nGraph neural network (GNN) has recently been\nexploited as an emerging line of deep learning architectures, which has powered various domains, including NLP tasks (Vashishth et al.,\n2020). Specifically, GNN models are able to model\ncomplex structural data containing semantic units\n(node) with relationships (edge) between them (Xu\net al., 2019). For the EDS task, each document\nis represented as a graph structure in which the\nnodes are the semantic units of the document such\nas words and sentences. Sequentially, developing\nedges among sentence nodes are capable to model\nthe cross-sentence relations, which is able to handle\nthe limitation of traditional Seq2Seq-based meth-\n6249\nFigure 1: Overview of HeterGraphLongSum model. Passages of each document are defined as a set of sentences in\nsequence with a fixed number of sentences. In this architecture, the edges from passage to word and sentence to\npassage are not taken into account because of the redundancy.\nods in terms of long-distance dependency among\nsentences (Cui et al., 2020). In particular, crosssentence edges can be constructed explicitly between sentences (sentence-to-sentence) (Xiao and\nCarenini, 2019; Jing et al., 2021; Yasunaga et al.,\n2017) or through intermediate bridge via common words (sentence-word-sentence)(Wang et al.,\n2020a) or latent topics (sentence-topic-sentence)\n(Cui et al., 2020).\nAlthough the aforementioned approaches have\nachieved remarkable results in the EDS problem,\nmost of the architectures are proposed for short\ndocuments (i.e., new articles). Long-form document is still a remaining challenge in this research\nfield due to two main reasons: i) most traditional\nSeq2Seq methods truncate longer documents into\nsmall fixed-length sequences (i.e., passages) (Zaheer et al., 2020; Zhang et al., 2021), which leads to\ninformation loss problem, especially for the extractive summarization; ii) using GNN-based methods\nis able to mitigate the information loss by enabling\ncross-sentence relations, however, representing an\neffective way for long-text documents into graph\nstructure is still an open research issue. Specifically, since the vocabulary size is limited, when\nthe length of the document is increased, more sentences become neighbors with each other (via common words) which is the cause of the similar embedding between sentences. Therefore, a graph\nstructure, which includes only word nodes and sentence nodes, might not be an effective way to represent the long documents for the EDS problem.\nIn order to alleviate the aforementioned challenges, this paper presents a new graph-based architecture, which contains three semantic units such\nas word, sentence, and passage. In particular, the\npassage nodes are adopted for learning the crossrelations between sentences in different passages.\nFurthermore, the passage node can be regarded as\nthe local structure of a group of sentence nodes in\nwhich the edges between passages and sentences\nhave the possibility to reduce the harm of similar representations of sentences when expanding\ngraph structure with long documents. Figure 1\nillustrates the model architecture of HeterGraphLongSum. Specifically, the main contributions of\nthis paper are threefold as follows:\n\u2022 We present a novel GNN-based method for\nmodeling long-form documents. Specifically,\ninstead of using common methods for learning long documents with the hierarchical perspective (e.g., word-to-sentence-to-passage),\nwe consider passage as one of the node types,\nwhich is updated simultaneously with other\nnodes in the graph. In this regard, more semantic units (additional nodes) in the graph enable the capability to enrich the cross-relations\nbetween elements (e.g., sentence representa-\n6250\ntion).\n\u2022 We propose a new Heterogeneous GNN (HeterGNN) model for the EDS task, focusing\non long documents (e.g., scientific papers).\nEspecially, we consider this issue without employing pre-trained encoders (e.g., BERT). In\nthis regard, our method is able to extend to\nother low-resource languages without any obstacles.\n\u2022 We evaluate the proposed model with two\nbenchmark long document datasets such as\nPubMed and ArXiv. The experiential results\nindicate that our method is able to achieve the\nstate-of-the-art level in this research field.\n2 Related Work\n2.1 Neural Extractive Summarization\nNeural networks have achieved great success in\nextractive summarization, which explores different neural components to develop an end-to-end\nlearning model (Zhong et al., 2019). The encoderdecoder frameworks are mainly developed by using RNN (Cheng and Lapata, 2016; Nallapati et al.,\n2017; Zhou et al., 2018) and Transformer (Zhang\net al., 2020; Xiao and Carenini, 2020) with autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or non auto-regressive (Narayan et al.,\n2018; Arumae and Liu, 2018) decoder. Sequentially, recent remarkable results are mainly developed by using pre-trained language models (e.g.,\nBERT (Devlin et al., 2019)) such as BERTSUM\n(Liu and Lapata, 2019) and MATCHSUM (Zhong\net al., 2020). Most of the aforementioned studies formulate the EDS task as sentence labeling\nor sentence ranking problems. In this paper, we\nformulated this task as the binary-labeling problem\n(Nallapati et al., 2017) and exploited our model\nwith a non-pre-trained CNN/BiLSTM encoder in\nwhich we believe that this method is able to easily\nextend to other low resource languages.\n2.2 Graph-based Summarization\nEarly works on graph-based methods for EDS tasks\nrely on the similarity scores between sentences in\nunsupervised manners such as TextRank (Mihalcea\nand Tarau, 2004) and LexRank (Erkan and Radev,\n2004). The core idea of using graph representation is to utilize the linguistic information of sentences. Consequentially, GNNs have been adopted\nfor learning cross-sentence relations with remarkable performances, using the concept of discourse\ngraph (Yasunaga et al., 2017; Xu et al., 2020). Recently, the trend research focuses on representing\ndocuments with different types of nodes (heterogeneous graphs) to utilize the effects of additional\nsemantic units such as words, sentences (Wang\net al., 2020a; Jin et al., 2020) and latent topics (Cui\net al., 2020). In this study, the proposed model exploits the heterogeneous graph structure with more\ncomplex units by adding semantic passage nodes\nto leverage the problems of adopting graph-based\nmodels in long document summarization.\n2.3 Long Document Summarization\nLong document summarization has recently received increased attention since the remained challenge of modeling long texts (Frermann and Klementiev, 2019). Specifically, the current potential\nsolution for this issue is to truncate documents into\nsmall fixed-length sequences and use sliding window methods to process the document separately\n(Beltagy et al., 2020; Zaheer et al., 2020). However, this paradigm leads the serious information\nloss, which is not suitable for the EDS task, due\nto this task requiring the information relations of\nextracted sentences (Li et al., 2020). In this regard, several promising approaches have been introduced for long document summarization. Cohan\net al. (2018) presents a hierarchical encoder to capture the discourse structure of the input document\nwith a discourse-aware decoder for abstractive summarization. Xiao and Carenini (2019) leverages\nthe long text summarization task by incorporating a distributed representation of both the global\n(whole document) and local (section/topic) contexts. Cui and Hu (2021) employs a dynamic memory network with sliding multiple windows to mitigate the information loss between segments of\nsentences. Regarding the graph-based methods,\nCui et al. (2020) adopts a modified graph attention network (GAT) for capturing inter-sentence\nrelationship. Furthermore, latent topics are added\nas an additional type of node, which incorporates\nsentence nodes to improve the performance of long\ndocument summarization in terms of capturing the\nrelational information of long-distance sentences.\nIn this study, we take the graph-based structure\nfor EDS of long text into account with a different perspective by considering word nodes and\nsentence nodes for capturing both inter and intra-\n6251\nsentence relations. Moreover, passage nodes are\njointly trained to improve the performance of long\ntexts by learning the cross-relations of sentences\nwith long-distance and mitigating the similar representation problem in the large-scale graph structure.\n3 HeterGraphLongSum model\nHeterGraphLongSum aims to learn a heterogeneous graph structure for long text summarization.\nSpecifically, we model an input document with\nthree types of nodes such as word, sentence, and\npassage nodes, as a heterogeneous graph, and using\ngraph attention network (GAT) (Velickovic et al.,\n2017) for capturing information relations among\nnodes.\n3.1 Graph Construction\nLet G = {V, E} represent an arbitrary graph,\nwhere V and E denote the node and edge sets,\nrespectively. Specifically, as shown in the Figure\n1, our directed graph can be defined as V = {Vw \u222a\nVs \u222a Vp} and E = {Ew2s \u222a Es2w \u222a Ew2p \u222a Ep2s},\nwhere Vw, Vs, and Vp stand for three semantic\nunits of a document (i.e., word, sentence, and\npassage), and Ew2s, Es2w, Ew2p, and Ep2s stand\nfor four types of edges such as word-to-sentence,\nsentence-to-word, word-to-passage, and passageto-sentence, respectively. Accordingly, the proposed heterogeneous graph structure is designed\nbased on two assumptions as follows:\n\u2022 The passage units are not available on most\npublicity datasets in this research field. Therefore, following the previous works on longform document representations(Zaheer et al.,\n2020; Zhang et al., 2021), we format the passages in form of a sequence of sentences and\ncreated them by concatenating a fixed size\nwith n sentences. In this regard, the number\nof sentences for each passage is a hyperparameter, which is tuned during the validation\nprocess.\n\u2022 Regarding the certain edge types, instead of\nadopting the full connection between semantic units, only four types of edges are taken\ninto account such as word-to-sentence (w2s),\nsentence-to-word (s2w), word-to-passage\n(w2p), and passage-to-sentence (p2s). Accordingly, the edge from passage-to-word\n(p2w) and sentence-to-passage (s2p) are not\nconsidered because of the redundancy. Specifically, p2w is not considered since many\nwords receive the same information (i.e., from\nthe passage), which might harm the overall\nperformance. Furthermore, there are two\ntypes of edge to update the passage information such as w2p and s2p. In this regard, we design w2p in our graph structure\nto enable the cross-passage relations via path\npassage \u2192 sentence \u2192 word \u2192 passage\nand remove the s2p edge type. We prove this\nassumption via the ablation study in the experiment section.\nIntuitively, by adding two types of edges from\npassage nodes, cross-sentences relations can be\nsimultaneously processed in two ways: i) local\ninformation with path sentence \u2192 word \u2192\npassage \u2192 sentence; ii) global information with\npath sentence \u2192 word \u2192 sentence. The additional local information enables the model to\nmitigate the problem of similarity representation\nbetween sentences when the graph structure is expanded by adding passage information. Specifically, this issue is specific to sentences located in\ndifferent positions in the document, which is especially suitable for learning long documents.\n3.2 Graph Encoder Embedding\nSupporting the matrix features of word node, sentence node and passage node are sequentially denoted as Xw \u2208 R|Vw|\u00d7dw , Xs \u2208 R|Vs|\u00d7ds\n, and\nXp \u2208 R|Vp|\u00d7dp\n, respectively. The initialized embedding representation of the word node is encoded\nby using Glove (Pennington et al., 2014). In the\ncase of sentences, instead of using pre-trained models, we combine Convolutional Neural Network\n(CNN) and bidirectional Long Short-Term Memory\n(BiLSTM) for the encoder, which can be formulated as follows:\n(Xs)j = CNN(x1:m) \u2295 BiLSTM(x1:m) (1)\nwhere m denotes the number of words in the sentence sj . In this regard, the Passage feature is\nencoded by using Bi-directional LSTM based on\nthe hidden state of sentences, which is extracted\nfrom the last layer network as follows:\n(Xp)\ni = BiLSTM \u0010\n(Xs)\n(j)\n\u0011\n(2)\nwhere (Xp)\ni\ndenotes embedding of the i-th passage\nnode, j and k are the j-th sentence and number of\n6252\nsentence per passage (k \u2217 i \u2264 j \u2264 k \u2217 (i + 1)),\nrespectively.\n3.3 Graph Learning Layer\nThe vectors of nodes are initialized with embedding features, where H0\ns = Xs H0\nw = Xw, and\nH0\np = Xp, respectively. Sequentially, the node representations are updated with the graph attention\nnetwork.\nGraph Attention Network: Given the heterogeneous graph structure and initialized features of\neach node, GAT is adopted to calculate the hidden\nstates of nodes. Specifically, supporting h\u20d7\ni \u2208 R\ndhi\nand Ni denote the input hidden representation and\nthe neighbors of node i-th, respectively, the graph\nattention layer can be calculated as follows:\nzij = LeakyRelu(\u20d7aT\n(Wqh\u20d7\ni\n||Wkh\u20d7\nj ))\n\u03b1ij =\ne\nzij\nP\nk\u2208Ni\ne\nzik\n\u20d7h\n\u2032\ni = \u03c3\n\uf8eb\n\uf8ed\nX\nj\u2208Ni\n\u03b1ijWvh\u20d7\nj\n\uf8f6\n\uf8f8\n(3)\nwhere Wq, Wk, Wv, and \u20d7\u03b1 are learnable parameters and optimized during the training process.\nThe symbol || indicates the concatenation operator. \u03c3 denotes the non-linear transform function\nand \u20d7h\n\u2032\ni\ndenotes the hidden state which presents information gained from the neighboring nodes. Alternatively, multi-head attention can be used for\nimproving the performance, which is calculated as\nfollows:\n\u20d7h\n\u2032\ni =\u2225\nK\nk=1 \u03c3\n\uf8eb\n\uf8ed\nX\nj\u2208Ni\n\u03b1ijWk\nv h\u20d7\nj\n\uf8f6\n\uf8f8 (4)\nFurthermore, in order to mitigate the gradient vanishing problem, the residual connection is added\nto the original representation to provide the final\nhidden state:\n\u20d7h\n\u2032\u2032\ni = h\u20d7\ni +\n\u20d7h\n\u2032\ni\n(5)\nGraph Propagation: After initialization, the sentence nodes are updated with their neighbor word\nnodes and passage nodes by using GAT and FFN\nlayer:\nU\n1\nw2s = GAT(H0\ns\n, H0\nw, H0\nw)\nU\n1\np2s = GAT(H0\ns\n, H0\np\n, H0\np\n)\nU\n1\ns = \u03c3(U\n1\nw2s + U\n1\np2s\n)\nH1\ns = F F N(U\n1\ns + H0\ns\n)\n(6)\nSequentially, word nodes are updated with the new\nrepresentation of sentences. Similarly, the passage\nnodes are updated by the updated word embedding.\nThe updated process at an iteration of GAT can be\nformulated as follows:\nU\nt\nw2s = GAT(Ht\u22121\ns\n, Ht\u22121\nw , Ht\u22121\nw )\nU\nt\np2s = GAT(Ht\u22121\ns\n, Ht\u22121\np\n, Ht\u22121\np\n)\nU\nt\ns = \u03c3(U\nt\nw2s + U\nt\np2s\n)\nHt\ns = F F N(U\nt\ns + Ht\u22121\ns\n)\nU\nt\nw = GAT(Ht\u22121\nw , Ht\ns\n, Ht\ns\n)\nHt\nw = F F N(U\nt\nw + Ht\u22121\nw )\nU\nt\np = GAT(Ht\u22121\np\n, Ht\nw, Ht\nw)\nHt\np = F F N(U\nt\np + Ht\u22121\np\n)\n(7)\nNote that, H1\nw and H1\np\nare set to the same values\nwith H0\nw and H0\np\n, respectively.\n3.4 Sentence Extraction\nFor the sentence selector layer, we first extract document representation from the hidden state of passages via the attention layer, then combine document representation and each sentence by using the\nconcatenate operator, which is sequentially formulated as follows:\nzi = ReLu(\n\u20d7a\nT\np\n\u20d7 (hp)i)\n\u03b1i =\ne\nzi\nP\nj\ne\nzj\nh\u20d7\nd =\nX\nm\n\u03b1m \u2217\n\u0010\nh\u20d7\np\n\u0011\nm\n(8)\nh\u20d7\nd,sk = F F N \u0010\nh\u20d7\nd \u2225 h\u20d7\nsk\n\u0011\n(9)\nwhere \u20d7ap is learnable parameter. i and k represent\nindexes of passage i-th and sentence k-th, respectively. \u03b1i\nindicates the amount of contribution of\npassage i-th to document representation h\u20d7\nd. Consequently, if \u03b1i gets a high attention score, this\npassage tends to be more significant than other\npassages. Finally, the output sentence-document\nrepresentation h\u20d7\nd,si\nis used for sentences classification by using binary cross-entropy loss as the\ntraining objective:\nL =\n1\nN\ni\nX=N\ni=1\nyi\nlog ( \u02c6yi) + (1 \u2212 yi)log (1 \u2212 y\u02c6i)\n(10)\n6253\nModel arXiv PubMed\nR-1 R-2 R-L R-1 R-2 R-L\nSumBasic\u2217 29.47 6.95 26.30 37.15 11.36 33.43\nLexRank\u2217 33.85 17.36 28.99 39.19 13.89 34.59\nOracle+ 53.88 23.05 34.90 55.05 27.48 38.66\nCheng & Lapata (2016)+ 42.24 15.97 27.88 43.89 18.53 30.17\nSummaRuNNer+ 42.91 16.65 28.53 43.89 18.78 30.36\nXiao & Carenini (2019)(Xiao and Carenini, 2019) 43.62 17.36 29.14 44.85 19.7 31.43\nMatch-Sum 40.59 12.98 32.64 41.21 14.91 36.75\nTopic-GraphSum(Cui et al., 2020) 44.03 18.52 32.41 45.95 20.81 33.97\nSSN-DM(Cui and Hu, 2021) 45.03 19.03 32.58 46.73 21.00 34.10\nHeterGraphLongSum (iter=1) 46.62 18.69 40.77 48.75 22.45 43.97\nHeterGraphLongSum (iter=2) 47.36 19.11 41.47 48.86 22.63 44.19\nTable 1: Results on the test set. Report results with * are from Cohan et al. (2018), and results with + are from Xiao\nand Carenini (2019). Other results are obtained from respective papers. Our results are calculated by averaging\nvalues of 3 runs.\n5 Conclusion\nThis paper presents a new GNN-based model for\nextractive long document summarization. Specifically, GNN has been introduced as a promising\napproach for exploiting the complex relation of\nelements (e.g., word and sentence) from an input\ndocument. However, representing long documents\nas graph structure is still a remaining challenge.\nSpecifically, lacking cross-relation information between sentences (e.g., long-distance of position in\nthe document) and the increment of nodes might\ninfluence the performance. In this regard, this paper proposes a heterogeneous graph including three\ntypes of nodes such as word, sentence, and passage,\n6256\nwhich are simultaneously learned for enabling the\ncross-relation between sentences. The evaluation\nof two standard long documents datasets such as\narXiv and PubMed shows that the proposed model\noutperforms state-of-the-art models in this research\nfield without relying on pre-trained language models (e.g., BERT).\n", "SumGPT3": "Introduction:\nThe paper addresses the challenge of extractive document summarization, focusing on long-form documents, which remain a significant challenge due to the complexity of modeling long texts.\n\nNovelty:\n\nHeterogeneous Graph Structure: The paper introduces a novel approach by employing a heterogeneous graph structure consisting of word, sentence, and passage nodes. This structure enables the model to capture intricate relationships between elements in long documents, addressing the limitations of traditional methods that often result in information loss.\n\nGraph Attention Network (GAT) Integration: By incorporating GAT, the model effectively learns the interdependencies among nodes within the heterogeneous graph. This allows for the simultaneous processing of local and global information, enhancing the representation of long documents.\n\nMulti-Head Attention Mechanism: The utilization of multi-head attention further improves the model's performance by capturing diverse aspects of the document's semantic structure. This mechanism enhances the model's ability to discern important information for summarization.\n\nDynamic Passage Representation: Unlike previous methods that truncate long documents into fixed-length sequences, the proposed model dynamically represents passages, mitigating information loss and ensuring comprehensive coverage of document content.\n\nConclusion:\nThe paper presents a pioneering approach in extractive document summarization, particularly for long-form documents. By leveraging a heterogeneous graph structure, integrated with advanced attention mechanisms, the model achieves state-of-the-art performance on benchmark datasets without relying on pre-trained language models like BERT. These innovations pave the way for more effective summarization of lengthy documents, addressing a crucial need in the field of natural language processing.", "SumGem": "Introduction\n\nThis paper proposes a novel GNN-based approach for extractive summarization of long documents. GNNs have been shown to be effective in capturing complex relationships between elements in documents. However, representing long documents as graph structures poses challenges, such as capturing long-distance relationships between sentences and the impact of increasing the number of nodes on performance.\n\nNovelty\n\nThis paper addresses the limitations of existing GNN-based methods for long document summarization by introducing a heterogeneous graph structure with three types of nodes: words, sentences, and passages. These nodes are simultaneously learned to capture cross-sentence relationships, including long-distance dependencies. Here are the key aspects of the novelty:\n\nPassage Nodes: The model incorporates passage nodes as an additional semantic unit in the graph structure. This enables learning cross-sentence relations between sentences from different passages.\nAddressing Information Loss: By including passage nodes, the model mitigates the information loss problem that can occur in traditional graph-based methods for long documents, where similar sentence representations arise due to the large number of nodes.\nNon-pre-trained Language Models: The model utilizes a CNN-BiLSTM encoder for sentence embedding, avoiding the need for pre-trained language models like BERT. This makes the approach more applicable to low-resource languages.\nConclusion\n\nThe proposed Heterogeneous GraphLongSum model achieves state-of-the-art performance on benchmark long document summarization datasets (arXiv and PubMed) without relying on pre-trained language models. This demonstrates the effectiveness of the proposed approach for capturing long-distance relationships and mitigating information loss in extractive summarization of long documents.", "SumGPT4": "### Introduction\nThe paper presents a novel approach to extractive document summarization (EDS) for long-form documents, which has been a persistent challenge in natural language processing (NLP). Traditional methods often struggle with the loss of information when truncating long documents and capturing cross-sentence relationships.\n\n### Novelty\nThe main innovations of this research are outlined as follows:\n1. **Heterogeneous Graph-Based Model**: Unlike previous approaches that rely on simpler node types, this paper introduces a heterogeneous graph structure incorporating word, sentence, and passage nodes. This complexity allows for more nuanced relationships within the document structure.\n2. **No Pre-trained Models Required**: The proposed model, HeterGraphLongSum, does not depend on pre-trained language models like BERT, making it adaptable for low-resource languages and reducing dependency on large-scale pre-trained systems.\n3. **Passage Nodes for Cross-Relation Learning**: The inclusion of passage nodes is particularly innovative as it enables learning cross-relations between sentences in different passages, which helps in addressing the challenge of long-distance dependencies and semantic similarity issues in large documents.\n4. **Graph Attention Network (GAT)**: The model utilizes a GAT to dynamically update node relationships based on the structure of the graph, which is critical for accurately representing and summarizing large and complex documents.\n5. **Evaluation on Benchmark Datasets**: The model's effectiveness is demonstrated through rigorous testing on standard long document datasets like PubMed and arXiv, where it achieves state-of-the-art performance.\n\n### Conclusion\nThe proposed HeterGraphLongSum model marks a significant advancement in the field of extractive summarization for long documents. By integrating a novel graph-based approach with passage nodes and a GAT, it effectively manages the complexities of large-scale document summarization. This model not only outperforms existing methods but also offers a scalable solution applicable to various languages and domains, addressing key limitations in current EDS techniques.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary also effectively covers the key points of the paper, including the introduction to the problem, novelty (heterogeneous graph structure with three node types), and addressing limitations of existing methods. It outlines the use of passage nodes to mitigate information loss and highlights the non-dependency on pre-trained models like BERT. While it is well-structured, it lacks the detailed explanation of the model components (like GAT and multi-head attention) and specific results.", "expGem": "This summary, while addressing some aspects of the paper (GNN-based approach, heterogeneous graph structure with word, sentence, and passage nodes), lacks coherence and detail compared to the other summaries. It doesn't clearly explain the innovations or the methodology used (like GAT and multi-head attention). It also doesn't explicitly mention the performance results or the implications of the model in the field of natural language processing. Overall, it provides a less clear and informative overview compared to the other summaries.", "expGPT4": "This summary provides a comprehensive overview of the paper's contributions, starting with the introduction of the problem (extractive summarization of long documents), highlighting the novelty (heterogeneous graph structure with word, sentence, and passage nodes), and detailing the methodology (GAT integration, multi-head attention, dynamic passage representation). It concludes effectively by summarizing the model's achievements without relying on pre-trained models and its implications for natural language processing. This summary is clear, structured, and covers all essential aspects succinctly."}, "19": {"title": "Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks", "url": "https://aclanthology.org/2020.lrec-1.229", "text": "1. Introduction\nTranslation quality can be assessed in many different ways\n(House, 2015), for example, in the context of MT it is typically assessed in terms of adequacy and fluency (Koehn\nand Monz, 2006). While human evaluation does provide a\ngood estimate of translation quality, it is time consuming,\nexpensive, subjective and not directly applicable to new\ntranslations.\nAutomatic translation evaluation can be fast, cheap and consistent. A typical method is to compare the similarity between MT output and references, e.g. BLEU (Papineni et\nal., 2002). On the other hand, more recent reference-free\napproaches to MT Quality Estimation (MTQE), see (Bojar\net al., 2018; Barrault et al., 2019), use machine learning to\npredict MT quality from linguistic features from the source\nsentences and MT outputs. The popularity of MTQE is\nlargely driven by the research in MT development and the\nnecessity of evaluating mass output by various types of MT\nsystems. At the same time, automatic human translation\nestimation (HTQE) has received much less attention, as this\nis a much more challenging task.\nHowever, there is a surging need of automating the evaluation of human translation. This task fits into practical\nscenarios where human translations are scored by experts\nfor certification, course examination and possibly other applications such as self-evaluation in autonomous learning.\nTranslation proficiency test is often a compulsory module in\nuniversity language and translation programmes at different\nlevels. Language learners and/or trainee translators need\nto have their work graded in a formative and/or summative\nevaluation framework. In particular, during the course of\nlearning to translate, trainee translators can have feedbacks\nfrom such automatic evaluation systems that are \u2018always\nthere\u2019, without the constraints of the fixed working schedule of course instructors. HTQE (particularly fine-grained\nHTQE) can help in providing quick feedback so that trainees\ncan carry out in-depth diagnostic analyses on their own. In\nthe language service industry, fast turn-around of quality\nevaluation is also desirable for quality assurance and control. For translation or localization service users who do not\nalways possess a working bilingual proficiency, they need to\nhave some computational support on their side to determine\nthe quality of the service they paid for. Nevertheless, expert\nhuman input may not be immediately available. In a different context, large scale translation certification examinations, such as the ATA certification Exam1\n, ITI professional\nassessment2\n, CATTI3\nrequire assessment of many submissions. Using automated evaluation can help in reducing the\ncost of organizing the examination and mitigate the subjectivity of human evaluation in case an automatic evaluation\nsystems can yield reliable judgement of the quality of input\ntranslations.\nThe reference-free MTQE approaches, nevertheless, do not\nnecessarily work well on the task of predicting quality of human translations, since human translators tend to differ from\nMT in the kinds of errors they make. There has been some\nrecent work on HTQE (Yuan et al., 2016) using rich syntactic and semantic features, which are however languageand resource-dependent. To address these shortcomings, we\ntake a different direction and investigate a neural network\nmodel for fine-grained HTQE. In particular we propose a\ncustomized attention mechanism in order to capture both local and global bilingual quality information. Experiments\nshow that the proposed method outperforms two featurebased methods with 0.22+ higher correlation with human\njudgement, maintaining stable performance across four aspects of translation quality.\n2. Related Work\nConventional feature-based methods have been used for\ntranslation quality estimation, particularly for MT. A number of attempts have been made to use machine learned\nclassifiers and regressors for sentence level MT quality in\n1\nhttps://www.atanet.org/certification/aboutpractice_test.php\n2\nhttps://www.iti.org.uk/membership/professional-assessment\n3\nhttp://www.catti.net.cn/\n1859\nthe series of quality estimation shared tasks, predicting indirect quality indexes, such as post-editing effort (Specia,\n2011), post-editing distance (Specia and Farzindar, 2010),\npost-editing time (Koponen et al., 2012).\nAutomatic quality estimation of human translations is a\nnewly emerging topic. Yuan et al. (2016) developed a\nfeature set to predict adequacy and fluency of human translations at the document level, which includes comparison\nbetween parsed trees, argument roles, phrase alignments,\netc. In contrast, Zhou and Bollegala (2019) took an unsupervised approach to approximate and grade human translations into different categories using the bidirectional Word\nMover\u2019s Distance (Kusner et al., 2015).\nThere has been recent work using neural modelsto compare\na target translation with reference(s) in MT evaluation. For\nexample, Gupta et al. (2015) use Tree Long Short Term\nMemory (Tree-LSTM) based networks for reference-based\nMT evaluation. They propose a method that is competitive\nto the current complex feature engineering. Guzm\u00e1n et\nal. (2015) implemented neural models aiming to select\nthe better translation from a pair of hypotheses, given the\nreference translation.\nNeural models for MT Quality Evaluation have been also recently tested either as Neural Language models on a mixture\nof n-grams (Paetzold and Specia, 2016) or a reference-free\nMTQE prediction model built on quality vectors obtained\nfrom parallel corpora (Kim and Lee, 2016).\nOften sentence-level MTQE learn to predict translation\nquality in a indirect manner by ranking translations from\nbest to worst, while learning the direct assessment which\nmatches human evaluators is a challenging task, requiring\nextensive feature engineering and suffering from data sparsity, particularly for sentence-level predictions. Compared\nwith discrete models with manual quality features, neural\nnetwork models take low-dimensional dense embeddings as\nthe input, which can be trained from a large-scale dataset,\nthereby overcoming the issue of sparsity, and capture complex non-local syntactic and semantic information that discrete indicator features can hardly encode.\nThere has been some research on different ways for integration of LSTMs and CNNs, since the two methods for\nbuilding the neural networks are somewhat complementary.\nRoussinov et al. (2020) studied the use of LSTMs (or pretrained transformers) with convolution filters to predict the\nlexical relations for pairs of concepts, for example, Tom\nCruise is an actor or a rat has a tail. Most similar to\nour work is the study by (Zhou et al., 2016), which also\nused a stacked architecture with LSTM followed by twodimensional pooling to obtain a fixed-length representation\nfor text classification tasks. Here we contribute by having\na novel stacked siamese architecture applied to a different\ntask, namely HTQE.\nTherefore, our contribution is two-fold: we work on a more\nchallenging task (Guzm\u00e1n et al., 2017) than learning the\nrelative ranking of translations or estimating the similarity\nbetween candidate translations and references to simulate\nthe scores produced by professional evaluators; we propose\na stacked neural networks for fine-grained HTQE without\nrelying on engineered features and many external resources.\nx1 x2 \u2026 xm\ns1 y2 \u2026 yn y1\nWord embeddings\nCNN\nBiLSTM\nPooling Pooling\nattentions\nCNN\n+\nMSE Loss\nS1 S2 S3 S4\nScores\nFigure 1: Model Structure\n3. Models\nOur neural network architecture is shown in Figure 1. Given\na translation pair, the source sentence x and the translated\nsentence y are encoded into a fixed-sized vector representation through two separate CNN-BiLSTM-Attention stacks.\nDenoting the final vectors as x and y respectively, our\nmodel predicts four quality scores (usefulness, terminology, idiomatic writing and target mechanics as defined by\nthe ATA, see their definitions below in the Dataset section)\nusing a linear regression on the concatenation of x and y.\n3.1. Context-aware Word Representation\nGiven a source sentence x or a translation y, which can be\nrepresented by w1, w2, . . . , wn, we first transform the words\ninto vector representations. To this end, we build multiple\nconvolution layers upon standard word embedding layers for\ncontext-aware word representation.\nFor a convolution layer of width k, we apply multiple kernels Hi \u2208 R\nd\u00d7(2k+1) before a non-linearity transformation.\nSpecifically, for a window centred at i-th word, the output\nfi\nis given by:\nfi = relu(hHi\n, w[i\u2212k:i+k]i + bi),\nwhere w[i\u2212k:i+k] denotes the window size, bi\nis a bias.\nThe word representation is then the concatenation of all\nconvolution layers.\n3.2. Sentence-level Representation\nTo capture global information of a sentence, bidirectional\nLSTMs (Graves et al., 2013) are used on fi\n. The outputs\ninclude a sequence of forward hidden states. and a sequence\nof backward hidden states We then concatenate the two\nsequences into one hi =\n\u2212\u2192hi\n||\u2190\u2212\nhi for representing wi\n. In this\nway, each annotation hi contains summarized information\nabout the whole input sentence, but with a strong attention\nto the details surrounding the i-th word.\n3.3. Attention mechanism\nDifferent parts in a translation pair do not contribute equally\nto the semantic adequacy and language fluency of the final\n1860\nUT TS IW TM Score\nMin. 2.00 2.00 3.50 1.50 11.50\n1st Quartile 17.50 14.50 18.50 9.50 60.00\nMedian 23.00 18.00 20.50 11.50 71.50\nMean 22.17 16.73 19.42 10.94 69.24\n3rd Quartile 28.50 20.50 21.50 12.50 82.50\nMax. 34.50 25.00 25.00 15.00 98.50\nKrippendorff\u2019s \u03b1 0.96 0.96 0.74 0.89\nTable 1: Description of the dataset\noutput. Attention mechanisms have shown their efficiency\nin a number of NLP tasks (Vaswani et al., 2017). After\nobtaining the sentence representations centred at different\nwords, we take repeated reading and aligning, using a crossattention mechanism to detect those bits which are important\nfor quality estimation.\nIn particular, we use the weighted average of the source\nrepresentations to decide which parts of the translated sentence are important for quality estimation and vice versa.\nGiven hi for each word, the final sentence representation\nafter attention is:\ns =\nXn\ni\n\u03b1ihi\n,\nwhere \u03b1i\nis the attention weight for hi and it is computed\nby:\n\u03b1i =\nexp(f(hi\n, h))\nPn\ni\nexp(f(hi\n, h))\nThe score function f is:\nf(h, hi) = v\nT\ntanh(Wa1h + Wa2hi),\nwhere v \u2208 R\nda , Wa1 \u2208 R\nda\u00d72h\nand Wa2 \u2208 R\nda\u00d72h\nare\ntrainable parameters.\n3.4. Training\nGiven a training triple (x, y, s), where x is the source sentence, y is the translated sentence and s \u2208 R\nk\nis the score\nvector annotated by human judges from k different aspects,\nrespectively. MSE loss is used for training.\n`(x, y, s) = 1\nk\nX|scorei(x, y) \u2212 si\n|\n2 + \u03bb||\u0398||2\nwe use Adam (Kingma and Ba, 2014) to optimize parameters. To avoid over-fitting, dropout is applied with a rate of\n0.001. \u03bb is the l2 regularization parameter.\n5. Conclusions\nThis paper presents a neural model for the Human Translation Quality Estimation (HTQE) task, which involves a\nweighted cross attention mechanism to adaptively detect\nthe relevant parts in the source-target sentence pairs. Despite having no hand-crafted features, experimental results\nshow that the neural model with attention can outperform\nconventional feature-based methods as well as a baseline\nneural model. To our knowledge, we are the first to apply\nneural networks to reference-free fine-grained HTQE. Our\ncode and dataset of expert-annotated translations with finegrained scores for the English-Chinese direction is available\nunder a permissive licence.7\nIn the future, we plan expanding this study in two directions.\nWhile initial experiments with BERT (Devlin et al., 2018)\ndid not show improvements in the model, we will try truly\ncross-lingual language models such as XML-R (Conneau et\nal., 2019), since cross-lingual language models are likely to\nbe more effective in comparison to the current model which\nuses independent embeddings for each language, while the\ntraining set itself is too small to infer links between languages from bilingual data. Next, we will experiment with\nthe integration of other features into attention, such as alignment information from large parallel corpora, to introduce\nquality vectors similarly to (Kim and Lee, 2016). Even\nthough the neural architecture outperforms feature-based\nmethods, we can try integrating features which manifest\ntranslators\u2019 decision-making into the neural network.\n", "SumGPT3": "**Introduction**\n\nThe paper addresses the need for automated Human Translation Quality Estimation (HTQE), particularly in contexts such as certification, course evaluation, and the language service industry. While traditional Machine Translation Quality Estimation (MTQE) methods, including reference-based metrics like BLEU and reference-free approaches using machine learning, have been well explored, HTQE remains challenging due to the distinct error patterns of human translators compared to machine translation systems.\n\n**Novelty**\n\n1. **Neural Network Model with Customized Attention Mechanism**: The paper introduces a novel neural network architecture incorporating a customized attention mechanism. This model aims to capture both local and global bilingual quality information, which is crucial for evaluating human translations.\n\n2. **Fine-Grained Quality Evaluation**: Unlike previous approaches that focus on broad quality metrics, this model targets fine-grained evaluation, scoring translations on aspects such as usefulness, terminology, idiomatic writing, and target mechanics. This fine-grained approach is designed to provide detailed feedback, particularly useful for language learners and trainee translators.\n\n3. **Elimination of Hand-Crafted Features**: The proposed model does not rely on hand-crafted features or extensive external resources, which are typically required in conventional feature-based methods. Instead, it leverages low-dimensional dense embeddings trained from a large-scale dataset, overcoming issues of data sparsity and capturing complex syntactic and semantic information.\n\n4. **Performance Improvement**: Experiments demonstrate that the proposed method achieves a 0.22+ higher correlation with human judgment compared to two feature-based methods, indicating its superior ability to predict translation quality accurately.\n\n**Conclusion**\n\nThe paper presents a significant advancement in HTQE by proposing a neural network model with a customized attention mechanism for fine-grained quality evaluation. This approach outperforms traditional feature-based methods, providing a more reliable and detailed assessment of human translations without the need for hand-crafted features. The findings suggest that integrating cross-lingual language models and additional features like alignment information from parallel corpora could further enhance the model's performance. The study's contributions offer practical implications for automated translation evaluation in educational and professional settings.", "SumGem": "## Summary\n\nThis paper proposes a novel neural network model for fine-grained Human Translation Quality Estimation (HTQE). The task aims to automatically assess the quality of human translations without requiring reference translations.\n\n## Novelty\n\n* **Stacked siamese architecture with attention:** The model utilizes a stacked siamese architecture with a new attention mechanism to assess translation quality. Siamese networks process source and target sentences independently through separate CNN-BiLSTM stacks. The attention mechanism focuses on relevant parts in both sentences to capture fine-grained quality aspects.\n* **Weighted cross-attention:** Unlike previous attention mechanisms that focus on one sentence at a time, this model employs a weighted cross-attention mechanism. This allows the model to consider how well each part of the source sentence is translated into the target sentence and vice versa. \n* **Reference-free approach:** Unlike many Machine Translation Quality Estimation (MTQE) methods, this model does not rely on reference translations. This makes it applicable to scenarios where reference translations are unavailable.\n* **No hand-crafted features:** The model achieves good performance without using manually engineered features, which can be time-consuming and domain-specific.\n\n## Conclusion\n\nThe paper demonstrates that the proposed neural network with a weighted cross-attention mechanism achieves superior performance compared to feature-based methods and a baseline neural model for fine-grained HTQE in English-to-Chinese translation. The authors make their code and dataset publicly available to facilitate further research in this area. Future work includes exploring the effectiveness of cross-lingual language models and incorporating additional features like alignment information from parallel corpora.\n", "SumGPT4": "### Introduction\nThe scientific paper addresses the challenges and solutions related to the assessment of translation quality, especially human translation quality estimation (HTQE). With an increased demand for automating the evaluation process of human translations in various professional, educational, and certification contexts, the study proposes a novel neural network approach that avoids reliance on traditional feature-based methods, focusing on a more dynamic and holistic evaluation mechanism.\n\n### Novelty\nThe key novelties introduced in the paper include:\n1. **Customized Attention Mechanism**: The paper introduces a neural network model incorporating a customized attention mechanism designed to capture both local and global bilingual quality information effectively. This approach contrasts with previous methods that mainly relied on discrete feature sets and were less adept at handling the nuanced errors typical in human translations.\n2. **Fine-grained HTQE**: The model is tailored for fine-grained evaluation, providing detailed feedback on various aspects of translation quality such as usefulness, terminology, idiomatic writing, and target mechanics. This specificity supports in-depth diagnostic analyses by translators, which is a significant advancement over the broader evaluations typical of previous systems.\n3. **Stacked Neural Architecture**: A novel stacked siamese architecture using CNN-BiLSTM networks is employed, which integrates the strengths of both convolutional neural networks and bidirectional long short-term memory networks. This hybrid approach enhances the model's ability to capture complex syntactic and semantic patterns in the text.\n4. **Reference-Free Approach**: Unlike many existing models that depend on comparison with a reference translation, this model operates without references, which is crucial for scenarios where no standard translation is available, thereby broadening its applicability.\n\n### Conclusion\nThe paper successfully demonstrates that its neural network-based model for HTQE significantly outperforms traditional feature-based methods, maintaining a higher correlation with human judgment across various translation quality aspects. This advancement indicates a promising direction for further development in translation quality assessment, especially in environments where quick, reliable feedback is crucial. The future work aims to integrate cross-lingual language models and additional contextual features to further refine the accuracy and applicability of the HTQE system.", "rankGPT3": 1, "rankGem": 3, "rankGPT4": 2, "expGPT3": "This summary is structured well, starting with an introduction and then detailing the novelty of the proposed model. It clearly outlines the key contributions of the paper, such as the neural network architecture with a customized attention mechanism and its application to fine-grained HTQE. It concludes with the significance of the findings and implications for future research. Overall, it provides a comprehensive overview of the paper's objectives and achievements.", "expGem": "While this summary covers the basic aspects of the paper, it lacks the detailed breakdown of the novelties and contributions compared to the other two summaries. It briefly mentions the customized attention mechanism and the fine-grained HTQE but does not delve into the specifics of how these novelties contribute to advancing the field. Additionally, the conclusion is quite general and does not highlight the experimental validation or specific performance metrics discussed in the other summaries.", "expGPT4": "This summary is also well-structured and provides a clear introduction to the topic of HTQE. It discusses the novelties of the proposed model effectively, such as the customized attention mechanism, fine-grained evaluation, and the use of a stacked siamese architecture. The conclusion summarizes the findings and future directions succinctly. It ranks slightly below the best summary because it lacks the detailed breakdown of specific novelties and the experimental validation mentioned in that summary.\n"}, "20": {"title": "Lexicon Guided Attentive Neural Network Model for Argument Mining", "url": "https://aclanthology.org/W19-4508", "text": "1 Introduction\nArgument Mining (AM) is an emerging research\narea that has drawn more and more attention since\naround 2010. Recently, Project Debater from IBM\nhas shown such an AI machine supported by argument mining techniques can do well at arguing. The task of AM can be divided into a few\nstages: (1) Extracting argumentative components\nfrom large texts, i.e., boundary detection or segmentation; (2) Classifying the extracted components into classes. In general, an argumentative component can be categorized into \u201dClaim\u201d,\nwhich usually contains conclusions and stance toward the given topic, or \u201dPremise\u201d, which contains reasoning or evidence used to support or attack a claim; (3) Predicting the relations between\nthe identified argumentative components, i.e., supporting and attacking (Cabrio and Villata, 2018).\nSome works also consider more complex relations such as recursively support/attack the relations themselves rather than merely build relations\nbetween components (Peldszus and Stede, 2013).\nArgument detection and classification can improve legal reasoning (Moens et al., 2007), policy\nformulation (Florou et al., 2013), and persuasive\nwriting (Stab and Gurevych, 2014). In this paper,\nwe focus on mining argumentative components\nfrom a large collection of documents and further\nclassifying them into roles of support/opposition.\nOur model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks (Cho et al., 2014).\nWith the help of the attention mechanism (Bahdanau et al., 2015), RNN can further attend on the\nkey information.\nWe propose a novel attention mechanism that\nis guided by argumentative lexicon information.\nLexicon information is reported as one kind of the\nmost frequently used features in argument mining (Cabrio and Villata, 2018). Previous works on\nAM have tried to integrate lexical features into the\nlearning models (Levy et al., 2017; Nguyen and\nLitman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other\ndomains. In the contrast of scarcity of general\nlexicon for AM, lexical resources are abundant in\nother fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004;\nMohammad and Turney, 2013; Kiritchenko and\nMohammad, 2016). As a more general domain,\nAM may get the benefits of not only in-domain\nlexicon, but also out-domain lexicons.\nThe contribution of this work is two-fold: (1)\nWe propose an attention mechanism to leverage\nlexicon information. (2) In the face of the scarcity\nof argument lexicon, we explore several different types of lexicons to verify whether outside resources are useful for AM tasks.\nThe rest of this paper is organized as follows.\nSection 2 summarizes related works about AM.\nThe dataset and linguistic resources used for experiments are shown in Section 3. We introduce\n68\nour model in Section 4 and show the experimental results in Section 5. We also look into the errors made by our best model in Section 6. Section 7 makes a discussion on experimental results\nand concludes this work.\n2 Related Works\nNeural networks have been used in varieties of\nAM tasks. To improve the vanilla LSTM model,\nStab et al. (2018a) use attention mechanism to fuse\ntopic and sentence information together. In the\nwork of Laha and Raykar (2016), they present several bi-sequence classification models on different\ndatasets. However, rather than using some sophisticated architecture such as attention, it considers\nonly different concatenation or condition method\non the output of LSTM. Eger et al. (2017) propose\nan end-to-end training model to mining argument\nstructure, identifying argument components.\nBesides syntactic and positional information,\nlexical information is also reported as one of\nthe most used features in argument mining task\n(Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN\nmodels. Teng et al. (2016) use lexical scores as the\nweights and do the weighted sum over the outputs\nof the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the\nmodel to focus on the lexicon words. Bar-Haim\net al. (2017) proposes an idea of expanding lexicons to improve stance classifying task.\nHowever, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and\nuse weak supervision. Shnarch et al. (2018) also\npresent a methodology to blend such weak labeled\ndata with high quality but scarce labeled data for\nAM. Al-Khatib et al. (2016) consider the distant\nsupervision method. Most of these works use the\nend-to-end training paradigm with the outside resources only for generating the weak label, which\nmay not fully leverage the information of the lexicons.\n4 Model\nThis section describes the development of the\nbaseline model and the proposed model. To identify sentence-level argumentative components, the\nmodel is given a sentence C, which contains a sequence of words w\nc\n1\n, wc\n2\n, ..., wc\nN and a topic T with\nwords w\nT\n1\n, wT\n2\n, ..., wT\nK. The input word sequence\nis then encoded as a sequence of word embeddings\nvia the GloVe word vectors. The pre-trained word\nvectors with the dimension of 100 released by Pennington et al. (2014) are used. Based on the given\ninput, the model makes a prediction y\u02c6 for the given\nsentence, i.e., classifying it as supporting argument, opposing argument, or non-argument. For\ncomparison, we implement a baseline model with\nthe vanilla bidirectional LSTM (BiLSTM).\nIn order to exploit the linguistic knowledge,\nLei et al. (2018) highlight the sentiment words of\nthe input sentence, computing attention weight for\neach word with them. By integrating the sentiment lexicon into the neural network model, the\nwork successfully improves the performance in\nsentiment analysis. This work proposes a model\nthat integrates an outside lexicon resource into attention mechanism (Vaswani et al., 2017). For\neach input sentence, we compose an additional\nsentence A, which contains words w\na\n1\n, wa\n2\n, ..., wa\nM\nbased on the given lexicon. The additional sentence A is then forwarded to the embedding layer\ntogether with input sentence C. The output of\n70\nmodel F1 Parg+ Parg\u2212 Rarg+ Rarg\u2212\nBiLSTM .5337 \u00b1 .0123 .4521 \u00b1 .0391 .4832 \u00b1 .0393 .2911 \u00b1 .1139 .4816 \u00b1 .1276\nClaimLex\u2217\n.5684 \u00b1 .0222 .4736 \u00b1 .0322 .5075 \u00b1 .0450 .3756 \u00b1 .1072 .5011 \u00b1 .0854\nSentimentLex\u2217\n.5718 \u00b1 .0165 .4937 \u00b1 .0365 .5125 \u00b1 .0414 .3590 \u00b1 .1043 .5240 \u00b1 .0889\nEmotionLex\u2217\n.5695 \u00b1 .0129 .4920 \u00b1 .0369 .5036 \u00b1 .0356 .3524 \u00b1 .0861 .5264 \u00b1 .1006\nWordNet\u2217\n.5788 \u00b1 .0142 .4846 \u00b1 .0292 .5191 \u00b1 .0376 .3724 \u00b1 .0818 .5235 \u00b1 .0772\nTable 1: The results of the baseline model and the proposed model with different lexicon resources. The highest\nscore of each column is highlighted in bold font.\nembedding layer is the sequences e\nc\n1\n, ec\n2\n, ..., ec\nN\nand e\na\n1\n, ea\n2\n, ..., ea\nM, representing the embedded sentences C and A, respectively. Then, e\nc\ni\nis fed into\nBiLSTM and results in h\nc\ni\nat the corresponding\ntime step. As for A, we add an RNN to collect its\ninformation and take the output h\na\nM at the last time\nstep as its representation. Though Lei et al. (2018)\nuse an LSTM to encode the sentimental sentences,\nwe do not follow their approach. In our work, the\nsimple RNN outperforms the LSTM in the preliminary experiments.\nThe attention weight of the i-th word (i.e. \u03b1i)\nis determined by the concatenation of the output\nof the BiLSTM h\nc\ni\nand the output of the RNN (i.e.\nh\na\nM), which is given the additional sentence A as\nthe input:\n\u03b1i =\nexp(\u03c3([h\nc\ni\n; h\na\nM]))\nPN\ni=1 exp(\u03c3([h\nc\ni\n; h\na\nM]))\n(1)\nwhere \u03b1i\nindicates the attention weight of i-th\nword of the input sentence, and [h\nc\ni\n; h\na\nM] indicates\nthe concatenation of i-th hidden state and the RNN\noutput state. The scoring function \u03c3(\u00b7) is designed\nas:\n\u03c3([h\nc\ni\n; h\na\nM]) = tanh(Wc[h\nc\ni\n; h\na\nM]) (2)\nwhere Wc indicates trainable parameters.\nAll the weighted hidden states are then summed\nup, and connected to a fully connected layer for\nthe final prediction:\no =\nX\nN\ni=1\nh\nc\ni\n0\n(3)\nh\nc\ni\n0\n= \u03b1ih\nc\ni\n(4)\nFigure 1 illustrates the architecture of our model.\n7 Conclusion\nIn this work, we propose a novel approach to leverage the lexicon from both in-domain and out-ofdomain sources for the task of argumentative component mining. We explore several sources from\ndifferent application scenarios, from claim lexicon\n(Levy et al., 2017) to other domain resources such\nas sentiment analysis (Hu and Liu, 2004), emotion detection (Mohammad and Turney, 2013),\nand the general domain lexicon resource (Miller,\n1995). Experimental results confirm the effectiveness of the integration of lexicon information. The\nscarcity of the resources in argument mining is\nalso highlighted in the discussion.", "SumGPT3": "### Introduction\n\nThe paper explores advancements in Argument Mining (AM), specifically focusing on the extraction and classification of argumentative components such as claims and premises from large texts. It proposes a model utilizing Recurrent Neural Networks (RNN) with an attention mechanism guided by lexicon information. The scarcity of general lexicons for AM contrasts with the abundance of lexical resources in other fields like sentiment analysis and emotion detection.\n\n### Novelty\n\n1. **Attention Mechanism Guided by Lexicon Information**: The paper introduces a novel attention mechanism that leverages lexicon information to improve the performance of AM tasks. Unlike previous models, which often used lexicons for weak supervision or feature extraction, this approach directly integrates lexicon-derived information into the model's attention mechanism.\n\n2. **Integration of Out-Domain Lexicons**: The study explores the utility of various types of lexicons, including those from sentiment analysis, emotion detection, and general language resources, to enhance the performance of AM. This is in response to the scarcity of argument-specific lexicons and represents a significant shift from the domain-specific lexicon reliance typical in AM research.\n\n3. **Model Architecture**: The proposed model uses a bidirectional LSTM (BiLSTM) for encoding input sentences and an RNN for processing additional sentences derived from lexicons. This combination allows for the effective merging of lexical information with the sequence data from the input, leading to improved argument classification.\n\n4. **Performance Improvement**: The experimental results demonstrate that incorporating lexicon information from various domains significantly enhances the model's performance. The model outperforms a vanilla BiLSTM baseline, showing the highest improvement when using general domain lexicon resources like WordNet.\n\n### Conclusion\n\nThe paper presents a significant advancement in AM by proposing a model that integrates lexicon information from both in-domain and out-domain sources through a novel attention mechanism. This approach addresses the challenge of lexicon scarcity in AM and demonstrates substantial performance improvements. The findings suggest that leveraging lexical resources from other fields can benefit AM tasks, paving the way for future research to explore more diverse lexical integrations.", "SumGem": "## Summary of Argumentative Component Mining with Lexicon Guided Attention\n\nThis paper proposes a novel attention mechanism to improve argumentative component mining, a task that aims to classify sentences into supporting arguments, opposing arguments, or neutral stance.\n\n**Novelty**\n\n* The paper addresses the scarcity of argument-specific lexicons by proposing a method to leverage lexicons from related domains like sentiment analysis and emotion detection. \n* The model incorporates an \"argument lexicon\" containing words that indicate claims and combines it with the input sentence during the encoding stage. \n* A novel attention mechanism is introduced that considers both the hidden states of the input sentence from a BiLSTM layer and the lexicon information to compute attention weights.\n\n**Conclusion**\n\nThe experimental results show that the proposed method achieves significant improvement over a baseline BiLSTM model, especially when using lexicons from related domains. This suggests that the model can effectively exploit external knowledge sources to enhance argumentative component mining. The paper highlights the need for more domain-specific argument lexicons to further improve the performance of the model.", "SumGPT4": "**Introduction**\n\nThe paper focuses on enhancing Argument Mining (AM) by leveraging lexicon-based attention mechanisms within recurrent neural network (RNN) models. Argument Mining, crucial for extracting and analyzing argumentative structures in texts, typically involves segmenting texts into argumentative components, classifying these components, and predicting their relationships. The paper aims to integrate lexicon information from various domains into the neural model to improve the detection and classification of argumentative components.\n\n**Novelty**\n\n1. **Lexicon-Guided Attention Mechanism**: The paper introduces a novel attention mechanism that utilizes lexicon information to enhance the performance of RNN models in AM tasks. This mechanism allows the model to focus on important words guided by domain-specific and general lexical resources.\n\n2. **Integration of Multiple Lexical Resources**: Unlike previous works that rely on domain-specific lexicons, this research explores the use of diverse lexical resources from sentiment analysis, emotion detection, and general lexical databases. This approach helps address the issue of domain specificity and scarcity of argument lexicons.\n\n3. **Comparison of Lexical Resources**: The study systematically compares the effectiveness of different lexicon types (ClaimLex, SentimentLex, EmotionLex, WordNet) in the context of AM, demonstrating that out-of-domain lexicons can be beneficial for AM tasks.\n\n4. **Improved Performance with Simple RNN**: The proposed model uses a simple RNN for encoding lexical sentences instead of more complex LSTM structures. Preliminary experiments showed that a simple RNN outperforms LSTM in this context, contributing to a more efficient model.\n\n**Conclusion**\n\nThe paper presents a significant advancement in AM by integrating diverse lexical resources into a novel attention mechanism within an RNN framework. This approach effectively addresses the limitations of domain-specific lexicons, improving the detection and classification of argumentative components. The results highlight the potential of leveraging lexicons from various domains to enhance AM, providing a robust solution to the scarcity of argument-specific lexical resources.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "covers the main points of the paper adequately, including the introduction, novelty aspects (attention mechanism, integration of lexicons), and conclusion. However, it is slightly less detailed in explaining the specific types of lexicons used and their comparative effectiveness. Nonetheless, it effectively communicates the main findings and contributions of the paper.", "expGem": "the shortest among the three and provides a more abbreviated overview of the paper. While it touches upon the introduction, novelty aspects (attention mechanism, lexicon integration), and conclusion, it lacks sufficient detail and depth compared to the other summaries. It briefly mentions the types of lexicons used but doesn't elaborate on their effectiveness or comparative analysis. Thus, it is less informative and comprehensive.", "expGPT4": "provides a clear and structured overview of the paper, covering all essential aspects such as the introduction, novelty of the approach, model architecture details, and conclusion. It effectively summarizes the key contributions, including the novel attention mechanism, integration of diverse lexicons, and performance improvements. The summary is well-organized, concise yet comprehensive, and maintains a good balance between technical details and clarity."}, "21": {"title": "Word-level Morpheme segmentation using Transformer neural network", "url": "https://aclanthology.org/2022.sigmorphon-1.15", "text": "1 Introduction\nMorphological analysis is the heart of nearly all\nnatural language processing tasks, such as sentiment analysis, machine translation, information\nretrieval, etc. Such natural language processing\ntasks become infeasible without any morphological analysis. One reason is the sparsity resulting\nfrom a high number of word forms that introduce\nout-of-vocabulary (OOV). Morphological segmentation is a way to deal with language sparsity by\nintroducing the standard segments within the words\nrather than dealing with word forms (having multiple morphemes).\nMorpheme segmentation is a type of morphological analysis in which words are divided into surface\nforms of morphemes, for example, successfulness\n= success @@ful @@ness. Automated morpheme\nsegmentation was studied in the early years of natural language development (NLP). However, significant progress has been made in recent years in\nusing various machine learning techniques.\nSince morphemes are the smallest meaningful\nlanguage units, information about the morphemic\nstructure of words is already used in various NLP\napplications and additional tasks, including machine translation and recognition of semantically\nrelated words (cognates).\nIn this paper, we proposes a supervised method\nfor word-level morphological segmentation using\na transformer neural network. The task of machine\ntranslation has seen significant progress in recent\ntimes with the advent of Transformer-based models (Vaswani et al., 2017) for this year\u2019s SIGMORPHON 2022 shared task on morpheme segemntation (Batsuren et al., 2022a) which at the word\nlevel, participants will be asked to segment a given\nword into a sequence of morphemes. Input words\ncontains all types of word forms: root words, derived words, inflected words, and compound words.\nHowever, to the best of our knowledge, there has\nnot been work that applies such morpheme segmentation transformer-based models.\nThe paper is organized as follows: Section 2 addresses the related work on supervised morpheme\nsegmentation, Section 3 describes the data used in\ntraining, Section 5 describes the model architecture,\nand section 6 presents the experiment results.\n2 Related work\nZ. Harris in (Harris, 1970) proposed the earliest\nmethod of morpheme segmentation. It detects morpheme boundaries by letter variety statistics (LVS)\n(C\u00b8 oltekin \u00a8 , 2010). Morfessor system (Creutz and\nLagus, 2007), (Smit et al., 2014) exploits unsupervised machine learning methods to be trained\non a large unlabelled text. Another kind of semisupervised machine learning for morpheme segmentation (Ruokolainen et al., 2014) was based on\nconditional random fields; the task was considered\nas sequential classifying and labeling letters of a\ngiven the word. A pure supervised method with\nsignificantly better quality for the twofold task of\nmorpheme segmentation with classification was\nproposed in (Sorokin and Kravtsova, 2018); it was\neffective due to applying a convolutional neural\nnetwork and training on the representative labeled\ndata. The model outperforms all previous morpheme segmentation models, giving F-measure up\n139\nto 98% on morpheme boundaries. Recent works\ndeveloped two more supervised machine learning\nmodels for morpheme segmentation with classification for Russian words (Bolshakova and Sapin,\n2019a), (Bolshakova and Sapin, 2019b). The first\nis based on decision trees with gradient boosting,\nwhile the second applies Bi-LSTM neural network.\nHowever, they were developed for morpheme segmentation applied CNN, Bi-LSTM, not applied\ntransformer neural network. Therefore, to study\npossible ways to build a more broad supervised\nmodel with a transformer neural network.\n4 Model architecture\nWe use the character level Transformer implementation of fairseq (Ott et al., 2019). Our model is composed of one encoder input word, and one decoder\noutput segmentation of the word. We train a monolingual word segmentation model for each given\nlanguage with identical parameters, 50 epochs, 1\nencoder layer, 1 decoder layer, 0.0001 learning\nrate, using the Adam optimizer (Kingma and Ba,\n2014) and the cross-entropy loss. Various hyperparameters of our Transformer model were experimentally tested in several experiments. The resulted model has the encoder and decoder layer\nwith 128 hidden units, and the batch size is 32. Encoder and decoder more layers slightly improve the\nquality (less than 0.5%), but the model became too\nheavy both for training and evaluation. We also use\ncreated checkpoints to save the checkpoint the latest and the best ones. It is also a safe guard in case\nthe training gets disrupted due to some unforeseen\nissue.\n4.1 Evaluation\nFor the word-level segmentation shared task, the\nfollowing evaluation metrics are provided.\n\u2022 Precision: fraction of correctly predicted morphemes on all predicted morphemes\n\u2022 Recall: ratio of correctly predicted morphemes on all gold morphemes\n\u2022 F-measure: the harmonic mean of the precision and recall\n\u2022 Edit distance: average Levenshtein distance\nbetween the predicted output and the gold instance.\nWe compare our results with the baseline model, in\nwhich the multilingual Bert tokenizer is shown in\ntable 2.\n6 Conclusion\nWe have presented the monolingual models for morpheme segmentation in 9 languages. Our model run\noutperforms the baseline. Even though our models\nas implemented prior to submission failed to attain\nreasonable evaluations scores on the word-level\nmorpheme segmentation task, our results indicate\nthat our model has the potential to have a better\nperformance after fine-tuning and the good performance of our model under varying morphological\ncomplexity languages.\nIn future work, we plan on exploring multilingual word-level morpheme segmentation a model.", "SumGPT3": "**Introduction:**\nThe paper addresses the challenge of morphological segmentation in natural language processing (NLP), a critical task for various applications like sentiment analysis and machine translation. By segmenting words into morphemes, this method helps mitigate the problem of word form sparsity. The study presents a supervised approach to word-level morphological segmentation using a Transformer neural network, an area that has not been explored extensively with this specific model architecture.\n\n**Novelty:**\n1. **Application of Transformer Models:** The primary novelty lies in applying Transformer-based models for morpheme segmentation, an approach previously unexplored in this specific task. While Transformers have been successful in other NLP tasks, their use in morpheme segmentation is novel.\n   \n2. **Character-level Implementation:** The model is implemented at the character level using the fairseq toolkit, which is relatively unique in comparison to past methods that often focused on word or subword levels.\n\n3. **Monolingual Word Segmentation:** The study focuses on training monolingual models for nine different languages, showing the potential for broader applicability across linguistically diverse datasets.\n\n4. **Experimental Hyperparameter Tuning:** The paper includes a comprehensive experimental analysis of various hyperparameters, optimizing the Transformer model for the task of morphological segmentation.\n\n5. **Performance Metrics:** Detailed evaluation metrics, including precision, recall, F-measure, and edit distance, provide a thorough assessment of the model's performance compared to the baseline multilingual BERT tokenizer.\n\n**Conclusion:**\nThe study presents a novel application of Transformer neural networks for morphological segmentation, outperforming baseline models. Despite initial implementation challenges and suboptimal evaluation scores, the results indicate significant potential for improved performance through further fine-tuning. Future work will explore multilingual models, expanding the applicability of this approach to a wider range of languages with varying morphological complexities.", "SumGem": "## Summary\n\nThis paper proposes a supervised method for word-level morphological segmentation using a transformer neural network. The authors argue that while there has been significant progress in morpheme segmentation using various machine learning techniques, transformer-based models have not yet been applied to this task.\n\n**Novelty**\n\n* The paper proposes a novel approach to morpheme segmentation by leveraging a transformer neural network for this task. \n* The authors train a monolingual word segmentation model for each language as opposed to previous work that focused on unsupervised or semi-supervised learning.\n\n## Conclusion\n\nThe paper acknowledges that the current implementation did not achieve competitive results on the shared task. However, the authors highlight the potential of their approach, particularly for languages with varying morphological complexity. They plan to explore a multilingual model in future work.", "SumGPT4": "### Summary of the Scientific Paper\n\n#### Introduction\nMorphological analysis is a foundational aspect of many natural language processing (NLP) tasks, addressing the issue of word form sparsity by breaking down words into morphemes, their smallest meaningful units. This paper introduces a novel approach to word-level morphological segmentation utilizing a transformer neural network. The goal is to improve upon existing methods, which range from traditional statistical approaches to recent neural network models, by applying advanced transformer architectures for better segmentation accuracy.\n\n#### Novelty\n1. **Transformer Neural Network Application:** The paper proposes the first application of transformer-based models for morpheme segmentation at the word level. Previous methods have relied on models like CNNs, Bi-LSTMs, and decision trees, but have not explored the transformer architecture for this specific task.\n\n2. **Supervised Learning Approach:** While earlier methods have used unsupervised or semi-supervised learning for morpheme segmentation, this work introduces a fully supervised method that leverages labeled data for training the transformer model. This approach aims to improve segmentation precision by directly learning from annotated examples.\n\n3. **Character-Level Segmentation:** The model operates at the character level, using a specific implementation of the fairseq framework. This allows the segmentation to be more granular and precise, addressing the challenge of segmenting complex word forms including root words, derived words, inflected words, and compound words.\n\n4. **Experimental Validation:** The paper reports extensive experimentation with hyperparameters, leading to an optimal transformer configuration for morpheme segmentation. This detailed tuning process is crucial for achieving high segmentation performance.\n\n#### Conclusion\nThe proposed transformer-based model for morpheme segmentation demonstrates promising results compared to traditional approaches and baseline models. Despite initial shortcomings in evaluation scores, the results suggest that with further fine-tuning, the model has significant potential to outperform existing methods, particularly in handling the morphological complexity of different languages. Future work will focus on extending this approach to a multilingual context to enhance its applicability across diverse linguistic landscapes.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "provides a comprehensive overview of the scientific paper, covering all essential aspects such as the introduction, novelty, and conclusion in a structured manner. It clearly outlines the motivation behind the study, the novel contributions (application of Transformer models for morpheme segmentation, character-level implementation, supervised learning approach), and the potential future directions. It maintains clarity and coherence throughout, making it the best summary among the three.", "expGem": "provides a brief overview but lacks depth and clarity compared to the other summaries. It mentions the novelty of using Transformer models but fails to elaborate on the specific contributions and methodology in detail. It also includes some incorrect statements (e.g., \"current implementation did not achieve competitive results on the shared task\" which is not explicitly mentioned in the other summaries). Overall, it lacks the structured approach and completeness that are in the other summaries.", "expGPT4": "effectively captures the essence of the paper by highlighting the challenge addressed (morphological segmentation), the novelty (use of Transformer models), and the methodological approach (supervised learning at the character level). It provides a concise overview but lacks some details compared to the best summary. It does, however, include key points such as the application of Transformer models and the focus on monolingual word segmentation."}, "22": {"title": "Word Sense Disambiguation with Recurrent Neural Networks", "url": "https://doi.org/10.26615/issn.1314-9156.2017_004", "text": "1 Introduction\nThe task of word sense disambiguation (WSD) has\nbeen defined as follows: \u201dthe ability to computationally determine which sense of a word is activated by its use in a particular context.\u201d (Navigli, 2009) Work on this problem has a long history and it is generally recognized as one of the\nfield\u2019s most difficult tasks, for a variety of reasons\nranging from data sparseness and the difficulty in\nconstructing good lexicons to the inherent complexity of the task itself. Several broad families\nof methods have been proposed: unsupervised,\nsupervised and knowledge-based methods (following the classification in Navigli (2009); note\nthat sometimes these approaches might overlap,\ne.g. semi-supervised ones have also led to some\npromising results (Yuan et al., 2016b)). Among\nthese, supervised systems typically achieve the\nbest results, as is comprehensively shown in Raganato et. al. (2017). Consequently, the challenge to improving WSD is threefold: increasing\nthe amount of available training data, extracting\nricher input features for the learning algorithms,\nimproving the learning algorithms themselves.\nThe task of generating more training data is notoriously difficult and expensive. In contrast to\nother similar classification tasks, such as part-ofspeech tagging, the variety of possible tags and\nword-tag pairs in WSD is enormous (consider the\nfollowing comparison: the popular Penn Treebank POS tagset contains 58 tags, whereas WordNet 3.1, one of the most popular word sense inventories, contains over 117,000 synsets). This\nraises a number of issues, such as data sparseness (some word senses are underrepresented in\nthe data, or not represented at all) and low interannotator agreement (word senses are often too\nfine-grained, which results in contradictory selections done by the human annotators; low interannotator agreement (IAA) is a sign of a theoretical ceiling to what an automated system trained\nand tested on such data could achieve; e.g., Navigli (2009) reports IAA for WordNet-style senses\nto be between 67 and 80%).\nThe two remaining issues are more amenable\nto improvement, especially with recent developments in the use of neural networks for NLP tasks.\nThis work will examine the usage of neural architectures in the context of WSD, as well as\nthe usage of embeddings as input features, thus\ntackling both problems (feature extraction and algorithm improvement). Its contribution is twofold: demonstrating that recurrent neural networks\n(RNNs) are suitable for solving the all-words lexical disambiguation task and exploring new direc25\ntions for generating and using embeddings, with\nrespect to the WSD task. It also briefly outlines an\noption for generating \u201dartificial\u201d data that can be\nused for training word and sense representations,\nor for training the WSD algorithms themselves.\nThe following section provides references to related work in the field that motivates the present\nstudy, then Section 3 describes the neural architecture. Section 4 describes the data used and the\nexperimental setup. Section 5 gives the empirical\nresults. Section 6 concludes the paper and outlines\npossible further work.\n2 Related Work\n2.1 Neural Network Language Models and\nWord Representations\nNeural network language models have driven\na wave of improvements in NLP tasks in recent years, due to a great extent to the widespread use of word representations, also known\nas \u201dword embeddings\u201d. Word embeddings are\nreal-valued vector representations of words in a\nlower-dimensional space (as opposed to, for instance, \u201done-hot\u201d representations, whereby the dimensionality is equal to the lexicon size). The\ntraining of embeddings can be accomplished in\ndifferent ways (e.g. using a feedforward neural\nnetwork as in the pioneering work of Bengio et.\nal. (2003) or using a convolutional one such as in\nCollobert and Weston (2008)).\nOne of the most significant contributions to the\nfield has been Mikolov et. al. (2013), which proposes fast and efficient methods for the training of\nembeddings on large corpora \u2013 using a simple loglinear feedforward network. The goal of the network is to predict a hidden section of the input text\nbased on a visible context (two variants are introduced \u2013 CBOW and Skipgram). The intermediate\nmatrix that performs the embedding in the lowerdimensional space is what is of greatest interest, as\nit provides the mapping from naive one-hot representation to distributed representation of meaning,\nwhich is also much more compact. It is demonstrable through simple arithmetic operations on\nthe representations that they are able to encode\nmeaningful distinctions along different semantic\ndimensions (e.g. number, sex, functional roles,\netc.). Subsequently, other approaches to obtaining\nembeddings from large natural language corpora\nhave been proposed (e.g., see Levy and Goldberg\n(2014) for dependency-based embeddings, Pennington et. al. (2014) for embeddings that reflect\nco-occurrence statistics in a large corpus).\n2.2 Word Embeddings as Features to\nSupervised Systems\nDistributed word representations can be very\nstrong features for supervised models and\nhave been successfully used in WSD systems.\nTaghipour and Zhong (2015) describe a way\nto incorporate word embeddings in a popular\nsupervised system \u2013 IMS (Zhong and Ng, 2010);\nthe addition of the embeddings leads to a significant improvement in accuracy when the system is\ntested on several different datasets. The traditional\nfeatures used by IMS are binary ones that indicate\ninformation about the window-bounded textual\ncontext of the words to be disambiguated (such as\nPOS, lemma, etc); word embeddings are added as\nreal-valued features which do not fit well with the\nbinary ones and therefore the authors use a scaling\nmethod from Turian et. al. (2010). A later study\n(Iacobacci et al., 2016) surveys different methods\nfor integrating embeddings in the IMS system,\nsuch as concatenating or averaging the vectors for\nthe surrounding words or weighing the vectors via\nfractional or exponential decay; the exponential\ndecay method gives very good improvements,\nespecially on the Senseval (SE) lexical sample\ntasks (SE 2, 3 & 7).\n2.3 Recurrent Neural Networks for Word\nSense Disambiguation\nThe IMS system discussed above uses an SVM algorithm to perform the disambiguation. A sensible\nquestion is \u2013 are neural networks a good alternative for WSD? This overview will focus on several\nneural approaches proposed recently.\nOne important development has been the adoption of recurrent neural networks (RNNs) as a viable tool for modeling language. RNNs are similar to deep feedforward networks, with the major difference that the hidden (context) layers have\ncyclic connections to themselves, which allows\nthem to maintain a dynamic memory of their previous states, as the latter change in time. This\nability to keep a memory trace of past \u201devents\u201d at\ntheoretically arbitrary distances from the present\nis an obvious advantage over algorithms that collect information from a fixed window around the\ntarget word. Especially in the case of more complex tasks such as WSD, vital information is often\nfound at the other end of the time series (i.e. the\n26\nsentence; sometimes the disambiguation of a word\nmight require going back even before a sentence\nboundary, and on rare occasions it might even depend on looking forward and beyond the current\nsentence).\nFor a long time RNNs were considered difficult to train, as their memory capabilities are often thwarted in practice by the phenomena known\ncollectively as the exploding/vanishing gradients\nproblem \u2013 the fact that with long time series the\nbackpropagated error gradients often grow too\nlarge or too small. While the exploding gradients\npart can be solved trivially by capping their values, a more elaborate solution was needed for the\nvanishing part. Long Short Term Memory (LSTM)\ncells (Hochreiter and Schmidhuber, 1997) were\ndeliberately designed to be able to selectively forget (parts of) old states and pay attention to new\ninputs (a good introduction to LSTMs is Graves\n(2012); another similar and newer development\nare Gated Recurrent Units (Cho et al., 2014)).\nA further and simple enhancement to such an\narchitecture is making it bidirectional (Graves and\nSchmidhuber, 2005), i.e. the input sequence is fed\ninto two recurrent context layers \u2013 one running\nfrom the beginning to the end of the sequence and\nthe other one running in reverse. Their outputs\nare concatenated and thus encompass forward as\nwell as backward-looking context. Bidirectional\nLSTMs (Bi-LSTMs) have been successfully applied to a number of sequence-to-sequence tasks\nin NLP, such as part-of-speech tagging, chunking, named entity recognition, dependency parsing ((Wang et al., 2015a), (Wang et al., 2015b),\n(Huang et al., 2015), (Wang and Chang, 2016)).\nRNNs have been used in several ways for WSD.\nOne such work is Kageb \u02da ack and Salomonsson \u00a8\n(2016), which uses a Bi-LSTM to solve the lexical sample tasks of Senseval-2 (Kilgarriff, 2001)\nand Senseval-3 (Mihalcea et al., 2004) \u2013 that is,\nthe model is disambiguating one word per sentence only. To that purpose, the output of the BiLSTM at the target word position is fed upstream\nfor the classification part; it takes into consideration both the left and the right contexts. It is\nreshaped through a lemma-specific hidden layer,\nso that classification between the possible senses\nfor the lemma can be carried out. In this way\nthe model shares the Bi-LSTM parameters across\nwords and is updated globally with every training\ncase, but is parametrized for specific lemmas. The\nmodel is on par (or slightly better) with state-ofthe-art systems, but uses no other features apart\nfrom the word embeddings.\nAnother approach to WSD that also uses RNNs\nis via calculating vector similarities. The goal of\nsuch models is to calculate a distributed representation both of the target sense and of the context\nwithin which it appears. Upon doing that, some\nsimilarity measure, such as cosine distance, can\nbe used to determine which of the possible word\nsenses for the target word is closest to the context representation. Such models are typically not\nsupervised, because they do not update their parameters against training data annotated with word\nsense information; however, they do rely heavily\non a pre-training procedure that enables the network to represent contexts as embeddings. RNNs\nare useful in this case, because they can capture\nwell syntactic information, as opposed to bag-ofword approaches. The embeddings for the different word senses are obtained by running the example sentences for them (e.g. those supplied in their\nWordNet entries) through the RNN. Naturally, the\nmore data that is available, the better sense representations can be built. Examples of such models\ncan be found in Yuan et. al. (2016a), Melamud et.\nal. (2016) and Le and Mikolov (2014).\nThe current work explores both RNN-based approaches to WSD outlined above, with respect to\nthe all-words lexical task.\n2.4 Relational Knowledge and Generation of\nEmbeddings\nThis subsection of the literature review will take\ninto account a last constellation of ideas that are\nalso important to the currently presented work.\nAs was already discussed, word embeddings are\nvery effective as features to supervised models.\nWord embeddings are typically generated by training models on large amounts of unlabeled natural\nlanguage. But other sources of information, such\nas, for instance, dependency paths, can also yield\nuseful training signals.\nOne innovative approach to the embedding of\nwords is Goikoetxea et. al. (2015). That work\nuses a knowledge base (more specifically WordNet) to generate an artificial (or pseudo) corpus,\nwhich is then used to generate embeddings. The\npseudo-corpus is created by utilizing WordNet\u2019s\nrelational structure (including relations such as hypernymy, synonymy, antonymy, derivation, etc.) \u2013\nmore specifically by traversing the semantic network and emitting a word sense identifier for each\n27\nnode in the graph that is visited. The traversal\nis accomplished via the Pagerank algorithm (Page\net al., 1999), as implemented in the UKB tool\n(Agirre and Soroa, 2009). Several million \u201drandom walks\u201d along the graph are produced in this\nway and each node along these sequences is replaced with a representative lemma (taken from\nthe WordNet entry for the sense). The pseudocorpus is then fed in the Word2Vec tool, which\nembeds the lemmas in a lower-dimensional space.\nThe embeddings are tested against a set of word\nembeddings taken from Mikolov et. al. (2013) \u2013\non popular similarity and relatedness datasets. The\nexperiments show them to be very competitive and\neven more effective in some cases.\nSimov et. al. (2017) builds on this work\nby enriching the knowledge graph with different\ntypes of relations, such as syntagmatic relations\nextracted from the WordNet glosses and inference over the hypernymy relations already present\nthere. Some of those expanded graphs yield embeddings that further improve the performance on\nthe similarity and relatedness tasks. The results\nsuggest that knowledge-based approaches to generating data are viable alternatives to working with\nhuge amounts of text (often amounting to hundreds of billions of tokens). Goikoetxea et. al.\n(2016) in turn demonstrates that word representations learned in this way can be viewed not simply as an alternative, but as a complement to embeddings learned from distributional knowledge.\nEven simple methods of combining distributional\nand relational-based embeddings, such as vector\nconcatenation, are show to improve accuracy for\nthe similarity and relatedness tasks. This line of\nwork is also valuable due to the possibilities it\nopens for training genuine sense embeddings.\nRegarding the generation of sense embeddings,\nit is worth mentioning a few more attempts at obtaining such vector representations that go beyond\naveraging the word embeddings for representative\nsentences. Chen et. al. (2014) obtain their sense\nembeddings from the sense glosses, but they filter out some of the words in the definition (functional words and such that fail a cosine similarity\ncomparison with the lemma in question). Johansson and Pina ( \u02dc 2015b; 2015a) present a method of\nembedding the senses together with the lemmas\nfor the WordNet entries which minimizes a metric of neighborhood calculated on the basis of the\nrelational structure in WordNet, with the lemma\nembeddings being represented as convex combinations of the different possible senses. Rothe\nand Schutze ( \u00a8 2015) derive sense and lexeme embeddings from word embeddings through autoencoders; their solution does not require any additional training resources beyond the word embeddings. The availability of word/lemma embeddings in the same space with sense embeddings\nopens up exciting possibilities for doing WSD,\nsome of which are explored in this work.\n3 Neural Network Architecture for WSD\n3.1 Network Implementation\nThe architecture implemented for this study\nemploys a Bi-LSTM context layer, similar to\nKageb \u02da ack and Salomonsson ( \u00a8 2016). Unlike that\nmodel, however, it is designed to perform disambiguation for all open-class words in a single context. This means that an input sequence is fed one\ntoken at a time into the Bi-LSTM and for each target word the output of the context layer is passed\nupstream in order to eventually feed a classification layer. That is, this architecture\u2019s context layer\nproduces a number of outputs per sentence, much\nlike in other sequence-to-sequence tasks such as\nPOS tagging, as opposed to just one representation for a specific word or for the whole context.\nThe architecture is represented graphically in\nFigure 1. The input sequence of words are preprocessed from string to integer format, where each\ninteger corresponds to a position in an embedding\nmatrix. A parameter setting allows some of the\nwords in the training sequences to be dropped,\nwhich should hypothetically reduce overfitting\n(this feature is a modified version of dropword\nfrom Kageb \u02da ack and Salomonsson ( \u00a8 2016); however, instead of replacing randomly selected words\nwith a hdroppedi tag, words here are directly\ndeleted from the training input). The integer inputs\nare then fed into an embedding layer that selects\nthe corresponding vectors (this layer is also trainable, i.e. the embeddings continue to adapt as the\nWSD model is being trained and could be saved\nand reused in other tasks that would benefit from\nsuch adaptation). The network is parametrized to\nbe able to access two parallel embedding layers, so\nthat embeddings for the same words but from separate sources can be combined at this stage (via\nsimple concatenation).\nConsequently, the input vectors are fed into\na Bi-LSTM layer that runs them sequentially\nthrough itself, analyzing the time series simul28\nFigure 1: Recurrent neural network for word sense\ndisambiguation: The dotted lines mean that a component or a connection is optional (in the case\nof concatenating embeddings from two different\nsources).\ntaneously from-beginning-to-end and from-endto-beginning. Dropout (Srivastava et al., 2014)\ncan be added to both the input and the output of\nthe forward and backward cells of the Bi-LSTM;\nthe network can have an arbitrary number of BiLSTM layers stacked on top of each other. The\noutputs of the context layer are filtered out in order to extract just the ones corresponding to target\nwords. These are then put through a linear hidden\nlayer that transforms each input (dimension = 2 *\nnumber of neurons in each LSTM cell) into a large\nvector with one position per every word sense that\ncorresponds to a lemma seen in the training data.\nThere is a softmax layer on top, which creates a\nprobability distribution over the word sense lexicon. Finally, cross entropy loss is calculated between this vector and the gold label and the mean\nvalue for the batch is passed to the optimizer. The\nimplementation is written within the Tensorflow\nframework (Abadi et al., 2016).\n3.2 WSD Features\nThe model uses no hand-crafted features during\nthe training process, all the information comes\nfrom the embeddings that are loaded as the first\nlayer of the network. As principal features,\nthe GloVe embeddings were used (Pennington\net al., 2014), more specifically the set trained on\nWikipedia and Gigaword (6 billion tokens text, vocabulary of 400K words, uncased, vector dimensionality is 300). It remains as a future task to test\nthe other available options, some of which have\nbeen trained on far larger corpora, as well as other\ncorpora-based word representation methods altogether.\nThe option for a second embedding layer, parallel to the default one, was added so that the ideas\npresented in subsection 2.4 could be tested in such\na setup. An extension to WordNet was selected\non the basis of Simov et. al. (2017) \u2013 the socalled \u201dWN30WN30glConOne\u201d, one of the best\nperforming graphs on the similarity and relatedness tasks. The new relations in it are constructed\non the basis of co-occurring word sense annotations in the WordNet glosses, taken from eXtended\nWordNet (Mihalcea and Moldovan, 2001); for a\nmore detailed description, see Simov et. al (2016).\nThe UKB1\ntool was used with the reported settings to replicate this pseudo-corpus (as described\nin subsection 2.4, producing 500 million random\nwalks along the graph structure. Then Word2Vec2\nwas used to create lemma embeddings based on\nthe artificial corpus, again using the settings indicated in the referenced article:\n\u2022 Size of the embeddings = 300\n\u2022 Number of training iterations = 7\n\u2022 Number of negative examples = 5\n\u2022 Window size = 15\n\u2022 Threshold for occurrence of words = 1e-7\n\u2022 Algorithm = Skipgram\nThis method provides representations only for\nlemmas and only for open-class words. Therefore a lot of the input words (mostly functional\nones) do not have matching vectors. And because embeddings are created only with respect\nto lemmatized input, morphological information\nis largely disregarded. However, if such pseudocorpora truly generate information that can complement what is learned from large natural language corpora, there is a plausible hypothesis that\nsuch an embedding could improve the accuracy of\na WSD system. The combination method (concatenation) was chosen because it is easy to implement and because of the evidence provided in\nGoikoetxea et. al. (2016) that it gives adequate\nresults compared to more complex ones.\n1\nhttp://ixa2.si.ehu.es/ukb/\n2\nhttps://code.google.com/archive/p/word2vec/\n29\n3.3 Artificial Corpora with Mixed Lemmas\nand Word Senses\nBy employing the procedure described in subsection 3.2, one can generate a pseudo-corpus that is a\nmix of word sense IDs and representative lemmas\n(this can be done easily via one of the parameters\nprovided by the UKB tool). Based on the previous discussion of the literature, such a corpus can\nserve for at least two purposes:\n1. In order to embed lemmas and word senses\nsimultaneously in the same space. These representations can be used in approaches such\nas those discussed in subsection 2.3 \u2013 for calculating context embeddings and comparing\nthem with sense embeddings.\n2. In order to generate artificial training data for\nsupervised WSD systems. The fact that such\ncorpora can be used as training data for the\ngeneration of meaningful embeddings, suggest that they could have some worth as input\nto WSD models as well.\nNaturally, such a corpus would be very noisy,\nespecially if it is to be used as training data for\nWSD. The proposed method is quite naive in\nthat lemmas are picked randomly to substitute the\nsense IDs, but more sophisticated strategies can\nbe devised as well. For instance, the glosses for\nthe WordNet synsets can be added automatically\nnext to the IDs generated by UKB. In this way\nmore meaningful linguistic data will be intermixed\nwith the (semi-) random walks performed by the\nPagerank algorithm. This would however require\na rethinking of the Word2Vec parameters used, as\ndistances between individual sense IDs are likely\nto increase substantially (the gloss annotations in\nMihalcea et. al. (2001) can be used to ameliorate this problem). This approach will allow for\nthe learning of embeddings for functional words\nas well; otherwise there would be no information\nabout them that could be passed to the RNN.\nHere, a tentative modification to the already described architecture is proposed that makes use of\nsuch an artificial corpus and the lemma/sense representations generated from it. This work is in its\nearliest stages, but could potentially connect in a\nmeaningful way to the rest of the ideas discussed\nhere. The modification is simple \u2013 instead of mapping the RNN outputs to a huge lexicon-sized vector, the final hidden layer produces a vector the\nsize of the sense embeddings. No softmax is applied thereafter, but the sense embedding for the\ngold label word sense is compared with the one\nproduced by the network (different cost functions\ncan be used to that purpose, such as cosine distance, etc; a naive one is used here \u2013 least squares).\nThis training signal is used to optimize the parameters of the network, so that it can learn how\nto calculate context embeddings for specific target words in a more informed manner. The disambiguation is done by picking the closest sense, via\ncosine similarity.\nSuch a method has the advantage that it optimizes on all of the semantic dimensions encoded\nby the sense embeddings \u2013 i.e. with each training\ncase, the network should be learning important information about the whole semantic space, not just\nabout a single label that it aims to pick amongst the\nrest in the lexicon. To test this solution, a mixed\ncorpus of size 200 million random walks was produced, where the probability of emitting a sense\nID or a lemma is evenly split. The same Word2Vec\nsettings were used as the ones reported above for\nthe lemma embeddings.\n6 Conclusion and Future Work\nThis work has presented a supervised architecture for WSD using recurrent neural networks,\ntested on the all-words lexical task. The model\nis inspired by the very successful recent applications of LSTM cells to NLP problems. The article also explores the utility of combining word\nembeddings learned from a large corpus of text\nwith lemma embeddings learned from an \u201dartificially generated\u201d corpus based on a knowledge resource (WordNet). A comparison with the bestscoring systems on a popular evaluation dataset\nshows that the neural network is well-positioned\nwith respect to them. The fact that the addition\nof the lemma embeddings from the pseudo-corpus\nimproves significantly the results, signals that they\ncould be further boosted by exploring different\nfeature spaces and combinations of them.\nThere are a number of changes and additions\nthat could be undertaken to improve the WSD algorithm. Further parameter optimization is naturally among them, as is the evaluation of other\nsets of word representations and the generation of\nimproved \u201dartificial corpora\u201d. Another improvement that could boost accuracy scores and for\nwhich RNNs are naturally suited, is allowing the\nmodel to store a representation for the previously\nseen sentences, i.e. to disambiguate words using\nwhole texts as context, rather than just separate\nsentences. While this would not matter in tasks\nsuch as POS tagging and syntactic parsing, it is\nvery important with regards to WSD.\nAnd finally, a network could be trained to\njointly solve two related tasks: the WSD one and\nthat of adapting context embeddings to word sense\nembeddings (as discussed in 3.3). Since the similarity task is optimizing the RNN with respect to\nall semantic dimensions of the embedding space,\nthe classification pathway should be able to directly benefit from that. This provides further\nmotivation for improvements in the generation of\nless naive \u201dmixed\u201d corpora. It also remains to be\ntested whether such pseudo-corpora can be successfully used as training data for the supervised\nWSD systems themselves (as complements to expensive corpora such as SemCor).\n", "SumGPT3": "Introduction\nWord sense disambiguation (WSD) is a challenging task in natural language processing (NLP) aimed at determining the intended meaning of a word based on its context. The paper explores the use of recurrent neural networks (RNNs), particularly bidirectional long short-term memory networks (Bi-LSTMs), for solving the all-words WSD task. It focuses on leveraging neural architectures and word embeddings to enhance WSD performance, addressing the issues of feature extraction and algorithm improvement.\n\nNovelty\nBi-LSTM Context Layer for All-words Disambiguation:\n\nThe paper presents a Bi-LSTM architecture designed to perform disambiguation for all open-class words within a single context, unlike previous models that focused on specific words per sentence.\nThe Bi-LSTM layer processes input sequences and generates outputs for each target word in the context, similar to sequence-to-sequence tasks like part-of-speech tagging.\nParallel Embedding Layers:\n\nThe architecture incorporates two parallel embedding layers to combine embeddings from different sources, enhancing the model's feature set.\nIt uses GloVe embeddings alongside lemma embeddings derived from a pseudo-corpus generated via WordNet's relational structure.\nArtificial Corpora with Mixed Lemmas and Word Senses:\n\nThe paper proposes generating a pseudo-corpus mixing word sense IDs and representative lemmas to create embeddings that capture both lexical and semantic information.\nThese embeddings are tested as features for WSD models, with the hypothesis that they can complement traditional corpus-based embeddings.\nOptimization on Semantic Dimensions:\n\nThe proposed model includes a novel method of training on context embeddings optimized across all semantic dimensions, improving the overall semantic understanding of the embeddings.\nPotential for Joint Task Training:\n\nThe paper suggests future improvements by training the network to solve WSD jointly with the task of adapting context embeddings to word sense embeddings, leveraging the optimization of all semantic dimensions.\nConclusion\nThe paper demonstrates the effectiveness of using Bi-LSTMs for WSD and highlights the benefits of combining traditional corpus-based word embeddings with those derived from knowledge-based pseudo-corpora. It provides a promising direction for enhancing WSD performance through neural network architectures and innovative use of embeddings, suggesting further optimizations and joint task training as future work.", "SumGem": "## Summary of the Scientific Paper on Word Sense Disambiguation using Recurrent Neural Networks\n\n**Introduction**\n\nThis paper proposes a new approach to word sense disambiguation (WSD) using recurrent neural networks (RNNs). The task of WSD is to determine the specific meaning of a word used in a particular context. This paper focuses on the all-words lexical task, where the model needs to disambiguate all open-class words in a sentence.\n\n**Novelty**\n\nThe paper explores several novel aspects in its approach to WSD using RNNs:\n\n* **Combining word embeddings from different sources:** The model utilizes two embedding layers, allowing the combination of embeddings learned from a large text corpus (e.g., GloVe) with embeddings generated from a knowledge resource like WordNet.\n* **Using a pseudo-corpus for generating lemma embeddings:** The paper introduces a method for creating a pseudo-corpus based on WordNet's relational structure. This corpus is used to generate embeddings for lemmas, which are then combined with word embeddings from a text corpus.\n* **Training on a mixed corpus of word senses and lemmas:** The paper proposes using a pseudo-corpus containing both word senses and lemmas for training the RNN. This allows the model to learn representations for both senses and lemmas in the same space.\n* **Adapting context embeddings to word sense embeddings:** The paper suggests a training approach where the RNN learns to adapt context embeddings to word sense embeddings. This allows the model to optimize for all semantic dimensions encoded by the sense embeddings.\n* **Potential for using pseudo-corpora as training data for WSD systems:** The paper explores the possibility of using the generated pseudo-corpus directly as training data for supervised WSD systems.\n\n**Conclusion**\n\nThe paper demonstrates that the proposed RNN-based approach with combined embeddings achieves competitive results on a benchmark WSD dataset. The inclusion of lemma embeddings generated from a pseudo-corpus significantly improves the model's performance. The paper highlights several promising avenues for future research, including further exploration of different feature spaces and corpus generation methods for improved WSD accuracy.\n", "SumGPT4": "**Introduction:**\n\nThe paper addresses the challenging task of Word Sense Disambiguation (WSD), which involves determining the correct sense of a word based on its context. WSD is particularly difficult due to issues like data sparsity and the complex nature of constructing comprehensive lexicons. Traditional approaches to WSD include unsupervised, supervised, and knowledge-based methods, with supervised methods generally achieving superior results. This paper explores the potential of neural architectures, specifically recurrent neural networks (RNNs), and the use of word and sense embeddings to enhance WSD.\n\n**Novelty:**\n\n1. **Recurrent Neural Networks for All-Words WSD:** The paper introduces a Bi-LSTM architecture designed to disambiguate all open-class words in a single context rather than focusing on a single word per sentence. This approach differs from previous methods that use RNNs for specific words, leveraging bidirectional LSTMs to process context dynamically and improve accuracy across entire sentences.\n\n2. **Combining Corpus-Based and Knowledge-Based Embeddings:** The study investigates the combination of traditional corpus-based word embeddings (e.g., GloVe) with embeddings generated from a pseudo-corpus derived from WordNet's relational structure. This method, which combines corpus-derived and knowledge-based embeddings, aims to enhance feature richness and improve disambiguation.\n\n3. **Use of Artificially Generated Corpora:** The paper proposes the generation of artificial corpora by mixing word sense identifiers and representative lemmas from WordNet using the UKB tool. This pseudo-corpus is used to create embeddings that serve as additional training data, demonstrating potential in complementing traditional training data for WSD.\n\n4. **Direct Comparison and Optimization in Embedding Space:** A novel modification to the RNN architecture is suggested, where the final layer produces vectors directly compared to sense embeddings in the same space. This approach aims to optimize the network against the entire semantic space, rather than individual sense labels, potentially improving the learning of semantic dimensions.\n\n**Conclusion:**\n\nThe paper presents a Bi-LSTM-based model for WSD that handles all-words disambiguation and shows that combining embeddings from natural and artificial corpora can significantly boost performance. The study highlights the promise of integrating knowledge-based embeddings and suggests that further improvements can be achieved through optimizing pseudo-corpora generation and allowing models to utilize broader contexts beyond single sentences. These innovations offer a path toward more effective and resource-efficient WSD systems.", "rankGPT3": 2, "rankGem": 3, "rankGPT4": 1, "expGPT3": "This summary is also well-structured and covers the introduction, novelty, and conclusion sections adequately. It discusses the challenges of WSD and the paper's approach using Bi-LSTMs for all-words WSD effectively. It highlights the novelty in combining corpus-based and knowledge-based embeddings, using pseudo-corpora, and optimizing embeddings in the same space as sense embeddings. However, it could benefit from more detailed explanations in certain areas and lacks a bit of depth.", "expGem": "This summary is ranked the lowest primarily due to its brevity and lack of detailed explanation compared to the other summaries. While it touches on some key points such as the use of Bi-LSTM, combining embeddings, and using artificial corpora, it doesn't delve deeply into how these methods contribute to the novelty of the paper or their impact on WSD performance. It could be improved by expanding on specific innovations and providing more context around the methods used in the paper.", "expGPT4": "This summary is ranked the highest because it provides a comprehensive overview of the paper's key contributions and innovations in a clear and structured manner. It covers the introduction, novelty, and conclusion of the paper effectively. It details the use of Bi-LSTM for all-words WSD, the integration of different types of embeddings (corpus-based and knowledge-based), the creation and use of pseudo-corpora for training, and optimization strategies for embedding spaces. The conclusion summarizes the findings and future research directions succinctly. It provides a balanced view of the entire paper's scope and contributions."}, "23": {"title": "Joint Opinion Relation Detection Using One-Class Deep Neural Network", "url": "https://aclanthology.org/C14-1064", "text": "1 Introduction\nOpinion summarization aims to extract and summarize customers\u2019 opinions from reviews on products or\nservices (Hu and Liu, 2004; Cardie et al., 2004). With the rapid expansion of e-commerce, the number of\nonline reviews is growing at a high speed, which makes it impractical for customers to read throughout\nlarge amounts of reviews to choose better products. Therefore, it is imperative to automatically generate opinion summarization to help customers make more informed purchase decisions, where detecting\nopinion relation is a crucial step for opinion summarization.\nBefore going further, we first introduce some notions. An opinion relation, is a triple o = (s, t, r),\nwhere three factors are involved: s is an opinion word which refers to those words indicating sentiment\npolarities; t is an opinion target, which can be any entity or aspect of an entity about which an opinion has\nbeen expressed; r refers to the linking relation between s and t. As in Example 1, s={clear}, t={sceen},\nand there is a linking relation between the two words because clear is used to modify screen.\nExample 1. This mp3 has a clear screen.\nFor a valid opinion relation, there are three requirements corresponding to the three factors: (i) the\nopinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) the\nopinion word modifies the opinion target. Previous weakly supervised methods often expand a seed set\nand identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) or\nsyntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below.\nAssumption 1. Terms that are likely to have linking relation with the seed terms are believed to be\nopinion words or opinion targets.\nFor example, if one has an opinion word seed clear (which satisfies requirement i), and one finds that\nit modifies the word screen in Example 1 (which satisfies requirement iii). Then one infers that screen\nis an opinion target according to Assumption 1 (whether screen is correct is not checked). However, in\nExample 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related to\nThis work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer\nare added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/\n677\nmp3 domain. If one follows Assumption 1, thing will be mistaken as an opinion target. Similarly, in\nExample 2(b), if one uses mp3 to extract another as an opinion word, he may get an objective word.\nExample 2. (a) This mp3 has many good things. (b) Just another mp3 I bought.\nThe reason for the errors above is that Assumption 1 only verifies two requirements for an opinion\nrelation. Unfortunately, this issue occurs frequently in online reviews. As a result, previous methods\noften suffer from these noise terms. To produce more precise opinion summary, it is argued that we shall\nfollow a more restricted assumption as follows.\nAssumption 2. The three requirements: the opinion word, the opinion target and the linking relation\nbetween them, shall be all verified during opinion relation detection.\nTo make accordance with Assumption 2, this paper proposes a novel joint opinion relation detection\nmethod, where opinion words, opinion targets and linking relations are simultaneously considered in a\nclassification scenario. Following previous works, we provide a small set of seeds (i.e. opinion words\nor targets) for supervision, which are regarded as positive labeled examples for classification. However,\nnegative labeled examples (i.e. noise terms) are hard to acquire, because we do not know which term\nis not an opinion word or target. This leads to One-Class Classification (OCC) problem (Moya et al.,\n1993). The key to OCC is semantic similarity measuring between terms, and Deep Neural Network\n(DNN) with word embeddings is a powerful tool for handling this problem. We consequently integrate DNN into a OCC classifier and develop a One-Class Deep Neural Network (OCDNN). Concretely,\nopinion words/targets/relations are first represented by embedding vectors and then jointly classified.\nExperimental results show that the proposed joint method which follows Assumption 2 significantly\noutperforms state-of-the-art weakly supervised methods which are based on Assumption 1.\n2 Related Work\nIn opinion relation detection task, previous works often used co-occurrence statistics or syntax information to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a\npioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) defined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features.\nHai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests\n(LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff\nand Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang\net al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for\nuseful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double\nPropagation which introduced eight heuristic syntactic rules to detect opinion relations.\nHowever, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of\nwhich employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM\n(Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference\nfor opinion extraction (Choi et al., 2006; Yang and Cardie, 2013).\nJoint methods had been shown to achieve better performance than pipeline approaches. Nevertheless,\nmost existing joint models rely on full supervision, which have the difficulty of obtaining annotated\ntraining data in practical applications. Also, supervised models that are trained on one domain often fail\nto give satisfactory results when shifted to another domain. Our method does not require annotated data.\n3 The Proposed Method\nTo detect opinion relations, previous methods often leverage some seed terms, such as opinion word\nseeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Hai\net al., 2012). These seeds can be used as positive labeled examples to train a classifier. However, it is\nhard to get negative labeled examples for this task. Because opinion words or targets are often domain\n678\ndependent and words that do not bear any sentiment polarity in one domain may be used to express\nopinion in another domain. It is also very hard to specify in what case there is no linking relation\nbetween two words.\nTo deal with this problem, we employ one-class classification, and develop a One-Class Deep Neural\nNetwork (OCDNN) for opinion relation detection. The architecture of OCDNN is shown in Figure 1,\nwhich consists of two levels. The lower level learns feature representations unsupervisedly for opinion\nwords/targets/relations, where the left component uses word embedding learning to represent opinion\nwords/targets, and the right component maps linking relations to embedding vectors by a recursive autoencoder. Then the upper level uses the learnt features to perform one-class classification.\nFigure 1: The architecture of OCDNN. Figure 2: An example of recursive autoencoder.\n3.1 Opinion Seed Generation\nTo obtain training data for OCDNN, we shall first get some seed terms as follows.\nOpinion Word Seeds. We manually pick 186 domain independent opinion words from SentiWordNet\n(Baccianella et al., 2010) as the opinion word seed set SS.\nOpinion Target Seeds. Likelihood Ratio Tests (LRT) (Dunning, 1993) used in (Hai et al., 2012) is\nemployed to generate opinion target seeds. LRT aims to measure how greatly two terms Ti and Tj are\nassociated with each other by sentence-level corpus statistics which is defined as follows,\nLRT = 2[log L(p1, k1, n1) + log L(p2, k2, n2) \u2212 log L(p, k1, n1) \u2212 log L(p, k2, n2)] (1)\nwhere k1 = tf(Ti\n, Tj ), k2 = tf(Ti\n, T\u00af\nj ), k3 = tf(T\u00af\ni\n, Tj ), k4 = tf(T\u00af\ni\n, T\u00af\nj ), tf(\u00b7) denotes term frequency;\nL(p, k, n) = p\nk\n(1 \u2212 p)\nn\u2212k\n, n1 = k1 + k3, n2 = k2 + k4, p1 = k1/n1, p2 = k2/n2 and p =\n(k1 + k2)/(n1 + n2). We measure LRT between a domain name (e.g. mp3, hotel, etc.) and all opinion\ntarget candidates. Then N terms with highest LRT scores are added into the opinion target seed set T S.\nLinking Relation Seeds. Linking relation can be naturally captured by syntactic dependency, because\nit directly models the modification relation between opinion word and opinion target. We employ an\nautomatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013)\nand get 12 opinion patterns with highest confidence as the linking relation seed set RS.\nAfter seed generation, every opinion relation so = (ss, st\n, sr) in review corpus that satisfies ss \u2208 SS,\nst \u2208 T S and sr \u2208 RS is taken as a positive labeled training instance.\n3.2 Opinion Relation Candidate Generation\nThe opinion term candidate set is denoted by C = {SC, T C}, where SC/T C represents opinion\nword/target candidate. Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu\net al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phrases as\nopinion target candidates. A statistic-based method in Zhu et al. (2009) is used to detect noun phrases.\nAn opinion relation candidate is denoted by co = (cs, ct\n, cr), where cs \u2208 SC, ct \u2208 T C, and cr is a\npotential linking relation. To get cr, we first get dependency tree of a sentence using Stanford Parser (de\n679\nMarneffe et al., 2006). Then, the shortest dependency path between a cs and a ct\nis taken as a cr. To\navoid introducing too many noise candidates, we constrain that there are at most four terms in a cr.\n3.3 Word Representation by Word Embedding Learning\nWord embedding, a.k.a word representation, is a mathematical object associated with each word, which\nis often used in a vector form, where each dimension\u2019s value corresponds to a feature and might even\nhave a semantic or grammatical interpretation (Turian et al., 2010). By word embedding learning, words\nare embedded into a hyperspace, where two words that are more semantically similar to each other are\nlocated closer. This characteristic is precisely what we want, because the key to one-class classification\nis semantic similarity measuring (illustrated in Section 3.5).\nFor word representation, we use a matrix LT \u2208 R\nn\u00d7|Vw|\n, where i-th column represents the embedding\nvector for term ti\n, n is the size of embedding vector and Vw is the vocabulary of LT. Therefore, we\ncan denote ti by a binary vector bi \u2208 R\n|Vw|\nand get its embedding vector by xi = LT bi\n. The training\ncriterion for word embeddings is,\n\u02c6\u03b8 = argmin\n\u03b8\nX\nc\u2208C\nX\nv\u2208Vw\nmax{0, 1 \u2212 s\u03b8(c) + s\u03b8(v)} (2)\nwhere \u03b8 is the parameters of neural network used for training. See Collobert et al. (2011) for the detailed\nimplementation.\n3.4 Linking Relation Representation by Using Recursive Autoencoder\nThe goal of this section is to represent the linking relation between an opinion word and an opinion target\nby a n-element vector as we do during word representation. Specifically, we combine embedding vectors\nof words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntactic\ndependency structure. In this way, linking relations are no longer limited to the initial seeds during\nclassification, because linking relations that are similar to the seed relations will have similar vector\nrepresentations.\nFigure 2 shows a linking relation representation process by an example: too loud to listen to the player.\nFirst, we get its dependency path between the opinion word cs:loud and the opinion target ct:player.\nThen cs and ct are replaced by wildcards [SC] and [TC] because they are not concerned in the linking\nrelation. The dash line box in Figure 2 shows a standard autoencoder, which is a three-layer neural\nnetwork, where the number of nodes in input layer is equal to that of output layer. It takes two n-element\nvectors as input and compresses semantics of the two vectors into one n-element vector in hidden layer\nby,\ny = f(W(dep)\n[x1; x2] + b), W(dep) =\n1\n2\n[I1; I2; Ib] + \u000f (3)\nwhere [x1; x2] is the concatenation of the two input vectors and f is the sigmoid function; W(dep)\nis a\nparameter matrix that is chosen according to the dependency relation between x1 and x2 (In the case of\ny1, W(dep) = W(xcomp)\n), which is initialized by Ii\n, where Ii\nis a n \u00d7 n unit matrix, Ib is a n-element\nnull vector, and \u000f is sampled from a uniform distribution U[\u22120.001, 0.001] (Socher et al., 2013). Then\nW(dep)\nare updated during training. The training criterion of autoencoder is to minimize Euclidean\ndistance between the original input and its output,\nErae = ||[x1; x2] \u2212 [x\n0\n1\n; x\n0\n2\n]||2\n(4)\nwhere [x\n0\n1\n; x\n0\n2\n] = W(out)y and W(out)\nis initialized by W(dep)\nT\n.\nWe always start the combination process from [SC] and it is repeated along the dependency path. For\nexample, the result vector y1 of the first combination is used as the input vector when computing y2.\nFinally, the linking relation is represented by a n-element vector (the green vector in Figure 2).\n680\n3.5 One-Class Classification for Opinion Relation Detection\nWe represent an opinion relation candidate co = (cs, ct\n, cr) by a vector vo = [vs; vt\n; vr], which is\na concatenation of the opinion word embedding vs, the opinion target embedding vt and the linking\nrelation embedding vr. Then vo is feed to the upper level autoencoder in Figure 1.\nTo perform one-class classification, the number of nodes in the hidden layer of the upper level autoencoder is constrained to be smaller than that of the input layer. By using such a \u201cbottleneck\u201d network\nstructure, characteristics of the input are first compressed into the hidden layer and then reconstructed\nby the output layer (Japkowicz et al., 1995). Concretely, characteristics of positive labeled opinion relations are first compressed into the hidden layer, and then the autoencoder should be able to adequately\nreconstruct positive instances in the output layer, but should fail to reconstruct negative instances which\npresent different characteristics from positive instances. Therefore, the detection of opinion relation is\nequivalent to assessing how well a candidate is reconstructed by the autoencoder. As the input vector\nvo consists of representations for opinion words/targets/relations, characteristics of the three factors are\njointly compressed by one hidden layer. Either false opinion word/target/relation will lead to failure of\nreconstruction. Consequently, our approach follows Assumption 2.\nFor opinion relation detection, candidates with reconstruction error scores that are smaller than a\nthreshold \u03d1 are classified as positive. Determining the exact value of \u03d1 is very difficult. Inspired by other\none-class approaches (Liu et al., 2002; Manevitz and Yousef, 2007), we introduce some negative opinion\nterms to help to estimate \u03d1.\n1 Although negative instances are hard to acquire, Xu et al. (2013) show that\na set of general nouns (such as thing, one, etc., we denote them by GN) seldom appear to be opinion\ntargets. One the other hand, we create a 50-opinion-word validation set SV from SentiWordNet.\nTo estimate \u03d1, we first introduce a positive proportion (pp) score,\npp(t) = tf +(t)/tf(t), t \u2208 P E, P E = {co|Er(co) < \u03d1} (5)\nwhere P E denotes the opinion relations that are classified as positive, Er(\u00b7) is the reconstruction error\nof OCDNN and tf +(\u00b7) is the frequency of term in P E. Then an error function E\u03d1 is minimized, which\nbalances between the proportion of non-target terms (GN) in P E (which shall be as small as possible)\nand the proportion of opinion words in validation set (SV ) in P E (which shall be as large as possible).\nE\u03d1 =\nX\nt\u2208GN\u2229P E\n[pp(t) \u2212 0]2 +\nX\ns\u2208SV \u2229P E\n[pp(s) \u2212 1]2\n(6)\n3.6 Opinion Target Expansion\nWe apply bootstrapping to iteratively expand opinion target seeds. It is because the vocabulary of seed\nset is limited, which cannot fully represent the distribution of opinion targets. So we expand opinion\ntarget seeds in a self-training manner to alleviate this issue. After training OCDNN, all opinion relation\ncandidates are classified, and opinion targets are ranked in descent order by,\ns(t) = log tf(t) \u00d7 pp(t). (7)\nThen, top M candidates are added into the target seed set T S for the next training iteration.", "SumGPT3": "Received part 2/2\n\nIntroduction:\nThe paper addresses the challenge of opinion summarization in the context of rapidly expanding online reviews, where manually sifting through large volumes of data becomes impractical for consumers. Opinion summarization, involving the extraction and condensation of customer sentiments from reviews, is crucial for aiding purchase decisions in e-commerce.\n\nNovelty:\n1. **Refined Assumption for Opinion Relation Detection**: The paper introduces a more stringent assumption (Assumption 2) compared to previous methods, requiring all three factors - opinion word, opinion target, and linking relation - to be verified simultaneously during opinion relation detection. This approach aims to mitigate errors caused by the previous assumption's tendency to include noise terms, leading to more precise opinion summaries.\n  \n2. **OCDNN for One-Class Classification**: It proposes a novel method, One-Class Deep Neural Network (OCDNN), to address the lack of negative labeled examples in opinion relation detection. By integrating Deep Neural Networks (DNNs) with word embeddings into an OCC framework, the paper achieves semantic similarity measurement between terms, enabling more accurate classification of opinion words, targets, and relations.\n\n3. **Recursive Autoencoder for Linking Relation Representation**: The paper introduces a method to represent linking relations between opinion words and targets using recursive autoencoders based on syntactic dependency structures. This approach allows for a more nuanced representation of linking relations, improving classification accuracy.\n\nConclusion:\nThe paper presents a novel approach to opinion summarization by refining the assumption for opinion relation detection, introducing OCDNN for one-class classification, and employing recursive autoencoders for linking relation representation. These advancements contribute to more accurate and informative opinion summaries, enhancing decision-making processes for consumers in e-commerce environments.", "SumGem": "## Summary of the Scientific Paper on Opinion Relation Detection\n\n**Introduction**\n\nThis paper proposes a novel method for joint opinion relation detection, aiming to improve the accuracy of identifying opinions in customer reviews. \n\n**Novelty**\n\nThe paper introduces a new approach to joint opinion relation detection that addresses limitations of prior methods. Here are the key aspects of this novelty:\n\n* **Assumption 2**: Traditional methods rely on Assumption 1, which validates opinion relations based on two factors: the presence of an opinion word and a linked target. This paper proposes Assumption 2, which requires verification of all three factors - opinion word, opinion target, and the linking relation between them - to ensure a more precise identification of opinion relations.\n\n* **One-Class Deep Neural Network (OCDNN)**: Due to the difficulty of acquiring negative examples for training, the paper proposes a one-class classification approach using a Deep Neural Network (DNN). This allows the model to learn from positive examples (known opinion relations) and identify new relations with similar characteristics.\n\n* **Recursive Autoencoder for Representing Linking Relations**: The paper introduces a method to represent linking relations between opinion words and targets using a recursive autoencoder. This enables the model to capture the syntactic structure of the relation and identify similar relations even if they don't contain the exact seed relation terms.\n\n**Conclusion**\n\nThis paper presents a novel OCDNN approach for joint opinion relation detection that overcomes limitations of previous methods by incorporating all three factors (opinion word, target, and linking relation) and utilizing a one-class classification strategy with a recursive autoencoder for handling the scarcity of negative training data. This method offers a more precise and robust way to identify opinion relations in customer reviews. ", "SumGPT4": "### Introduction\nThe paper addresses the challenge of opinion summarization from online product and service reviews, where the detection of opinion relations\u2014links between opinion words and their targets\u2014is a critical step. Traditional methods often rely on assumptions that fail to consider all three components of opinion relations comprehensively, leading to inaccuracies. This work proposes a novel approach to improve the precision of opinion relation detection using a One-Class Deep Neural Network (OCDNN).\n\n### Novelty\n- **Assumption 2 for Opinion Relation Detection:** The paper introduces a more stringent assumption that mandates simultaneous verification of opinion words, opinion targets, and their linking relations. This contrasts with previous methods that typically check only partial aspects, often leading to noise and inaccuracies in opinion relation identification.\n  \n- **One-Class Deep Neural Network (OCDNN):** The proposed method integrates a deep neural network into a one-class classification framework to detect opinion relations. Unlike traditional methods that require negative examples or full supervision, OCDNN effectively learns from a small set of positive examples without needing negative examples, addressing the difficulty of obtaining such data.\n\n- **Word and Relation Representation:** The approach employs word embeddings to represent opinion words and targets, and uses recursive autoencoders to capture the linking relations in a vectorized form. This combination enables the detection of opinion relations based on semantic similarity and syntactic dependencies.\n\n- **Iterative Seed Expansion:** The paper introduces a bootstrapping process for expanding the initial set of opinion target seeds. This iterative self-training mechanism enhances the model's ability to generalize by incrementally including new targets identified from the classified opinion relations.\n\n### Conclusion\nThe proposed OCDNN-based method significantly outperforms existing weakly supervised approaches by ensuring that all components of opinion relations are verified. This joint detection mechanism, relying on semantic similarity and leveraging deep learning, provides a more accurate and scalable solution for opinion summarization in various domains, overcoming the limitations of traditional methods.", "rankGPT3": 2, "rankGem": 1, "rankGPT4": 3, "expGPT3": "This summary is ranked second because it adequately introduces the problem and the novelty of the proposed method. It covers Assumption 2 and OCDNN but lacks some details on the recursive autoencoder part mentioned in the paper.\nWhile it captures the essence of the paper's contribution, it could be improved by including more specific details about all the novel methods introduced, especially the recursive autoencoder part, which is crucial according to the paper.", "expGem": "This summary is ranked the highest because it effectively captures all the key aspects of the scientific paper in a structured and clear manner. It clearly outlines the novel contributions such as Assumption 2 for opinion relation detection, OCDNN for one-class classification, and the use of recursive autoencoders for linking relation representation. The conclusion succinctly summarizes the advancements and their potential impact.\nhe summary is comprehensive yet concise, covering all the essential points without unnecessary details. It provides a good balance between explaining the novelty of the proposed method and its implications for opinion summarization.", "expGPT4": "This summary is ranked the lowest because it provides a less structured and less detailed overview compared to the others. It touches on the Assumption 2 and OCDNN but lacks clarity on how these components are integrated or their specific contributions.\nThe summary seems to mix some key points but lacks a clear delineation of each novelty introduced in the paper. It doesn't provide as clear a picture of how each method (Assumption 2, OCDNN, recursive autoencoders) contributes to improving opinion relation detection, which is essential for understanding the paper's contributions."}, "24": {"title": "Towards a General, Continuous Model of Turn-taking in Spoken Dialogue using LSTM Recurrent Neural Networks", "url": "https://aclanthology.org/W17-5527", "text": "1 Introduction\nOne of the most fundamental aspects of dialogue\nis the organization of speaking between the participants. Since it is difficult to speak and listen\nat the same time, the interlocutors need to take\nturns speaking, and this turn-taking has to be coordinated somehow. This poses a challenge for\nspoken dialogue systems, where the system\nneeds to coordinate its speaking with the user to\navoid interruptions and (inappropriate) gaps and\noverlaps.\nFor a full account of turn-taking, there are\nmany different aspects that need to be modelled.\nFor example, the system should be able to detect\nwhether the user is likely to continue speaking\nafter a brief silence, or whether the system\nshould respond (Meena et al., 2014; Ferrer et al.,\n2002). Another related issue is to detect places\nwhere it is appropriate to give brief feedback (socalled backchannels) while the user is speaking\n(Morency et al., 2008). If the user starts speaking, it is also important to estimate whether the\nuser is most likely initiating a longer utterance,\nor a shorter listener response (Neiberg and\nTruong, 2011; Selfridge et al., 2013). When the\nsystem is speaking, it is important to assess\nwhether the user will interpret pauses in the system\u2019s speech as turn-yielding (an opportunity to\ntake the turn) or not, depending on how the system\u2019s utterance is synthesized (Hjalmarsson,\n2011). So far, these different problems have\nmostly been addressed as separate issues, using\ndifferent models.\nIn this paper, we present a general, continuous\nmodel of turn-taking, trained on dialogue data.\nThe model is general, in that we do not train it\nfor specific turn-taking decisions, but instead\ntrain it to forecast the probability that the speakers will continue speaking over a future time\nwindow. The model is continuous, in that it does\nthis at every time step, and not at certain events\n(such as when someone stopped speaking). We\nargue that this predictive model is potentially\nuseful for a number of different types of predictions and decisions that are relevant for spoken\ndialogue systems.\nA similar approach was taken by Ward et al.\n(2010). However, their experiments only yielded\nmodest improvements over the baseline. An explanation for this might be that turn-taking is a\nhighly context-dependent phenomenon, and that\nrepresentation of dialogue context is a challenging task, typically involving a lot of heuristics\nand feature engineering. To address this problem,\nand make as few assumptions as possible, we\ntrain the model using Long Short-Term Memory\n(LSTM) Recurrent Neural Networks (RNN),\nwhere the context-modelling is left to the net220\nwork, and we feed it with fairly basic features\nrepresenting cues known to be relevant for turntaking.\nThe paper is organized as follows. We start\nwith a review of related work on the problem of\nturn-taking in dialogue, and give a brief overview of RNNs. We then describe the proposed\nmodel in more detail, how it was applied in this\nstudy, and how features were extracted. Using\nthe HCRC Map Task Corpus (Anderson et al.,\n1991), we then present two experiments on turntaking predictions, both at pauses and at speech\nonset. Finally we investigate how the model can\nbe applied to make predictions on humancomputer dialogue data.\n2 Background\n2.1 Turn-taking in Spoken Dialogue\nTraditionally, spoken dialogue systems have\nrested on a very simplistic model of turn-taking,\nwhere a certain amount of silence (e.g., 700ms)\nis used as an indicator that the user has stopped\nspeaking, and that the turn is yielded to the system. One obvious problem with this model is that\nturn-shifts often are supposed to be much more\nrapid than this, with very short gaps, and that\npauses within a turn often might be longer. Thus,\nthe system will sometimes appear to give sluggish responses, and sometimes interrupt the user.\nSeveral studies have shown that humans coordinate their turn-taking using much more sophisticated cues. For example, an incomplete syntactic\nclause or a filled pause (such as \u201cuhm\u201d) typically\nindicates that the speaker is not yielding the turn\n(Clark and Fox Tree, 2002), and turn-taking is\nrelated to information density in the words spoken (Dethlefs et al., 2016). Prosodically, a rising\nor falling pitch at the end of a segment tend to be\nturn-yielding, whereas a flat pitch is turn-holding\n(Edlund and Heldner, 2005). The intensity of the\nvoice tends be lower when yielding the turn, and\nthe duration of the last phoneme tends to be\nshorter. Gaze has also been found to be an important cue \u2013 speakers tend to not look at the addressee during an utterance, but then shift the\ngaze towards the addressee when yielding the\nturn (Kendon, 1967). Studies have also shown\nthat the more turn-yielding cues are presented\ntogether, the more likely it is that the other\nspeaker will take the turn (Gravano and\nHirschberg, 2011; Koiso et al., 1998; Duncan\nand Niederehe, 1974).\nSeveral models have been presented for taking\nthese different cues into account and to predict\nturn-taking events. A common approach is to\nsegment the speech into so-called Inter-Pausal\nUnits (IPU), which is a stretch of audio from one\nspeaker without any silence exceeding a certain\namount (such as 200ms). Given the end of an\nIPU, the model has to predict whether the speaker is making a pause and \u201cholding\u201d the turn, or\nwhether the speaker is yielding the turn. Various\nfeature sets and machine learning algorithms\nhave been proposed, and tested on both humanhuman and human-machine dialogue data\n(Meena et al., 2014; Schlangen, 2006; Neiberg\nand Gustafson, 2011; Johansson and Skantze,\n2015; Ferrer et al., 2002; Kawahara et al., 2012).\nThese kinds of models assume that turn-taking\nonly occurs when a speaker has stopped speaking. However, in studies of human-human dialogue it is clear that overlaps are fairly frequent\n(Heldner and Edlund, 2010). A common phenomenon, that often leads to overlapping speech,\nis backchannels \u2013 short utterances (such as\n\u201cmhm\u201d or \u201cyeah\u201d), which the listener provides to\nshow continued attention (Yngve, 1970). Models\nhave been proposed to continuously detect where\nin the speech these are suitable (Morency et al.,\n2008). Given that a listener starts to speak, the\ncurrent speaker must also detect whether the listener is simply providing a backchannel (so that\nthe speaker may continue), or is intending to\nclaim the floor to produce a longer response\n(Neiberg and Truong, 2011).\nAnother limitation of IPU-based models of\nturn-taking is that they are purely reactive. Several studies have shown that humans are able to\npredict upcoming turn-taking events (Tice and\nHenetz, 2011), and that this prediction facilitates\nrapid and accurate turn-taking (Ruiter et al.,\n2006). To implement this behaviour in spoken\ndialogue systems, it is important that they can\nprocess speech incrementally (Skantze and\nSchlangen, 2009), and not wait until the user is\ndone speaking. The model proposed in this paper\nis based on an incremental and predictive notion\nof turn-taking, where the model continuously\nmonitor the speech from the two interlocutors\nand makes predictions about future turn-taking\nevents.\n2.2 Modelling Context with Recurrent Neural Networks\nMost attempts at creating computational models\nof turn-taking have only considered a brief window before the turn-taking decision is being\nmade. Also, any dynamic events (such as a raise\nin pitch) in this window need to be transformed\n221\ninto a single feature vector using heuristics and\ncareful feature engineering. This is an obvious\ndrawback, since turn-taking is likely to be dependent on various contextual properties, such as\nprevious speaking activity. To address this problem, we propose to use Recurrent Neural Networks (RNNs), which are especially designed to\nlearn representations of context from low-level\nfeatures. Whereas a typical feedforward neural\nnetwork only transforms a single feature vector\ninto an output vector (possibly through a number\nof hidden layers), RNNs are neural networks\nwith loops that allow information to persist from\none step in time to the next, as illustrated in Figure 1. During training and backpropagation, the\nupdates are fed back in time, in order to adjust\nthe weights at previous time steps, and thereby\npotentially learn long-term dependencies.\nFigure 1. The principle behind RNNs with an unrolled view to the right. The neural network, A, looks\nat the input it at time t and outputs a value ot. The loop\nallows the network to remember the state at time t-1.\nA limitation of traditional RNNs is their inability to learn dependencies over longer time sequences. The reason for this is that the update\ngradients become too small over longer distances. This can be especially problematic for the\ncontinuous model proposed here, since important\nevents may occur many frames before the turntaking prediction is being made. To address this\nproblem, it is common to use an extension called\nLong Short-Term Memory (LSTM), which have\na cell state and a gating mechanism that allow\ninformation to pass longer paths in the network\nhistory, thereby avoiding the vanishing gradient\nproblem (Hochreiter and Schmidhuber, 1997).\nLSTM has been successfully applied to a number\nof tasks related to speech and language processing, such as voice activity detection (Eyben\net al., 2014), speech recognition (Graves et al.,\n2013), and spoken language understanding (Liu\nand Lane, 2016). To our knowledge, this is the\nfirst attempt at using LSTM RNNs for a continuous model of turn-taking.\n3 Model and Data\n3.1 The Model\nThe general principles for the model are illustrated in Figure 2. An RNN is trained to make continuous predictions about the speech activity for\none of the speakers (speaker S0) for an upcoming\nfixed time window, based on previous events in\nboth speaker channels. The speech signals for the\ntwo speakers (S0 and S1) are segmented into\nequally sized frames (or time steps). For each\nframe, features from both speakers are extracted\nand fed into an RNN with one LSTM layer. For\neach frame, the RNN outputs an N-dimensional\nvector with predictions of the probability that S0\nwill speak or not for the next N frames. For the\nexperiments in this paper, we use a frame size of\n50ms (20 frames per second), and a prediction\nwindow of 3 seconds (60 frames).\nFigure 2. How the model makes predictions and is\ntrained, with an unrolled view of the RNN. For each\nframe (50ms), the network predicts the probability of\nspeaker S0 speaking over the next N frames (with one\noutput node per frame).\nTo train the model, we use human-human dialogue data, with the voice activity of speaker S0\nfor the next N frames as target labels. Although\nthese labels are binary, the output nodes will be\ntrained to provide a probabilistic score (between\n0 and 1). To allow the model to train to make\npredictions for both speakers, the same network\nis trained on each dialogue twice, with each\nspeaker serving as both speaker S0 and S1.\nWhen applying the model, two network instances are used, one in which speaker A serves\nas S0 (to get predictions for speaker A), and one\nwhere speaker B serves as S0 (to get predictions\ni\nt\nA\not\n=\ni0\nA\no0\ni1\nA\no1\ni2\nA\no2\ni3\nA\no3\nOutputs\nLSTM layer\nPrediction of S0 speaking\nS0\nS1\nInput\nTraining labels\n(S0 speaking)\n1 1 1 1 1 1 1 1\nFeature extraction\n0 0 0 0\n222\nfor speaker B), with the speaker features\nswitched between the two networks. Some examples of what the predictions can look like are\nshown in the Appendix1\n. Note that although we\nwill here assume two speakers, the model is not\nlimited to dyadic interaction. In principle, it\ncould be applied to dialogues with any number of\nspeakers, where each speaker is modelled with\nits own network at application time.\nThe model should also be applicable for making decisions in dialogue systems. By feeding the\ntwo networks (as described above) in real time\nwith both the user\u2019s speech and the system\u2019s own\nspeech, the user\u2019s network will make predictions\nof how likely it is that the user will speak in the\nnear future. But the system\u2019s network will also\npredict how likely it is that the system should be\nspeaking in the future time window, given the\nassumption that a human-like behaviour is desired. The output of the two models could then\nbe combined to make decisions of whether the\nsystem should speak or not. In the simplest case,\nthe two predictions can be compared, and if the\nsystem\u2019s network has a stronger prediction than\nthe user\u2019s network, it would constitute a good\nplace to take the turn. Since the model is probabilistic, a more sophisticated decision theoretic\napproach could take the probabilities of the predictions, together with a utility, into account. For\nexample, it could still be desirable for the system\nto take the turn even if it is an unlikely place to\ndo so, given that the system has something important to say. Since the probabilities are updated\ncontinuously, even during silences, the model\ncould naturally generate variable gap lengths in\nthe system\u2019s response.\nAnother potential application of the model\nwould be for the generation of system responses.\nGiven different prosodic and syntactic realisations of a response, the model could predict\nwhether the user is likely to take the turn, for\nexample in pauses. To select a response which\nsignals the intended turn-taking cues, the system\ncould feed different candidate responses into the\nnetworks and predict how the user would react to\nthem. Yet another application would be to enhance Voice Activity Detection (VAD) with the\nprobability that the user will be speaking, given\nthe dialogue context.\nIn this paper, we will mainly evaluate the\nmodel on its predictive power when observing\nhuman-human interaction. However, we will also\n\n1 A video of live predictions can be seen at\nhttps://www.youtube.com/watch?v=wE2pPZQGR6U\ninvestigate whether it could be used for turntaking decisions in a spoken dialogue system,\naccording to the simple method outlined above.\n5 Conclusions and Discussion\nIn this paper, we have presented a first step towards a general model of turn-taking in spoken\ndialogue. Unlike most previous models, the proposed model is not trained towards specific turntaking decisions, but instead makes continuous\npredictions of future speech activity. To evaluate\nthe model, we have applied it to two different\nturn-taking decisions for which it was not specifically trained. First, to detect the next speaker at\npauses, where the model achieves a better performance than more traditional attempts on the\nsame dataset, and better than human performance. Second, to project the length of an utterance at speech onset, where the model also yields\na better performance than traditional models.\nFinally, we have tested the model on humanrobot dialogue data. Most likely due to the large\ndifferences in training and testing conditions, the\nmodel was not directly applicable for making\nturn-taking decisions in this setting. However, it\ncould at least be used for feature extraction to\ntrain a separate model on a small set of manually\nlabelled data.\nSo far, we have relied on manually labelled\nPOS features (for the Full model). For future\nstudies, we would like to see how well the model\nwould cope with automatic online POS tagging\nof ASR results. Although we have worked with\nmanually annotated speech segments, these could\nalso be extracted with a VAD. All other features\nwere automatically extracted.\nAs noted earlier, the model should be applicable to multi-party interaction. Another obvious\nextension is to use multi-modal features, such as\ngaze and gestures, which have shown to be important for turn-taking (Kawahara et al., 2012;\nJohansson and Skantze, 2015).\nSo far, we have only tested the model on binary decisions, in order to make the results as clear\nand comparable as possible. However, this clearly only hints at some of the potential applications\nof the model (which can be grasped by looking at\nthe examples in the Appendix). For example,\nsince the model is continuous and predictive, it\nshould be possible to use it for preparing a dialogue system to make responses before the user\u2019s\nutterance is completed. Since the model is probabilistic, it should be possible to use it in a decision-theoretic framework, as discussed in 3.1\nabove. However, to make the model directly applicable to spoken dialogue systems, it should\nprobably be trained on a more diverse set of interactions, more similar to the actual dialogue\nsystem application.", "SumGPT3": "Received part 2/2. Now, let's summarize the paper.\n\n**Introduction:** \nThe paper addresses the intricate dynamics of turn-taking in dialogues, crucial for natural conversation flow in spoken dialogue systems. It discusses challenges such as interruptions, gaps, and overlaps in coordinating speaking turns between system and user.\n\n**Novelty:**\n1. **Continuous Predictive Model:** Unlike traditional models, the paper proposes a novel continuous predictive model trained on dialogue data. This model forecasts the probability of speakers continuing their turn over a future time window, enabling real-time decision-making in dialogue systems.\n2. **Recurrent Neural Networks (RNNs):** Leveraging RNNs, specifically LSTM, the model learns contextual dependencies from low-level features, offering a more nuanced understanding of turn-taking beyond brief windows. This allows for better adaptation to various dialogue contexts and reduces reliance on heuristic-based feature engineering.\n3. **Multi-Party Interaction and Multi-Modal Features:** The model's potential extends to multi-party interactions and integration of multi-modal features like gaze and gestures, enriching its applicability and adaptability in diverse conversational settings.\n4. **Probabilistic Decision-Making:** By providing probabilistic predictions, the model facilitates decision-theoretic frameworks, enabling dialogue systems to anticipate and prepare responses before a user's utterance completion.\n\n**Conclusion:**\nThe paper presents a pioneering step towards a comprehensive model of turn-taking in spoken dialogue, departing from traditional binary decision-making models. It highlights the model's superior performance in predicting next speakers and utterance lengths, paving the way for more efficient and natural human-computer interactions. Further exploration into diverse interaction datasets and integration of automatic features extraction are suggested for broader applicability in real-world dialogue systems.", "SumGem": "## Summary of the Scientific Paper on Turn-Taking in Spoken Dialogue\n\n**Introduction**\n\nThis paper proposes a novel, continuous model for turn-taking in spoken dialogue. Unlike previous models that focus on specific turn-taking decisions (e.g., predicting if a speaker will continue speaking after a pause), this model continuously predicts the probability of future speech activity for a speaker within a specific time window. \n\n**Novelty**\n\nThe paper highlights several key aspects of the proposed model's novelty:\n\n* **Continuous Prediction:**  The model predicts speech activity throughout the conversation, not just at specific events like pauses or the end of utterances. \n* **Context-aware Predictions:**  The model utilizes Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) to learn contextual information from past speech activity in both speakers' channels. This allows the model to consider a broader range of cues for turn-taking decisions compared to traditional feature-based approaches.\n* **Generalizability:** The model is designed to be generalizable to various scenarios. It is not limited to two speakers and can be potentially applied to multi-party interactions. Additionally, the model can be trained on different modalities beyond speech, such as gaze and gestures.\n* **Probabilistic Framework:**  By providing probabilistic predictions, the model allows for more sophisticated decision-making in dialogue systems. This could involve using the probabilities within a decision-theoretic framework to determine when the system should take the turn.\n\n**Conclusion**\n\nThe paper demonstrates the effectiveness of the proposed model in predicting turn-taking behavior on human-human dialogue data. It also explores potential applications in spoken dialogue systems, suggesting that the model can be used for various tasks, including informing response generation and enhancing voice activity detection. While the model requires further exploration for direct implementation in dialogue systems due to potential domain mismatches, it offers a promising approach for building more natural and human-like conversational agents.\n ", "SumGPT4": "**Introduction**  \nThe paper presents a continuous and general model for turn-taking in spoken dialogue systems. Unlike traditional methods, which address turn-taking through isolated cues or decision points, this model aims to forecast the probability of speakers continuing their speech over a future time window, based on dialogue data and basic turn-taking cues.\n\n**Novelty**  \n1. **Continuous Prediction:** The model continuously predicts future speech activity at every time step, rather than only reacting to specific turn-taking events such as pauses or the end of speech segments. This allows for more fluid and contextually aware dialogue management.\n\n2. **General Approach:** Instead of focusing on specific turn-taking cues, the model uses Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) to generalize turn-taking prediction. This method enables the model to adapt to various types of turn-taking decisions without extensive feature engineering.\n\n3. **Context Modelling:** The use of LSTM RNNs helps the model capture and utilize the context of prior speech activity over longer sequences, avoiding the need for manual heuristic features that are typically used in traditional models.\n\n4. **Training with Dialogue Data:** The model is trained on dialogue data rather than engineered rules, making it versatile for different types of dialogue systems and turn-taking decisions.\n\n5. **Application Flexibility:** The model's continuous and probabilistic nature allows for various applications, such as improving Voice Activity Detection (VAD), generating context-sensitive system responses, and making dynamic turn-taking decisions in real-time.\n\n**Conclusion**  \nThe proposed model represents a significant shift from traditional, reactive turn-taking systems by offering a continuous, predictive approach that leverages RNNs to handle the complexities of spoken dialogue. This allows for more sophisticated and human-like turn-taking, potentially enhancing the performance of dialogue systems in real-time interactions. The success of the model in human-human dialogue prediction suggests its potential applicability in human-computer interactions, although further training on diverse dialogue scenarios is needed for practical deployment.", "rankGPT3": 1, "rankGem": 2, "rankGPT4": 3, "expGPT3": "Detail and Clarity: This summary is highly detailed, breaking down the model\u2019s components and contributions clearly and concisely. It covers all major points: continuous predictive model, use of LSTM RNNs, applicability to multi-party interactions, and probabilistic decision-making.\nComprehensive Explanation: It explains the novelty of the model in a structured manner, making it easy to understand the advancements over traditional models.\nConclusion: The conclusion is thorough, explaining the implications and potential future work, which provides a complete picture of the paper's contributions.", "expGem": "Detail and Clarity: This summary is also detailed and clear but slightly less comprehensive than the best summary. It covers the major innovations such as continuous prediction, context-aware predictions using LSTM RNNs, generalizability, and the probabilistic framework.\nFocus on Novelty: It focuses well on the model\u2019s novelty and its advantages over traditional methods.\nConclusion: The conclusion is strong but not as expansive as the best summary. It mentions the model's effectiveness and potential applications but does not delve as deeply into the future implications and potential improvements.", "expGPT4": "Detail and Clarity: While this summary is clear and covers the main points, it is less detailed compared to the other two summaries. It mentions continuous prediction, general approach, context modeling, training with dialogue data, and application flexibility but does not elaborate on these points as thoroughly.\nConciseness vs. Depth: It is concise but at the expense of depth, making it less informative about the specific innovations and implications of the model.\nConclusion: The conclusion is brief and lacks the comprehensive view of the paper\u2019s impact and future directions that are present in the other summaries."}}